msgid ""
msgstr ""
"Project-Id-Version: 0\n"
"POT-Creation-Date: 2016-07-16 15:44+0200\n"
"PO-Revision-Date: 2016-07-16 15:44+0200\n"
"Last-Translator: Automatically generated\n"
"Language-Team: None\n"
"Language: en-US \n"
"MIME-Version: 1.0\n"
"Content-Type: application/x-publican; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: Publican v4.3.0\n"

msgid "First Boot"
msgstr "初回起動時の操作"

msgid "Running qlustar-initial-config"
msgstr "qlustar-initial-configの実行"

msgid "After the server has booted the newly installed Qlustar OS, log in as root and start the post-install configuration process by running the command"
msgstr "新たにインストールしたQlustar OSでサーバを起動した後、rootでログインして以下のコマンドを実行し、インストール後設定のプロセスを開始して下さい。"

msgid ""
"\n"
"/usr/sbin/qlustar-initial-config\n"
"	"
msgstr "/usr/sbin/qlustar-initial-config\n"
"       "

msgid "This will first thoroughly check your network connectivity and then complete the installation by executing the remaining configuration steps as detailed below. During the package update process you might be asked whether to keep locally modified configuration files. In this case always choose the option"
msgstr "このプロセスは、始めにネットワークの接続状態を確認し、そして以下に詳細を示す残りの設定手順をを実行してインストールを完了します。パッケージアップデートの途中で、ローカルで変更を加えた設定ファイルを保持するかを聞かれるかもしれません。その場合は常に以下の選択肢を選択して下さい。"

msgid ""
"\n"
"keep the local version currently installed.\n"
"      "
msgstr ""
"\n"
"現在インストールされているローカルの設定ファイルを保持する\n"
"      "

msgid "Remaining configuration steps run-through"
msgstr "残りの設定手順"

msgid "If your chosen hostname can't be resolved via <firstterm>DNS</firstterm>, you will see a non-fatal error message reminding you that the hostname should be registered in some (external) name service (typically DNS)."
msgstr "<firstterm>DNS</firstterm>により名前解決ができないホスト名を選択した場合、なんらかの(外部の)ネームサービス(通常DNS)に登録されたホストネームの使用を促す致命的でないエラーのメッセージが表示されます。"

msgid "Cluster name"
msgstr "クラスタ名"

msgid "Name the cluster"
msgstr "クラスタの名前"

msgid "First, you will be asked for the name of the new Qlustar cluster. This can be any string and is used in some places like the <firstterm>slurm</firstterm> or <firstterm>ganglia</firstterm> configuration."
msgstr "はじめに、新しいQlustarクラスタの名前を入力します。この名前はどのような文字列でも構いません。この名前は<firstterm>slurm</firstterm>や<firstterm>ganglia</firstterm>などの設定に使用されます。"

msgid "NIS Setup"
msgstr "NISセットアップ"

msgid "Setting up NIS"
msgstr "NISの設定"

msgid "Next is the setup of the <firstterm>NIS</firstterm> database. Just confirm the suggested NIS server by pressing <keycombo><keycap>Ctrl-D</keycap><keycap>Enter</keycap></keycombo> to proceed."
msgstr "次は<firstterm>NIS</firstterm>データベースの設定です。表示されたNISサーバ名を確認し、<keycombo><keycap>Ctrl-D</keycap><keycap>Enter</keycap></keycombo>を押して処理を進めて下さい。"

msgid "Configuring ssh"
msgstr "sshの設定"

msgid "Creating an SSH key"
msgstr "SSH keyの生成"

msgid "An <firstterm>ssh key</firstterm> for the root user is generated next. You can enter an optional <firstterm>pass-phrase</firstterm> for it. Note however, that a non-empty pass-phrase will have the effect that you will have to specify it any time, you try to ssh to another host in the cluster. If you don't want that, work without a pass-phrase. This key, will be used to enable login by root to any node of the cluster without specifying a password."
msgstr "次にrootユーザの<firstterm>ssh key</firstterm>を生成します。パスフレーズを設定することもできます。しかしながら空ではないパスフレーズを設定した場合、クラスタ内のホスト間でsshする際にいつもそのパスフレーズを入力しなくてはなりません。sshの度にパスフレーズを入力することを望まない場合は、パスフレーズに何も入力せずに手順を進めて下さい。このssh keyはクラスタ内のどのノード間でもパスワードの入力無しにrootユーザがsshログインすることを可能にします。"

msgid "Configuring Nagios"
msgstr "Nagiosの設定"

msgid "Setting the Nagios password"
msgstr "Nagiosパスワードの設定"

msgid "The configuration of <firstterm>Nagios</firstterm> requires you to choose a password for the Nagios admin account. Please type in the password twice."
msgstr "Nagiosの設定では、Nagiosの管理アカウントのパスワードを決める必要があります。パスワードは2回入力して下さい。"

msgid "Configuring QluMan"
msgstr "QluManの設定"

msgid "Bootstrapping QluMan"
msgstr "QluManのブートストラップ"

msgid "<application>QluMan</application>, the Qlustar management framework (see the QluMan Guide), requires a <firstterm>mysql</firstterm> (mariaDB) database. You will be asked for the password of the QluMan DB user next. After entering it, the QluMan database and configuration settings will be initialized. This can take a while, since a number of OS images and chroots (see <xref linkend=\"sec-Adding-Software\" />) will be generated during this step."
msgstr "Qlustarの管理フレームワーク（詳細はQluManガイド参照）QluManは、mysql(mariaDB)データベースを使用します。次にQluManデータベースユーザのパスワードを聞かれます。パスワードを入力した後、QluManデータベースと設定が初期化されます。初期化のステップでは、いくつかのOSイメージとchroots（詳細はソフトウェア追加の節を参照）を生成するため、しばらく時間を要します。"

msgid "Configuring Slurm"
msgstr "Slurmの設定"

msgid "Setting the Slurm DB password"
msgstr "Slurmデータベースパスワードの設定"

msgid "If slurm was selected as the cluster resource manager, its configuration requires the generation of a <firstterm>munge key</firstterm> and the specification of a password for the slurm mysql account. Enter the chosen password twice when asked for it. Note that the slurm database daemon is also being configured by this process. Hence, you will be ready to take full advantage of the accounting features of slurm."
msgstr "slurmをクラスタのリソース管理に選択した場合、muge keyの生成とslurmのmysqlアカウントのパスワードの設定を行います。パスワードを聞かれたら、２回パスワードを入力して下さい。slurmデータベースのデーモンは、この設定プロセスで同じく設定されます。従って、slurmのアカウンティング機能の利点を全て利用できる準備が整うのです。"

msgid "Configuring the virtual Demo Cluster"
msgstr "仮想デモクラスタの設定"

msgid "Setting up the virtual demo cluster"
msgstr "仮想デモを設定する"

msgid "If you have chosen to setup some virtual demo nodes during installation, you will be asked for the user name of a test account that can be used to explore the cluster. The account will be generated with the default password for the cluster (see the information on the screen)."
msgstr "インストールの過程でいくつかの仮想デモノードのセットアップを選択した場合、クラスタのテスト利用に使用するテストアカウントのユーザ名を聞かれます。このアカウントは自動生成されるデフォルトパスワードと共に発行されます（詳細はスクリーンに表示されます）。"

msgid "Setting the MariaDB root password"
msgstr "MariaDBのrootパスワードの設定"

msgid "To conclude the configuration procedure, you will be asked to set the password for the MariaDB/MySQL root account. Setting a password here is important. It prevents unauthorized access to the Qlustar or other databases on your head-node."
msgstr "設定手順の締めくくりでは、MariaDB/MySQLのrootカウントのパスワードを聞かれます。ここで設定するパスワードは重要です。Qlustarや管理ノード上の他のデーターベースに許可なくアクセスすることを防ぎます。"

msgid "Final Reboot"
msgstr "さいごに再起動"

msgid "Please reboot again once all the previous steps are complete (e.g. by typing the command <command>shutdown -r</command> now ). After the head-node is up and running again, test its network connectivity by pinging its public IP address (hostname). Do the same for the virtual front-end node, if you have chosen to configure one. It should have booted as well after the head-node is up and running. You can try to login to it using ssh. A test mail should have been sent to the e-mail address(es) you specified during the installation. If you didn't receive one, review your settings in <filename>/etc/aliases</filename> and/or <filename>/etc/postfix/main.cf</filename>. In case some of them are wrong, you can execute"
msgstr "これまでのステップを全て完了したら再び再起動して下さい（例：shutdown -r nowコマンドを実行する）。管理ノードが起動し終えたら、まずpublic IPアドレス（ホストネーム）にpingを打ってネットワークの接続状態を確認して下さい。仮想フロントエンドノードの設定をインストールの過程で選択した場合、仮想フロントエンドに向けて同じくpingを打って下さい。仮想フロントエンドノードは、管理ノードが起動した後に自動で起動しているはずです。仮想フロントエンドノードにsshでログインできるか試して下さい。インストールプロセスで有効なメールリレーサーバの設定を行った場合は、eメールが届きます。もし、有効な設定をしたにも係わらずメールが届かない場合は、/etc/aliases又は/etc/postfix/main.cfに書かれた設定を確認して下さい。もしそれらの設定に誤りがあった場合は、以下のコマンドを実行して"

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>dpkg-reconfigure postfix</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>dpkg-reconfigure postfix</command>\n"
"       "

msgid "to modify them."
msgstr "それらの設定を編集して下さい。"

msgid "Starting the virtual Demo Cluster"
msgstr "仮想デモクラスタの起動"

msgid "If you have chosen to configure a virtual demo-cluster, you can start it by executing the command:"
msgstr "仮想デモクラスタの設定をインストールの過程で選択した場合、以下のコマンドを実行することで仮想デモクラスタを起動できます："

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>demo-system-start</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>demo-system-start</command>\n"
"       "

msgid "and to stop it"
msgstr "停止するときは"

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>demo-system-stop</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>demo-system-stop</command>\n"
"       "

msgid "These commands use the configuration file <filename>/etc/qlustar/vm-configs/demo-system.conf</filename>. If you find that the (automatically calculated) amount of RAM per VM is not right, you can change the variable <varname>CN_MEM</varname> to some other value in that file. The consoles of the virtual nodes (and also of the virtual front-end node if you chose to set one up) are accessible in a screen session. Type"
msgstr "これらのコマンドは、設定ファイル/etc/qlustar/vm-configs/demo-system.confを使用しています。もし、（自動で計算された）仮想マシン当たりのメモリ量が適切でない場合は、設定ファイルの中にあるCN_MEM変数の値を変えることができます。仮想ノード（及び仮想フロントエンドノードのセットアップを選択した場合は仮想フロントエンドノード）のコンソールには、screenセッションからアクセス可能です。以下をタイプして"

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>console-fe-vm</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>console-fe-vm</command>\n"
"       "

msgid "to attach to the console session of the virtual FE node and"
msgstr "仮想フロントエンドノードのコンソールセッションにアタッチします。そして、"

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>console-demo-vms</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>console-demo-vms</command>\n"
"       "

msgid "to attach to the console sessions of the virtual demo cluster nodes. Note that the screen command character is <keycap>Ctrl-t</keycap>. To detach from the screen session, type <keycombo><keycap>Ctrl-t</keycap><keycap>d</keycap></keycombo>, to switch to the next/previous screen type <keycombo><keycap>Ctrl-t</keycap><keycap>n</keycap></keycombo> / <keycombo><keycap>Ctrl-t</keycap><keycap>p</keycap></keycombo>. More details on the usage of screen (or byobu, the Debian customized version we use) are available in the corresponding <literal>man pages</literal>. To check whether all nodes are up and running, type"
msgstr "上記コマンドで仮想デモクラスタノードのコンソールセッションにアタッチします。補足として、screenのコマンドキャラクタはCtrl-tです。screenセッションからデタッチするには、Ctrl-t dと入力して下さい、次/前のscreenにスイッチするにはCtrl-t n/Ctrl-t pと入力して下さい。より詳細なscreen(又はbyobu, Qlustarで使用しているscreenのDebianのカスタマイズバージョン)の使用方法は、該当するmanページに記載されています。全てのノードが起動し利用できる状態か確認するには、以下をタイプします。"

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>dsh -a uptime</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>dsh -a uptime</command>\n"
"       "

msgid "<command>dsh</command> or <command>pdsh</command> can be used to execute arbitrary commands on groups of nodes. Check their <literal>man pages</literal> and the corresponding section in the QluMan guide for further information."
msgstr "dshまたはpdshはノードのグループに対して任意のコマンドを実行できます。より詳細な情報については、それぞれのmanページ又はQluMan guideの該当する節をご確認下さい。"

msgid "Installed Services"
msgstr "インストールされているサービス"

msgid "At this stage, the following services are configured and running on your head-node:"
msgstr "この段階では、以下のサービスが設定され管理ノード上で動作しています。"

msgid "<application>Nagios3</application> (monitoring/alerts) with its web interface at http://<replaceable>headnode</replaceable>/nagios3/. Login as user <literal>nagiosadmin</literal> with the password you specified previously."
msgstr "Nagios3（モニタリング/アラート）　WEBインターフェスのURLはhttp://headnode/nagios3/。nagiosadminユーザでこれまでに設定したパスワードでログインします。"

msgid "<application>Ganglia</application> (monitoring) at http://<replaceable>headnode</replaceable>/ganglia/"
msgstr "Ganglia（モニタリング）　URLはhttp://headnode/ganglia/"

msgid "<application>DHCP/ATFTP</application> boot services"
msgstr "DHPC/ATFTP起動サービス"

msgid "<application>NTP</application> time server as client and server"
msgstr "NTPタイムサーバのクライアントとサーバ"

msgid "<application>NFS</application>-Server with exports defined in <filename>/etc/exports</filename>"
msgstr "NFSサーバ。/etc/exportsで設定したディレクトリを共有"

msgid "Depending on your choice of software packages: <application>Slurm</application> (DB + control daemon), <application>Torque</application> server, <application>Corosync</application> (HA), <application>Munge</application> (authentification for slurm/torque). Note that among the latter only slurm and munge are configured automatically during installation. Torque and Corosync require a manual configuration."
msgstr "選択したソフトウェアパッケージにより:Slurm (DB + control daemon)、Torqueサーバ、Corosync (HA)、Munge (slurm/torque用の認証サービス)。slurmとmungeのみがインストール中に自動で設定されます。TorqueとCorosyncはマニュアルの設定が必要です。"

msgid "<application>NIS</application> server"
msgstr "NISサーバ"

msgid "Mail service <application>Postfix</application>"
msgstr "メールサービスPostfix"

msgid "<application>MariaDB</application> server (mysql fork)"
msgstr "MariaDBサーバ (mysql派生)"

msgid "<application>QluMan</application> server (Qlustar Management)"
msgstr "QluManサーバ (クラスタ管理)"

msgid "Please note, that you shouldn't install the default Ubuntu MySQL server packages on the head-node, since QluMan requires <link xlink:href=\"https://mariadb.org/___blank___\">MariaDB</link> and packages of the latter conflict with the MySQL packages. MariaDB is a complete and compatible substitute for MySQL."
msgstr "QluManは<link xlink:href=\"https://mariadb.org/___blank___\">MariaDB</link>とMySQLパッケージと共存できないパッケージを必要としているため、管理ノードにUbuntu標準のMySQLサーバをインストールしないで下さい。MariaDBはMySQL互換の完全な代替ソフトウェアです。"

msgid "Adding Software"
msgstr "ソフトウェアの追加"

msgid "As explained <link xlink:href=\"../QluMan_Guide/index.html#sec-UnionFS-Chroots___blank___\"> elsewhere</link>, the RAM-based root file-system of a Qlustar compute/storage node can be supplemented by a global NFS-exported chroot to allow access to software not already contained in the boot images themselves. During installation, one chroot per selected <link xlink:href=\"../Installation_Guide/index.html#Edge-Platform-Selection___blank___\">edge platform</link> was automatically created. The chroots are located at <filename>/srv/apps/chroots/<replaceable>chroot name</replaceable></filename>, where <replaceable>chroot name</replaceable> would be trusty or wheezy. Each of them contains a full-featured installation of the corresponding Qlustar edge platform. To change into a chroot, convenience bash shell aliases of the form chroot-<replaceable>chroot name</replaceable> are defined for the root user on the head-node. You may use them as follows (e.g. for Debian/Wheezy, if it was selected at install):"
msgstr "他のドキュメント<link xlink:href=\"../QluMan_Guide/index.html#sec-UnionFS-Chroots___blank___\"> QluMan Guide</link>に記載のように、Qlustar 計算ノード/ストレージノードの起動イメージに含まれていないソフトウェアへのアクセスを可能にするため、Qlustar 計算ノード/ストレージノードのRAMベースルートファイルシステムはNFSで共有されたchrootを追加で重ねています。インストール中、選択した<link xlink:href=\"../Installation_Guide/index.html#Edge-Platform-Selection___blank___\">edge platform</link>ごとに1つのchrootが自動で作成されます。chrootは<filename>/srv/apps/chroots/<replaceable>chroot name</replaceable></filename>に配置しています。<replaceable>chroot name</replaceable>は、trustyまたはwheezyです。それぞれのchrootは、該当するQlustar edge platformの全ての機能が利用できるようインストールしています。chroot環境に移行するには、chroot-<replaceable>chroot name</replaceable>という形式の便利なbashシェルエイリアスが管理ノードのrootユーザに設定済みです。例えば以下のようにして使用します。（例　Debian/Wheezyをインストール中に選択した場合）："

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>chroot-wheezy</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>chroot-wheezy</command>\n"
"       "

msgid "Once you're inside a chroot, you can use the standard Debian/Ubuntu tools to control its software packages, e.g."
msgstr "一度chroot環境に入ったら、Debian/Ubuntuの通常のソフトウェアパッケージ管理ツールを使用することができます。例）"

msgid ""
"\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>apt-get update</command>\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>apt-get dist-upgrade</command>\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>apt-get install <replaceable>package</replaceable></command>\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>exit</command>\n"
"	"
msgstr ""
"\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>apt-get update</command>\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>apt-get dist-upgrade</command>\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>apt-get install <replaceable>package</replaceable></command>\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>exit</command>\n"
"       "

msgid "The nice thing about this mechanism is that software from packages installed in a particular chroot will be available instantaneously on all compute/storage nodes that are configured to use that chroot. Important: There is usually no need to install additional packages on the head-node itself (only in the chroot). Software packages installed directly on the head-node will not be visible cluster-wide."
msgstr "本メカニズムの利点は、あるchrootにインストールしたソフトウェアはそのchrootを使用するよう設定された計算/ストレージノードに自動的に反映される点です。重要：　通常管理ノードには追加のソフトウェアをインストールする必要はありません。（通常はchrootのみにインストールします。）管理ノードに直接インストールしたパッケージはクラスタ全体からは利用できません。"

msgid "Running the Cluster Manager QluMan"
msgstr "QluManクラスタ管理の実行"

msgid "Generating a one time token for the first admin login"
msgstr "adminの初回ログインのためのトークンの発行"

msgid "The Qlustar management GUI <application>qluman-qt</application> uses public/private keys for both encryption and authentication. For this there first needs to be an exchange of public keys between the client and the server. Normally this is done by an user with admin role through the GUI. But for the first admin login this must be accomplished using a root shell on the head-node:"
msgstr "Qlustar管理GUI <application>qluman-qt</application>はpublic/privateキーを暗号化と認証の双方に使用します。そのため、まずサーバ・クライアント間のpublicキーの交換が必要になります。通常、GUIを通してadminロールが与えられたユーザが行います。しかしadminログインの初回のみ、管理ノードのrootユーザとしてこれを実行する必要があります。"

msgid ""
"\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>qluman-cli --gencert</command>\n"
"Generating one-time login token for user 'admin':\n"
"Cluster  = QL\n"
"Hostname = beosrv-c\n"
"Port     = 6001\n"
"Pubkey   = b'T)5o]@hsjB2qyY&gt;eb:7)8@BA?idMf&gt;kh%^cRhV/#'\n"
"Enter new pin for one-time token: \n"
"Server infos and one-time login token for user 'admin':\n"
"---[ CUT FROM HERE ]---\n"
"00000191c2MAAcMuyCNQR0DPILx-y-BLCHpITepvG7R3I6452Cdqiu98u4PsM1VWFGqEAG\n"
"V8YN9K5kyJKHtQHGTB1JqZIwt4q0PLArnyNmhCkGLS6VxWWBDtBB9_dGPqLH4OeQ7sZ725\n"
"6XDGgrKo4Dldc_wuCALegczjYV8oc_yZ07X0oIYlzhDlDpk-hTm5bfW8_x904YF0wcv-G-\n"
"nK1ztRg854O7pC_p1YpEJuzWFqWv0e7ffi-ZgkxwfdGGKF3imp4d9yGY4h6Ixdn8TLG2gk\n"
"Z4XQ4dymvSO9hp8mUabfq7prVUOTYeChB2pOrom8XSQxjOoe4Yll5yv6da_CdGq50KrO8Q\n"
"C12Z4Pz2eSbvqXbo7c7DdLRjMc0v0Km3WyljgdsDYbKC5iT75Bgryc\n"
"---[ TO HERE ]---\n"
"	"
msgstr ""
"\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>qluman-cli --gencert</command>\n"
"Generating one-time login token for user 'admin':\n"
"Cluster  = QL\n"
"Hostname = beosrv-c\n"
"Port     = 6001\n"
"Pubkey   = b'T)5o]@hsjB2qyY&gt;eb:7)8@BA?idMf&gt;kh%^cRhV/#'\n"
"Enter new pin for one-time token: \n"
"Server infos and one-time login token for user 'admin':\n"
"---[ CUT FROM HERE ]---\n"
"00000191c2MAAcMuyCNQR0DPILx-y-BLCHpITepvG7R3I6452Cdqiu98u4PsM1VWFGqEAG\n"
"V8YN9K5kyJKHtQHGTB1JqZIwt4q0PLArnyNmhCkGLS6VxWWBDtBB9_dGPqLH4OeQ7sZ725\n"
"6XDGgrKo4Dldc_wuCALegczjYV8oc_yZ07X0oIYlzhDlDpk-hTm5bfW8_x904YF0wcv-G-\n"
"nK1ztRg854O7pC_p1YpEJuzWFqWv0e7ffi-ZgkxwfdGGKF3imp4d9yGY4h6Ixdn8TLG2gk\n"
"Z4XQ4dymvSO9hp8mUabfq7prVUOTYeChB2pOrom8XSQxjOoe4Yll5yv6da_CdGq50KrO8Q\n"
"C12Z4Pz2eSbvqXbo7c7DdLRjMc0v0Km3WyljgdsDYbKC5iT75Bgryc\n"
"---[ TO HERE ]---\n"
"       "

msgid "The token can also be saved directly into a file using the <parameter>-o &lt;filename&gt;</parameter> option and the user the token is for can be specified by the <parameter>-u &lt;username&gt;</parameter> option like this:"
msgstr "トークンは<parameter>-o &lt;filename&gt;</parameter>オプションを使用することで直接指定したファイル名のファイルに保存することも可能です、そして<parameter>-u &lt;username&gt;</parameter>オプションでトークンを割り当てるユーザを指定することが可能です。"

msgid ""
"\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>qluman-cli --gencert -u admin -o token</command>\n"
"Generating one-time login token for user 'admin':\n"
"Cluster  = QL\n"
"Hostname = beosrv-c\n"
"Port     = 6001\n"
"Pubkey   = b'T)5o]@hsjB2qyY&gt;eb:7)8@BA?idMf&gt;kh%^cRhV/#'\n"
"Enter new pin for one-time token: \n"
"Server infos and one-time login token for user 'admin' saved as 'token'\n"
"	"
msgstr ""
"\n"
"<prompt>(trusty) 0 root@cl-head ~ #</prompt>\n"
"<command>qluman-cli --gencert -u admin -o token</command>\n"
"Generating one-time login token for user 'admin':\n"
"Cluster  = QL\n"
"Hostname = beosrv-c\n"
"Port     = 6001\n"
"Pubkey   = b'T)5o]@hsjB2qyY&gt;eb:7)8@BA?idMf&gt;kh%^cRhV/#'\n"
"Enter new pin for one-time token: \n"
"Server infos and one-time login token for user 'admin' saved as 'token'\n"
"       "

msgid "The server infos and one-time login token are protected by the pin you just entered. This is important when the data is sent via unencrypted channels (e.g. email or chat programs) to users or when it is stored on a shared filesystem like NFS. The pin does not need to be a strong password. It is only used to make it non-trivial to use an intercepted token. The token can also only be used once. So once you use it yourself it becomes useless to anybody else. On the other hand, if somebody intercepts the token, guesses the pin and uses the token it will no longer work for you and you know something is wrong."
msgstr "サーバ情報とワンタイムトークンは入力したpinコードで保護されています。これは（emailやチャットプログラム等）暗号化されていないチャンネルでデータを送る際や、NFSのような共有ファイルシステムに保存する場合に重要です。pinは強いパスワードである必要はありません。傍受したトークンを使用する際に簡単には使用できないようにするために使用します。トークンは一度だけ利用できます。そのため一度トークンを使用したら、その後は使用できなくなります。一方で、傍受されたトークンが何者かに使用されてしまった場合、トークンが利用できなくなるため、何かが起きたことに気づくことができます。"

msgid "Starting the QluMan GUI"
msgstr "QluMan GUIの開始"

msgid "During installation, the Qlustar management GUI <firstterm>qluman-qt</firstterm> is installed for the virtual FE node, if one has been setup, otherwise on the head-node. If you like to have it available on the head-node in any case, just install it there like any other package:"
msgstr "インストール中、Qlustar管理GUI　<firstterm>qluman-qt</firstterm>は仮想フロントエンドノードにインストールされます（仮想フロントエンドノードのセットアップを選択した場合）。仮想フロントエンドノードのセットアップを選択しなかった場合は、管理ノードにセットアップされます。どのようなケースの場合でも管理ノードに<firstterm>qluman-qt</firstterm>をインストールしておきたい場合、他のパッケージと同様以下のようにしてインストールして下さい。"

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>apt-get install qluman-qt</command>\n"
"	  "
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>apt-get install qluman-qt</command>\n"
"         "

msgid "The installation on the head-node is not done by default, since it pulls and installs a lot of other packages that qluman-qt depends on, which will slow down updates. If you have the possibility, <link linkend=\"sec-QluMan-Workstation\">install qluman-qt on your workstation</link> and work from there. If not, you can launch qluman-qt remotely on the node, where it is installed (FE or head-node), per ssh (with X11 forwarding enabled / <parameter>-X</parameter> option) as follows:"
msgstr "qluman-qtをインストールするには、qluman-qtが依存する沢山のパッケージをインストールする必要があり、管理ノードのアップデートが遅くなるため、管理ノードへのインストールは標準では行われません。可能であれば、<link linkend=\"sec-QluMan-Workstation\">ワークステーションにqluman-qtをインストール</link>し、そこで利用して下さい。もし可能でない場合は、以下のようにqluman-qtがインストールされたノード（フロントエンド又は管理ノード）のqluman-qtをssh（<parameter>-X</parameter>オプションでX11転送を有効にすること）によりリモート実行することも可能です。"

msgid ""
"\n"
"<prompt>0 user@workstation ~ $</prompt>\n"
"<command>ssh -X root@servername qluman-qt</command>\n"
"	  "
msgstr ""
"\n"
"<prompt>0 user@workstation ~ $</prompt>\n"
"<command>ssh -X root@servername qluman-qt</command>\n"
"         "

msgid "This should bring up the <firstterm>Management Console</firstterm>. Using the one-time token generated as explained above you will now be able to add the cluster to the list of available connections. (Details about this are explained in the <link xlink:href=\"../QluMan_Guide/index.html#sec-add-cluster___blank___\">QluMan Guide</link>)."
msgstr "これにより管理コンソールが表示されます。上で生成したワンタイムトークンを使用して、利用可能なコネクションのリストにクラスタを追加することができます。（詳細は、<link xlink:href=\"../QluMan_Guide/index.html#sec-add-cluster___blank___\">QluMan Guide</link>に記載しています。）"

msgid "Installing the QluMan GUI on a workstation"
msgstr "ワークステーションにQluMan GUIをインストール"

msgid "If your workstation runs one of the edge platforms currently supported by Qlustar, you can install the QluMan GUI directly there. This is recommended, since the <emphasis role=\"bold\">responsiveness</emphasis> of a GUI, that is locally started, is a lot better as compared to one that is running via remote X11. To install the qluman-qt package on your workstation, you need to add the correct Qlustar repository to your <literal>apt sources list</literal>. This can be accomplished by executing the following as root on your workstation."
msgstr "もしワークステーションが現在Qlustarがサポートしているedge platformの中の1つで稼働している場合、QluMan GUIをワークステーションに直接インストールすることが可能です。sshを使用したX11転送の方法と比べ、ローカルでGUIを起動する場合はGUIの応答性が優れるため、こちらの方法を推奨しています。ワークステーションにqluman-qtパッケージをインストールするには、Qlustarリポジトリを<literal>apt sources list</literal>に追加する必要があります。これはワークステーションのrootユーザで以下のコマンドを実行することにより可能です。"

msgid ""
"\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>dpkg -l software-properties-common &gt; /dev/null 2&gt;&amp;1 || apt-get install software-properties-common</command>\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>gpg --no-default-keyring --primary-keyring /etc/apt/trusted.gpg --recv-keys E6BA110F3C0BC307</command>\n"
"	  "
msgstr ""
"\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>dpkg -l software-properties-common &gt; /dev/null 2&gt;&amp;1 || apt-get install software-properties-common</command>\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>gpg --no-default-keyring --primary-keyring /etc/apt/trusted.gpg --recv-keys E6BA110F3C0BC307</command>\n"
"         "

msgid "On a <literal>Debian/Wheezy</literal> workstation, you should use <application>python-software-properties</application> instead of <application>software-properties-common</application> in the first command. The second one should have imported the <firstterm>Qlustar PGP archive key</firstterm>, and must output a line like:"
msgstr "Debian/Wheezyワークステーションの場合、1つめのコマンドで<application>software-properties-common</application>ではなく<application>python-software-properties</application>を使用して下さい。2つめのコマンドは、<firstterm>Qlustar PGP archive key</firstterm>をインポートし、以下のような行を表示するはずです。"

msgid ""
"\n"
"gpg: key 3C0BC307: public key \"Q-Leap Networks (automatic archive key) &lt;info@q-leap.com&gt;\" imported\n"
"	  "
msgstr ""
"\n"
"gpg: key 3C0BC307: public key \"Q-Leap Networks (automatic archive key) &lt;info@q-leap.com&gt;\" imported\n"
"         "

msgid "The <application>gpg</application> command above might fail, the first time you execute it. This happens, if <application>gpg</application> has never been executed before for the root user. In this case, simply execute it a second time."
msgstr "初めて<application>gpg</application>コマンドを実行した時は失敗する場合があります。これはrootユーザが<application>gpg</application>コマンドを1度も実行したことがない場合に起こります。2つめのコマンドの実行に失敗した場合、もう一度同じコマンドを実行して下さい。"

msgid "Then if you have <literal>Ubuntu/Trusty</literal> execute:"
msgstr "そして<literal>Ubuntu/Trusty</literal>ワークステーションの場合は以下を実行します。"

msgid ""
"\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman 9.1-trusty main non-free'</command>\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman\n"
"9.1-trusty-proposed-updates main non-free'</command>\n"
"	  "
msgstr ""
"\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman 9.1-trusty main non-free'</command>\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman\n"
"9.1-trusty-proposed-updates main non-free'</command>\n"
"         "

msgid "Else for <literal>Debian/Wheezy</literal>:"
msgstr "次にDebian/Wheezyの場合は以下を実行します。"

msgid ""
"\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman 9.1-wheezy main non-free'</command>\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman 9.1-wheezy-proposed-updates main non-free'</command>\n"
"	  "
msgstr ""
"\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman 9.1-wheezy main non-free'</command>\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman 9.1-wheezy-proposed-updates main non-free'</command>\n"
"         "

msgid "After this you can install qluman-qt the usual way:"
msgstr "これでqluman-qtをいつもの方法でインストールできます。"

msgid ""
"\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>apt-get update</command>\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>apt-get install qluman-qt</command>\n"
"	  "
msgstr ""
"\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>apt-get update</command>\n"
"<prompt>0 root@workstation ~ #</prompt>\n"
"<command>apt-get install qluman-qt</command>\n"
"         "

msgid "On Ubuntu you need to have the <literal>universe repository</literal> enabled in your <literal>apt sources list</literal> for the above command to succeed."
msgstr "Ubuntuで上のコマンドを成功するためには、<literal>universe repository</literal>が<literal>apt sources list</literal>で有効である必要があります。"

msgid "Finally, the QluMan GUI can then be launched as an ordinary user in a shell on the workstation:"
msgstr "これでワークステーションの一般ユーザがQluMan GUIを起動できる状態になりました。"

msgid ""
"\n"
"<prompt>0 user@workstation ~ $</prompt>\n"
"<command>qluman-qt &amp;</command>\n"
"	  "
msgstr ""
"\n"
"<prompt>0 user@workstation ~ $</prompt>\n"
"<command>qluman-qt &amp;</command>\n"
"         "

msgid "The versions of the QluMan packages on the workstation should be the same as on the head-node(s) to ensure correct operation. Some unequal version combinations might work too, but are usually not well tested."
msgstr "正常なオペレーションを確実とするため、ワークステーションとヘッドノードにインストールしたQluManパッケージは同じバージョンであることが望ましいです。バージョンが異なる組み合わせで動く場合もありますが、通常動作検証があまりなされていません。"

msgid "Creating Users"
msgstr "ユーザ作成"

msgid "Authenticating users in the cluster can be done in many ways, hence the creation of users depends on what method is used. The most basic method is to use NIS. If there is no requirement of keeping user authentification in sync with some external service like e.g. <firstterm>LDAP</firstterm> this is sufficient. A NIS database is setup during the initial installation process and cluster users can be authenticated against it. Creating accounts that should use other authentification mechanisms is more complex and beyond the scope of this guide. Some options are explained in the admin manual. Add a test user by executing a command like this:"
msgstr "クラスタ内のユーザ認証は様々な方法で可能です。従って、どのような方法を使用するかによって作成方法も異なります。一番基本的な方法はNISを使用する方法です。もし外部の<firstterm>LDAP</firstterm>サービス等とユーザ認証情報を同期する必要がない場合は、NISで十分です。NISデータベースは初期のインストールプロセスで作成され、それを用いてユーザが認証されます。他の認証方法のためのユーザ作成については、より複雑で本ガイドの範囲を超えます。いくつかのオプションついては管理マニュアルに記載しています。testユーザの作成は以下のコマンドで行います。"

msgid ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>adduser.sh -u test -n \"Test User\"</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 root@cl-head ~ #</prompt>\n"
"<command>adduser.sh -u test -n \"Test User\"</command>\n"
"       "

msgid "The behavior of the <application>adduser.sh</application> script can be customized in its configuration file <filename>/etc/qlustar/common/adduser.cf</filename>. It also contains the definition of the initial user password."
msgstr "<application>adduser.sh</application>スクリプトの動作は、<filename>/etc/qlustar/common/adduser.cf</filename>ファイルの編集によりカスタマイズできます。このファイルは初期ユーザのパスワードも記載しています。"

msgid "Compiling an MPI program"
msgstr "MPIプログラムのコンパイル"

msgid "<link xlink:href=\"http://www.mcs.anl.gov/research/projects/mpi/___blank___\">MPI (Message Passing Interface)</link> is the de facto standard for distributed parallel programming on Linux clusters. The default <firstterm>MPI</firstterm> variant in Qlustar is <firstterm>OpenMpi</firstterm> and is automatically installed in the default chroot during installation. You can test the correct installation of MPI with two small \"hello world\" test programs (one in C the other one in FORTRAN90) as the test user you created earlier: Login on the front-end node as this user and execute"
msgstr "<link xlink:href=\"http://www.mcs.anl.gov/research/projects/mpi/___blank___\">MPI (Message Passing Interface)</link>は、Linuxクラスタにおける分散並列プログラミングで業界標準のライブラリです。Qlustarの標準の<firstterm>MPI</firstterm>ライブラリは、<firstterm>OpenMpi</firstterm>です。インストール中にデフォルトのchrootに自動的にインストールされています。2つの小さな\"hello world\"テストプログラム（1つはCでもう1つはFORTRAN90プログラム）を用いて、MPIが正しくインストールされているかテストすることができます。先ほど作成したtestユーザでフロントエンドノードにログインし、以下を実行して下さい。"

msgid ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>mpicc.openmpi-gcc -o hello-world-c hello-world.c</command>\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>mpif90.openmpi-gcc -o hello-world-f hello-world.f90</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>mpicc.openmpi-gcc -o hello-world-c hello-world.c</command>\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>mpif90.openmpi-gcc -o hello-world-f hello-world.f90</command>\n"
"       "

msgid "After this you should have created two executables. Check it with"
msgstr "実行後、2つの実行形式ファイルが生成されます。確認するには以下を実行して下さい。"

msgid ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>ls -l hello-world-?</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>ls -l hello-world-?</command>\n"
"       "

msgid "Now we're prepared to test the queuing system with the two programs."
msgstr "これで生成された2つのプログラムでキューイングシステムをテストする準備が完了しました。"

msgid "Running an MPI Job"
msgstr "MPIジョブの実行"

msgid "Still logged in as the test user and assuming at least two demo nodes are started, we can submit the two \"hello world\" programs created previously as follows (commands are given for slurm):"
msgstr "testユーザでフロントエンドノードにログイン中で、少なくとも2つのデモノードが走っていることを想定しています。以下のコマンドで先ほど作成した2つの\"hello world\"プログラムをサブミットします（以下はslurm向けのコマンドです）。"

msgid ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>OMPI_MCA_btl=\"tcp,self\" salloc -N 2 --ntasks-per-node=2 -p demo \\nsrun hello-world-c</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>OMPI_MCA_btl=\"tcp,self\" salloc -N 2 --ntasks-per-node=2 -p demo \\nsrun hello-world-c</command>\n"
"       "

msgid "This will run the job interactively on 2 nodes with 2 processes each (total of 4 processes). You should obtain an output like this:"
msgstr "これで2つのノード上で、1ノードにつき2プロセスのインタラクティブジョブ（合計4プロセス）が実行されます。以下のような出力が得られます。"

msgid ""
"\n"
"salloc: Granted job allocation 19\n"
"cpu_bind=NULL - beo-201, task  0  0 [13959]: mask 0x1\n"
"cpu_bind=NULL - beo-202, task  3  1 [13607]: mask 0x2\n"
"cpu_bind=NULL - beo-202, task  2  0 [13606]: mask 0x1\n"
"cpu_bind=NULL - beo-201, task  1  1 [13960]: mask 0x2\n"
"Hello world from process 1 of 4\n"
"Hello world from process 3 of 4\n"
"Hello world from process 0 of 4\n"
"Hello world from process 2 of 4\n"
"salloc: Relinquishing job allocation 19\n"
"salloc: Job allocation 19 has been revoked.\n"
"	"
msgstr ""
"\n"
"salloc: Granted job allocation 19\n"
"cpu_bind=NULL - beo-201, task  0  0 [13959]: mask 0x1\n"
"cpu_bind=NULL - beo-202, task  3  1 [13607]: mask 0x2\n"
"cpu_bind=NULL - beo-202, task  2  0 [13606]: mask 0x1\n"
"cpu_bind=NULL - beo-201, task  1  1 [13960]: mask 0x2\n"
"Hello world from process 1 of 4\n"
"Hello world from process 3 of 4\n"
"Hello world from process 0 of 4\n"
"Hello world from process 2 of 4\n"
"salloc: Relinquishing job allocation 19\n"
"salloc: Job allocation 19 has been revoked.\n"
"       "

msgid "The lines starting with <parameter>cpu_bind=NULL</parameter> are new in Qlustar 9 and appear due to the fact, that we have enabled the verbose setting for the configuration of <parameter>cpusets</parameter> under slurm. They indicate the binding (<emphasis role=\"bold\">core affinity</emphasis>) of the separate MPI tasks to the CPUs/cores of the compute nodes. This is done by the slurm <emphasis role=\"bold\">cgroup plugin</emphasis>, that is now enabled by default. Binding of processes to cores guarantees data (memory) locality and thus improves performance on <firstterm>NUMA systems</firstterm>, since access to local memory is faster than to remote memory. Note that essentially all modern multi-socket (CPU) systems have a NUMA architecture these days (<productname>Intel Xeon</productname>, <productname>AMD Opteron</productname>, ...), so this is relevant."
msgstr "<parameter>cpu_bind=NULL</parameter>で始まる行は、Qlustar9で新しく導入されました。slurm環境で<parameter>cpusets</parameter>の設定の表示を有効したためです。この行は、独立したMPIタスクの計算ノードのCPU/コアに対するbinding（<emphasis role=\"bold\">core affinity</emphasis>）を示しています。これはslurmの<emphasis role=\"bold\">cgroup plugin</emphasis>により実現しています。これは現在標準で有効になっています。プロセスをコアにバインドすることはデータ（メモリ）の局所性を保証し、<firstterm>NUMA systems</firstterm>におけるパフォーマンスを向上させます。ローカルのメモリアクセスは、リモートのメモリにアクセスするよりも高速なためです。昨今のマルチソケットシステムは基本的にNUMAアーキテクチャを採用しています（<productname>Intel Xeon</productname>, <productname>AMD Opteron</productname>, ...）、そのためこの機能は計算機のパフォーマンス向上に関与する機能なのです。"

msgid "Similarly the F90 version can be submitted as a batch job using the script <filename>hello-world-f90-slurm.sh</filename> (to see the output, execute <command>cat slurm-<replaceable>job#</replaceable>.out</command> after the job has finished):"
msgstr "F90バージョンも同様に<filename>hello-world-f90-slurm.sh</filename>スクリプトを用いてバッチジョブとして投入可能です（出力を確認するには、<command>cat slurm-<replaceable>job#</replaceable>.out</command>をジョブ完了後に実行して下さい）。"

msgid ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>sbatch -N 2 --ntasks-per-node=2 -p demo hello-world-f90-slurm.sh</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>sbatch -N 2 --ntasks-per-node=2 -p demo hello-world-f90-slurm.sh</command>\n"
"       "

msgid "Note that the environment variable <envar>OMPI_MCA_btl=\"tcp,self\"</envar> is used in the above two examples to prevent error messages from not finding an <firstterm>Infiniband network</firstterm>. The latter would otherwise occur, because we compile OpenMPI to use an IB network per default and if not found, a TCP network is used as backup. <firstterm>TCP</firstterm> can also be set as the default in the OpenMPI config file (in the chroot, typically under <filename>/srv/apps/chroots/trusty/etc/openmpi/x.y.z/openmpi-mca-params.conf</filename>) by adding the entry:"
msgstr "上の2つの例で使用した<envar>OMPI_MCA_btl=\"tcp,self\"</envar>環境変数は、<firstterm>Infiniband network</firstterm>が見つからないというエラメッセージが出力されることを予防するために使用しています。これは、標準でOpenMPIをIB networkを使用するようコンパイルしているために起こります。もしIB networkが見つからない場合は、TCP networkがバックアップとして使用されます。設定ファイル（chrootの中で、通常以下のファイル名<filename>/srv/apps/chroots/trusty/etc/openmpi/x.y.z/openmpi-mca-params.conf</filename>）を編集することで<firstterm>TCP</firstterm>を標準として使用するよう設定可能です。TCPを標準とするには以下のエントリーを追加します。"

msgid ""
"\n"
"btl = tcp,self\n"
"	"
msgstr ""
"\n"
"btl = tcp,self\n"
"       "

msgid "Running the Linpack benchmark"
msgstr "Linpackベンチマークの実行"

msgid "The <link xlink:href=\"http://www.top500.org/project/linpack/___blank___\">Linpack benchmark</link> s used to classify supercomputers in the The <link xlink:href=\"http://www.top500.org/___blank___\">Top 500</link> list. That's why on most clusters, it's probably run as one of the first parallel programs to check functionality, stability and performance. Qlustar comes with an optimized pre-compiled version of <firstterm>Linpack</firstterm> (using a current version of the <link xlink:href=\"http://www.openblas.net/___blank___\">OpenBlas library</link>) , and a script to auto-generate the necessary input file given the number of nodes, processes per node and total amount of RAM for the run."
msgstr "<link xlink:href=\"http://www.top500.org/project/linpack/___blank___\">Linpack benchmark</link>は、<link xlink:href=\"http://www.top500.org/___blank___\">Top 500</link>リストの中のスーパーコンピュータを分類するために使用されていました。そのような理由でだいたいのクラスタは、機能性・安定性・パフォーマンスを確認するために最初のプログラムとしてLinpack benchmarkを実行します。Qlustarは最適化コンパイル済みの<firstterm>Linpack</firstterm>（現バージョンの<link xlink:href=\"http://www.openblas.net/___blank___\">OpenBlas library</link>を使用）と、ノード数・ノートあたりのプロセス数・合計のメモリ量などを記載したLinpackの実行に必要なインプットファイルを自動生成するスクリプトを同梱しています。"

msgid "The test user has some pre-defined shell aliases to simplify the submission of Linpack jobs. Type alias to see what's available. They are defined in <filename>$HOME/.bash/alias</filename>. Example submission (assuming you have 4 running demo nodes):"
msgstr "testユーザは、Linpackジョブの投入を簡潔にするためのエイリアスがいくつか設定済みです。aliasとタイプしてどのようなエイリアスが設定済みか確認してみて下さい。それらのエイリアスは、<filename>$HOME/.bash/alias</filename>に設定されています。投入の例（4つのデモノードが走っていることを想定しています）"

msgid ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>linp-4-demo-nodes</command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>linp-4-demo-nodes</command>\n"
"       "

msgid "Check that the job is started (output should be similar):"
msgstr "ジョブが開始したか確認して下さい（以下に似たものが出力されるはずです）"

msgid ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>squeue</command>\n"
"JOBID PARTITION     NAME     USER  ST  TIME  NODES  NODELIST(REASON)\n"
"   27      demo linstres     test   R  2:46      4     beo-[201-204]\n"
"	"
msgstr ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>squeue</command>\n"
"JOBID PARTITION     NAME     USER  ST  TIME  NODES  NODELIST(REASON)\n"
"   27      demo linstres     test   R  2:46      4     beo-[201-204]\n"
"       "

msgid "ssh to one of the nodes in the <literal>NODELIST</literal> and check with top that Linpack is running at full steam, like:"
msgstr "<literal>NODELIST</literal>の中の1つのノードにsshして、topコマンドでLinpackが実行中か確認して下さい。"

msgid ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>top</command>\n"
"  PID USER  PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                 \n"
"18307 test  20   0  354m 280m 2764 R  100 28.0   6:42.92 xhpl-openblas           \n"
"18306 test  20   0  354m 294m 2764 R   99 29.3   6:45.09 xhpl-openblas\n"
"	"
msgstr ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>top</command>\n"
"  PID USER  PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                 \n"
"18307 test  20   0  354m 280m 2764 R  100 28.0   6:42.92 xhpl-openblas           \n"
"18306 test  20   0  354m 294m 2764 R   99 29.3   6:45.09 xhpl-openblas\n"
"       "

msgid "You can check the output of each Linpack run in the files: <filename>$HOME/bench/hpl/run/job-<replaceable>jobid</replaceable>-*/openblas/job-<replaceable>jobid</replaceable>-*-<replaceable>run#</replaceable>.out</filename> where <replaceable>jobid</replaceable> is the slurm <literal>JOBID</literal> (see the squeue command above) and <replaceable>run#</replaceable> is an integer starting from 1. The way the script is designed, it will run indefinitely, restarting Linpack in an infinite loop. So to stop it, you need to cancel the job like"
msgstr "各Linpack実行後の出力は、<filename>$HOME/bench/hpl/run/job-<replaceable>jobid</replaceable>-*/openblas/job-<replaceable>jobid</replaceable>-*-<replaceable>run#</replaceable>.out</filename>ファイルで確認できます。<replaceable>jobid</replaceable>はslurmの<literal>JOBID</literal>（上のsqueueコマンドを見て下さい）、<replaceable>run#</replaceable>は1から始まる整数です。スクリプトに記述している通り、ここで投入したジョブは無期限で実行しています。ループ数の期限つきでLinpackを再度実行する場合は、以下のコマンドを実行して実行中のジョブをキャンセルする必要があります。"

msgid ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>scancel <replaceable>jobid</replaceable></command>\n"
"	"
msgstr ""
"\n"
"<prompt>0 testuser@cl-front ~ $</prompt>\n"
"<command>scancel <replaceable>jobid</replaceable></command>\n"
"       "

