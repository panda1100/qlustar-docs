<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title xmlns:d="http://docbook.org/ns/docbook">Administration Manual</title><meta xmlns:d="http://docbook.org/ns/docbook" name="generator" content="publican v4.3.0" /><meta xmlns:d="http://docbook.org/ns/docbook" name="package" content="Qlustar_Cluster_OS-Administration_Manual-9.1-en-US-9.1-1" /><meta name="description" content="Qlustar is a complete Linux distribution based on Debian/Ubuntu. Its main purpose is to provide an Operating System (OS) that simplifies the setup, administration, monitoring and general use of HPC, Storage and Virtualization/Cloud Linux clusters. Hence we call Qlustar a Cluster OS." /><link rel="stylesheet" type="text/css" href="../../../../../chrome.css" /><link rel="stylesheet" type="text/css" href="../../../../../db4.css" /><link rel="stylesheet" type="text/css" href="../../../../../qlustar/en-US/css/brand.css" /><link rel="stylesheet" type="text/css" href="../../../../../print.css" media="print" /><script type="text/javascript" src="../../../../../jquery-1.7.1.min.js"></script><script type="text/javascript" src="../../../../labels.js"></script><script type="text/javascript" src="../../../../../toc.js"></script><script type="text/javascript">
		current_book = 'Administration_Manual';
		current_version = '9.1';
		current_product = 'Qlustar_Cluster_OS';
	   
                toc_path = '../../../..';
		loadMenu();
          </script></head><body class="toc_embeded "><div id="navigation"></div><div id="floatingtoc" class="hidden"></div><p id="title"><a class="left" href="https://qlustar.com"><img alt="Product Site" src="../../../../../qlustar/en-US/images//image_left.png" /></a><a class="right" href="http://qlustar.com/book/docs/main"><img alt="Documentation Site" src="../../../../../qlustar/en-US/images//image_right.png" /></a></p><ul class="docnav"><li class="home">Administration Manual</li></ul><div xml:lang="en-US" class="book" lang="en-US"><div class="titlepage"><div><div class="producttitle"><span xmlns:d="http://docbook.org/ns/docbook" class="productname">Qlustar Cluster OS</span> <span xmlns:d="http://docbook.org/ns/docbook" class="productnumber">9.1</span></div><div><h1 class="title producttitle"><a id="idm140006539932512"></a>Administration Manual</h1></div><div><h2 class="subtitle"> Manual for the configuration, customization and operation of Qlustar clusters. </h2></div><div><h3 class="corpauthor">
		<span class="inlinemediaobject"><img src="../../../../../qlustar/en-US/images/qlustar-logo.png" /></span>

	</h3></div><div><div xml:lang="en-US" class="authorgroup" lang="en-US">
	<div class="author"><h3 class="author"><span class="firstname">Qlustar</span> <span class="surname">Documentation Team</span></h3><div class="affiliation">
			<span xmlns:d="http://docbook.org/ns/docbook" class="orgname">Q-Leap Networks GmbH</span>

		</div><code class="email"><a class="email" href="mailto:qlustar-docs@q-leap.com">qlustar-docs@q-leap.com</a></code></div>
</div></div><div><div class="legalnotice"><a id="idm140006539929008"></a><h1 class="legalnotice">Legal Notice</h1>
	<div class="para">
		Copyright <span class="trademark"></span>Â©2015 Q-Leap Networks GmbH
	</div>
	<div class="para">
		This material may only be distributed subject to the terms and conditions set forth in the GNU Free Documentation License (GFDL), V1.2 or later (the latest version is presently available at http://www.gnu.org/licenses/fdl.txt).
	</div>
	<div class="para">
		The Qlustar license can be found at <code class="filename">/usr/share/qlustar/LICENSE.html</code> on an installed Qlustar head-node.
	</div>
</div></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p>
		<div class="para">
			Qlustar is a complete Linux distribution based on Debian/Ubuntu. Its main purpose is to provide an Operating System (OS) that simplifies the setup, administration, monitoring and general use of HPC, Storage and Virtualization/Cloud Linux clusters. Hence we call Qlustar a <span class="emphasis"><em>Cluster OS</em></span>.
		</div>

	</div></div></div></div><div class="toc"><dl class="toc"><dt><span class="preface"><a href="#idm140006539921168">Preface</a></span></dt><dd><dl><dt><span class="section"><a href="#idm140006539920944">1. Qlustar Document Conventions</a></span></dt><dd><dl><dt><span class="section"><a href="#idm140006539918784">1.1. Typographic Conventions</a></span></dt><dt><span class="section"><a href="#idm140006539917984">1.2. Pull-quote Conventions</a></span></dt><dt><span class="section"><a href="#idm140006539917728">1.3. Notes and Warnings</a></span></dt></dl></dd><dt><span class="section"><a href="#idm140006539861472">2. Feedback requested</a></span></dt></dl></dd><dt><span class="chapter"><a href="#admin-man-chap-introduction">1. Introduction</a></span></dt><dt><span class="chapter"><a href="#admin-man-chap-base-conf">2. Qlustar Base Configuration</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-network">2.1. Network Configuration and Services</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-ntwrk-config">2.1.1. Basic Network Configuration</a></span></dt><dt><span class="section"><a href="#admin-man-sect-dns">2.1.2. DNS</a></span></dt><dt><span class="section"><a href="#admin-man-sect-dhcp">2.1.3. DHCP</a></span></dt><dt><span class="section"><a href="#admin-man-sect-ip-msq">2.1.4. IP Masquerading (NAT)</a></span></dt><dt><span class="section"><a href="#admin-man-sect-time-server">2.1.5. Time Server</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-base-conf-serv">2.2. Basic Services</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-disk-part-file-sys">2.2.1. Disk Partitions and File-systems</a></span></dt><dt><span class="section"><a href="#admin-man-sect-nis">2.2.2. NIS</a></span></dt><dt><span class="section"><a href="#admin-man-sect-nfs-sect">2.2.3. NFS</a></span></dt><dt><span class="section"><a href="#admin-man-sect-auto-mounter">2.2.4. Automounter</a></span></dt><dt><span class="section"><a href="#admin-man-sect-ssh">2.2.5. SSH - Secure Shell</a></span></dt><dt><span class="section"><a href="#admin-man-sect-mail-server-postfix">2.2.6. Mail server - Postfix</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#admin-man-chap-hw-conf">3. Qlustar Hardware Configuration</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-hw-IB">3.1. Infiniband Networks</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-hw-IB-diag">3.1.1. IB Fabric Verification/Diagnosis</a></span></dt><dt><span class="section"><a href="#admin-man-sect-hw-IB-opensm">3.1.2. OpenSM Configuration</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#admin-man-chap-node-manage">4. Cluster Node Management</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-boot-process">4.1. Boot Process</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-comp-node-boot">4.1.1. Compute-node booting</a></span></dt><dt><span class="section"><a href="#admin-man-sect-tftp-boot-serv">4.1.2. TFTP Boot Server</a></span></dt><dt><span class="section"><a href="#admin-man-sect-RAM-disk-img">4.1.3. RAM-disk image</a></span></dt><dt><span class="section"><a href="#admin-man-sect-qluman-execd">4.1.4. QluMan Remote Execution Server</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-node-customize">4.2. Node Customization</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-node-options">4.2.1. Dynamic Configuration Settings</a></span></dt><dt><span class="section"><a href="#admin-man-sect-DHCP-client">4.2.2. DHCP-Client</a></span></dt><dt><span class="section"><a href="#admin-man-sect-config-dir">4.2.3. Cluster-wide Configuration Directory</a></span></dt><dt><span class="section"><a href="#admin-man-sect-NFS-boot-scripts">4.2.4. NFS boot scripts</a></span></dt><dt><span class="section"><a href="#admin-man-sect-add-dirs-files-links">4.2.5. Adding directories, files, links</a></span></dt><dt><span class="section"><a href="#admin-man-sect-hard-disc-init">4.2.6. Hard disc initialization</a></span></dt><dt><span class="section"><a href="#admin-man-sect-mail-trans-agent">4.2.7. Mail Transport Agent</a></span></dt><dt><span class="section"><a href="#admin-man-sect-infiniband">4.2.8. Infiniband</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-remote-control">4.3. Node Remote Control</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-ser-cons-para">4.3.1. Serial Console Parameter</a></span></dt><dt><span class="section"><a href="#admin-man-sect-ipmi-config">4.3.2. IPMI Configuration</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#admin-man-chap-monitoring">5. Monitoring Infrastructure</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-ganglia">5.1. Ganglia</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-monit-the-nodes">5.1.1. Monitoring the nodes</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-nagios">5.2. Nagios</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-nag-plug-ins">5.2.1. Nagios Plugins</a></span></dt><dt><span class="section"><a href="#admin-man-sect-head-node">5.2.2. Monitoring the head-node(s)</a></span></dt><dt><span class="section"><a href="#admin-man-sect-web-interface">5.2.3. Webinterface</a></span></dt><dt><span class="section"><a href="#admin-man-sect-restart">5.2.4. Restart</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#admin-man-chap-gen-adm">6. General Administration Tasks</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-user-manage">6.1. User Management</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-add-user">6.1.1. Adding User Accounts</a></span></dt><dt><span class="section"><a href="#admin-man-sect-rem-user">6.1.2. Removing User Accounts</a></span></dt><dt><span class="section"><a href="#admin-man-sect-man-user-restr">6.1.3. Managing user restrictions</a></span></dt><dt><span class="section"><a href="#admin-man-sect-shell-stp">6.1.4. Shell Setup</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-stor-man">6.2. Storage Management</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-raid">6.2.1. Raid</a></span></dt><dt><span class="section"><a href="#admin-man-sect-vol-man">6.2.2. Logical Volume Management</a></span></dt><dt><span class="section"><a href="#admin-man-sect-ZFS">6.2.3. Zpools and ZFS</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-os-pkg-man">6.3. OS Package Management</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-pkg-srcs">6.3.1. Package sources</a></span></dt><dt><span class="section"><a href="#admin-man-sect-dpkg">6.3.2. dpkg</a></span></dt><dt><span class="section"><a href="#admin-man-sect-apt">6.3.3. apt</a></span></dt><dt><span class="section"><a href="#admin-man-sect-debian-pkg-altern">6.3.4. Debian Package Alternatives</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#chap-Updating-Qlustar">7. Updating Qlustar</a></span></dt><dd><dl><dt><span class="section"><a href="#sec-Qlustar">7.1. Qlustar updates</a></span></dt><dd><dl><dt><span class="section"><a href="#updating-headnodes">7.1.1. Updating the head-node(s)</a></span></dt><dt><span class="section"><a href="#updating-chroots">7.1.2. Updating the chroot(s)</a></span></dt><dt><span class="section"><a href="#updating-nodes">7.1.3. Updating the nodes</a></span></dt></dl></dd></dl></dd><dt><span class="appendix"><a href="#idm140006538772208">A. Revision History</a></span></dt><dt><span class="index"><a href="#idm140006538754976">Index</a></span></dt></dl></div>
	
	 <div xml:lang="en-US" class="preface" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="idm140006539921168">
      â </a>Preface</h1></div></div></div>
	
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="idm140006539920944">
      â </a>1.Â Qlustar Document Conventions</h2></div></div></div>
	
	 <div class="para">
		Qlustar manuals use the following conventions to highlight certain words and phrases and draw attention to specific pieces of information.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm140006539918784">
      â </a>1.1.Â Typographic Conventions</h3></div></div></div>
		
		 <div class="para">
			Four typographic conventions are used to call attention to specific words and phrases. These conventions, and the circumstances they apply to, are as follows.
		</div>
		 <div class="para">
			<code class="literal">Mono-spaced Bold</code>
		</div>
		 <div class="para">
			Used to highlight system input, including shell commands, file names and paths. Also used to highlight keys and key combinations. For example:
		</div>
		 <div class="blockquote"><blockquote class="blockquote">
			<div class="para">
				To see the contents of the file <code class="filename">my_next_bestselling_novel</code> in your current working directory, enter the <code class="command">cat my_next_bestselling_novel</code> command at the shell prompt and press <span class="keycap"><strong>Enter</strong></span> to execute the command.
			</div>

		</blockquote></div>
		 <div class="para">
			The above includes a file name, a shell command and a key, all presented in mono-spaced bold and all distinguishable thanks to context.
		</div>
		 <div class="para">
			Key combinations can be distinguished from an individual key by the plus sign that connects each part of a key combination. For example:
		</div>
		 <div class="blockquote"><blockquote class="blockquote">
			<div class="para">
				Press <span class="keycap"><strong>Enter</strong></span> to execute the command.
			</div>
			 <div class="para">
				Press <span class="keycap"><strong>Ctrl</strong></span>+<span class="keycap"><strong>Alt</strong></span>+<span class="keycap"><strong>F2</strong></span> to switch to a virtual terminal.
			</div>

		</blockquote></div>
		 <div class="para">
			The first example highlights a particular key to press. The second example highlights a key combination: a set of three keys pressed simultaneously.
		</div>
		 <div class="para">
			If source code is discussed, class names, methods, functions, variable names and returned values mentioned within a paragraph will be presented as above, in <code class="literal">mono-spaced bold</code>. For example:
		</div>
		 <div class="blockquote"><blockquote class="blockquote">
			<div class="para">
				File-related classes include <code class="classname">filesystem</code> for file systems, <code class="classname">file</code> for files, and <code class="classname">dir</code> for directories. Each class has its own associated set of permissions.
			</div>

		</blockquote></div>
		 <div class="para">
			<span class="application"><strong>Proportional Bold</strong></span>
		</div>
		 <div class="para">
			This denotes words or phrases encountered on a system, including application names; dialog-box text; labeled buttons; check-box and radio-button labels; menu titles and submenu titles. For example:
		</div>
		 <div class="blockquote"><blockquote class="blockquote">
			<div class="para">
				Choose <span class="guimenu"><strong>System</strong></span> â <span class="guisubmenu"><strong>Preferences</strong></span> â <span class="guimenuitem"><strong>Mouse</strong></span> from the main menu bar to launch <span class="application"><strong>Mouse Preferences</strong></span>. In the <span class="guilabel"><strong>Buttons</strong></span> tab, select the <span class="guilabel"><strong>Left-handed mouse</strong></span> check box and click <span class="guibutton"><strong>Close</strong></span> to switch the primary mouse button from the left to the right (making the mouse suitable for use in the left hand).
			</div>
			 <div class="para">
				To insert a special character into a <span class="application"><strong>gedit</strong></span> file, choose <span class="guimenu"><strong>Applications</strong></span> â <span class="guisubmenu"><strong>Accessories</strong></span> â <span class="guimenuitem"><strong>Character Map</strong></span> from the main menu bar. Next, choose <span class="guimenu"><strong>Search</strong></span> â <span class="guimenuitem"><strong>Findâ¦</strong></span> from the <span class="application"><strong>Character Map</strong></span> menu bar, type the name of the character in the <span class="guilabel"><strong>Search</strong></span> field and click <span class="guibutton"><strong>Next</strong></span>. The character you sought will be highlighted in the <span class="guilabel"><strong>Character Table</strong></span>. Double-click this highlighted character to place it in the <span class="guilabel"><strong>Text to copy</strong></span> field and then click the <span class="guibutton"><strong>Copy</strong></span> button. Now switch back to your document and choose <span class="guimenu"><strong>Edit</strong></span> â <span class="guimenuitem"><strong>Paste</strong></span> from the <span class="application"><strong>gedit</strong></span> menu bar.
			</div>

		</blockquote></div>
		 <div class="para">
			The above text includes application names; system-wide menu names and items; application-specific menu names; and buttons and text found within a GUI interface, all presented in proportional bold and all distinguishable by context.
		</div>
		 <div class="para">
			<code class="command"><em class="replaceable">Mono-spaced Bold Italic</em></code> or <span class="application"><strong><em class="replaceable">Proportional Bold Italic</em></strong></span>
		</div>
		 <div class="para">
			Whether mono-spaced bold or proportional bold, the addition of italics indicates replaceable or variable text. Italics denotes text you do not input literally or displayed text that changes depending on circumstance. For example:
		</div>
		 <div class="blockquote"><blockquote class="blockquote">
			<div class="para">
				To connect to a remote machine using ssh, type <code class="command">ssh <em class="replaceable">username</em>@<em class="replaceable">domain.name</em></code> at a shell prompt. If the remote machine is <code class="filename">example.com</code> and your username on that machine is john, type <code class="command">ssh john@example.com</code>.
			</div>
			 <div class="para">
				The <code class="command">mount -o remount <em class="replaceable">file-system</em></code> command remounts the named file system. For example, to remount the <code class="filename">/home</code> file system, the command is <code class="command">mount -o remount /home</code>.
			</div>
			 <div class="para">
				To see the version of a currently installed package, use the <code class="command">rpm -q <em class="replaceable">package</em></code> command. It will return a result as follows: <code class="command"><em class="replaceable">package-version-release</em></code>.
			</div>

		</blockquote></div>
		 <div class="para">
			Note the words in bold italics above: username, domain.name, file-system, package, version and release. Each word is a placeholder, either for text you enter when issuing a command or for text displayed by the system.
		</div>
		 <div class="para">
			Aside from standard usage for presenting the title of a work, italics denotes the first use of a new and important term. For example:
		</div>
		 <div class="blockquote"><blockquote class="blockquote">
			<div class="para">
				Publican is a <em class="firstterm">DocBook</em> publishing system.
			</div>

		</blockquote></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm140006539917984">
      â </a>1.2.Â Pull-quote Conventions</h3></div></div></div>
		
		 <div class="para">
			Terminal output and source code listings are set off visually from the surrounding text.
		</div>
		 <div class="para">
			Output sent to a terminal is set in <code class="computeroutput">mono-spaced roman</code> and presented thus:
		</div>
		 
<pre class="screen">books        Desktop   documentation  drafts  mss    photos   stuff  svn
books_tests  Desktop1  downloads      images  notes  scripts  svgs
</pre>
		 <div class="para">
			Commands to be executed on certain nodes of a cluster or the admins workstation are indicated by using descriptive shell prompts including user and hostname. Note that by default, the shell prompt on Qlustar nodes always ends in the newline character, thus commands are typed on the line following the prompt. As mentioned above, the command itself is shown in <code class="computeroutput">mono-spaced bold</code> and the output of a command in <code class="computeroutput">mono-spaced roman</code>. Examples:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code>
<code class="command">echo "I'm executed by root on a head-node"</code>
I'm executed by root on a head-node
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@beo-01 ~ #</code>
<code class="command">echo "I'm executed by root on a compute node"</code>
I'm executed by root on a compute node
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@sn-1 ~ #</code>
<code class="command">echo "I'm executed by root on a storage node"</code>
I'm executed by root on a storage node
</pre>
		 
<pre class="screen">
<code class="prompt">0 user@workstation ~ $ </code>
<code class="command">echo "I'm executed by user admin on the admins workstation"</code>
I'm executed by user admin on the admins workstation
</pre>
		 <div class="para">
			Source-code listings are also set in <code class="computeroutput">mono-spaced roman</code> but add syntax highlighting as follows:
		</div>
		 <pre xml:lang="en-US" class="programlisting" lang="en-US"><span xmlns="" class="line">â</span>package org.<span xmlns="" class="perl_Function">jboss</span>.<span xmlns="" class="perl_Function">book</span>.<span xmlns="" class="perl_Function">jca</span>.<span xmlns="" class="perl_Function">ex1</span>;
<span xmlns="" class="line">â</span>
<span xmlns="" class="line">â</span><span xmlns="" class="perl_Keyword">import javax.naming.InitialContext;</span>
<span xmlns="" class="line">â</span>
<span xmlns="" class="line">â</span><span xmlns="" class="perl_Keyword">public</span> <span xmlns="" class="perl_Keyword">class</span> ExClient
<span xmlns="" class="line">â</span>{
<span xmlns="" class="line">â</span>   <span xmlns="" class="perl_Keyword">public</span> <span xmlns="" class="perl_DataType">static</span> <span xmlns="" class="perl_DataType">void</span> <span xmlns="" class="perl_Function">main</span>(String args[]) 
<span xmlns="" class="line">â</span>       <span xmlns="" class="perl_Keyword">throws</span> Exception
<span xmlns="" class="line">â</span>   {
<span xmlns="" class="line">â</span>      InitialContext iniCtx = <span xmlns="" class="perl_Keyword">new</span> InitialContext();
<span xmlns="" class="line">â</span>      Object         ref    = iniCtx.<span xmlns="" class="perl_Function">lookup</span>(<span xmlns="" class="perl_String">"EchoBean"</span>);
<span xmlns="" class="line">â</span>      EchoHome       home   = (EchoHome) ref;
<span xmlns="" class="line">â</span>      Echo           echo   = home.<span xmlns="" class="perl_Function">create</span>();
<span xmlns="" class="line">â</span>
<span xmlns="" class="line">â</span>      System.<span xmlns="" class="perl_Function">out</span>.<span xmlns="" class="perl_Function">println</span>(<span xmlns="" class="perl_String">"Created Echo"</span>);
<span xmlns="" class="line">â</span>
<span xmlns="" class="line">â</span>      System.<span xmlns="" class="perl_Function">out</span>.<span xmlns="" class="perl_Function">println</span>(<span xmlns="" class="perl_String">"Echo.echo('Hello') = "</span> + echo.<span xmlns="" class="perl_Function">echo</span>(<span xmlns="" class="perl_String">"Hello"</span>));
<span xmlns="" class="line">â</span>   }
<span xmlns="" class="line">â</span>}</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm140006539917728">
      â </a>1.3.Â Notes and Warnings</h3></div></div></div>
		
		 <div class="para">
			Finally, we use three visual styles to draw attention to information that might otherwise be overlooked.
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				Notes are tips, shortcuts or alternative approaches to the task at hand. Ignoring a note should have no negative consequences, but you might miss out on a trick that makes your life easier.
			</div>

		</div></div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="important"><div class="admonition_header"><p><strong>Important</strong></p></div><div class="admonition">
			<div class="para">
				Important boxes detail things that are easily missed: configuration changes that only apply to the current session, or services that need restarting before an update will apply. Ignoring a box labeled âImportantâ will not cause data loss but may cause irritation and frustration.
			</div>

		</div></div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="warning"><div class="admonition_header"><p><strong>Warning</strong></p></div><div class="admonition">
			<div class="para">
				Warnings should not be ignored. Ignoring warnings will most likely cause data loss.
			</div>

		</div></div>

	</div>
</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="idm140006539861472">
      â </a>2.Â Feedback requested</h2></div></div></div>
	
	<a id="idm140006539860352" class="indexterm"></a>
	<div class="para">
		Contact <code class="email"><a class="email" href="mailto:qlustar-docs@qlustar.com">qlustar-docs@qlustar.com</a></code> to report errors or missing pieces in this documentation.
	</div>
</div>
</div>
	 <div xml:lang="en-US" class="chapter" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="admin-man-chap-introduction">
      â </a>ChapterÂ 1.Â Introduction</h1></div></div></div>
	
	 <div class="para">
		Qlustar is a full-fledged Linux distribution based on Debian/Ubuntu. Its main focus is to provide an Operating System (OS) that simplifies the setup, administration, monitoring and general use of Linux clusters. Hence we call it a <em class="firstterm">Cluster OS.</em><a id="idm140006539856112" class="indexterm"></a>
	</div>
	 <div class="para">
		A Qlustar cluster typically consists of one or more <em class="firstterm">head-nodes</em>, <a id="idm140006539854576" class="indexterm"></a> and a larger number of compute-, storage- or cloud-nodes (in this manual we usually refer to these latter nodes simply as compute-nodes). In an HPC cluster, it is highly advisable to separate user login sessions onto dedicated <em class="firstterm">Front-End (FE) nodes</em><a id="idm140006539853568" class="indexterm"></a>. This leads to higher stability and security of the whole system, since then the head-node(s) (its most critical component), are not subject to problems arising from uncontrolled user activity. The FE node(s) can run on real physical hardware or (especially on small clusters with less activity) in a virtual machine (VM).
	</div>
	 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
		<div class="para">
			Qlustar has an installation option that allows the automatic setup/configuration of a KVM FE node.
		</div>

	</div></div>
	 <div class="para">
		For clusters with advanced file I/O performance requirements, the basic Qlustar configuration providing an NFS based setup to serve user home directories will be insufficient. In this case, a parallel file system like <em class="firstterm">Lustre</em> or <em class="firstterm">BeeGFS</em> will be needed. With QluMan, it's easy to add storage nodes to a cluster, that will be able to serve Lustre MDTs and OSTs or BeeGFS server components (if required also in a fail-safe high-availability configuration).
	</div>
	 <div class="para">
		Usually, all nodes in a cluster are connected through one or more dedicated internal Ethernet networks. If fast inter-node communication is required, an additional high-speed interconnect network like <em class="firstterm">Infiniband</em> may be used.
	</div>
	 <div class="para">
		<div class="mediaobject" align="center"><img src="./images/hpc-cluster-basic.png" align="middle" width="300" alt="Basic Qlustar HPC Cluster" /><div class="caption">
			<div class="para">
				Schematic view of a basic Qlustar HPC Cluster.
			</div>
			</div></div>
		 Most of the time, compute and cloud nodes are stripped-down servers with SATA hard disk drives (sometimes also disk-less), often without keyboard and mouse connection. These days, servers typically have a remote management interface (<em class="firstterm">IPMI</em>), that allows to perform most hardware related tasks of a node (like reset, power cycling etc.) over the network. In addition, IPMI provides remote access to a nodes console.
	</div>
	 <div class="para">
		The above figure shows a schematic description of the components building a basic HPC Cluster. The head-node typically requires a more powerful hardware configuration than the compute nodes to guarantee higher availability and to accommodate central disk storage. A Gigabit Ethernet and/or Infiniband network are the most common network interconnects of HPC clusters today.
	</div>
	 <div class="para">
		<div class="mediaobject" align="center"><img src="./images/hpc-cluster-advanced.png" align="middle" width="400" alt="Advanced Qlustar HPC Cluster" /><div class="caption">
			<div class="para">
				Schematic view of an advanced Qlustar HPC Cluster.
			</div>
			</div></div>
		 While the above entry-level hardware configurations will be sufficient for departmental clusters, a compute center will often have to provide a system with guaranteed up-time, scalable file I/O, as well as a high throughput / low latency network to satisfy the needs of demanding users. A schematic description of a hardware setup for such a scenario is shown in the following figure. Qlustar is equally capable to deploy such advanced configurations with little effort.
	</div>
</div>
	 <div xml:lang="en-US" class="chapter" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="admin-man-chap-base-conf">
      â </a>ChapterÂ 2.Â Qlustar Base Configuration</h1></div></div></div><div class="toc"><dl class="toc"><dt><span class="section"><a href="#admin-man-sect-network">2.1. Network Configuration and Services</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-ntwrk-config">2.1.1. Basic Network Configuration</a></span></dt><dt><span class="section"><a href="#admin-man-sect-dns">2.1.2. DNS</a></span></dt><dt><span class="section"><a href="#admin-man-sect-dhcp">2.1.3. DHCP</a></span></dt><dt><span class="section"><a href="#admin-man-sect-ip-msq">2.1.4. IP Masquerading (NAT)</a></span></dt><dt><span class="section"><a href="#admin-man-sect-time-server">2.1.5. Time Server</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-base-conf-serv">2.2. Basic Services</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-disk-part-file-sys">2.2.1. Disk Partitions and File-systems</a></span></dt><dt><span class="section"><a href="#admin-man-sect-nis">2.2.2. NIS</a></span></dt><dt><span class="section"><a href="#admin-man-sect-nfs-sect">2.2.3. NFS</a></span></dt><dt><span class="section"><a href="#admin-man-sect-auto-mounter">2.2.4. Automounter</a></span></dt><dt><span class="section"><a href="#admin-man-sect-ssh">2.2.5. SSH - Secure Shell</a></span></dt><dt><span class="section"><a href="#admin-man-sect-mail-server-postfix">2.2.6. Mail server - Postfix</a></span></dt></dl></dd></dl></div>
	
	 <a id="idm140006539838896" class="indexterm"></a>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-network">
      â </a>2.1.Â Network Configuration and Services</h2></div></div></div>
	
	 <a id="idm140006539836624" class="indexterm"></a>
	 <a id="idm140006539835856" class="indexterm"></a>
	 <div class="para">
		This section describes the basic network configuration and services of a Qlustar cluster.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-ntwrk-config">
      â </a>2.1.1.Â Basic Network Configuration</h3></div></div></div>
		
		 <a id="idm140006539833712" class="indexterm"></a>
		 <div class="para">
			The IP configuration of the head-node is defined in the file <code class="filename">/etc/network/interfaces</code> (type <code class="command">man interfaces</code> for details), while <code class="filename">/etc/resolv.conf</code> is the key-control file regarding the DNS configuration (see <a class="xref" href="#admin-man-sect-dns">SectionÂ 2.1.2, âDNSâ</a>). Network interfaces can be brought up or down using the commands <code class="command">ifup</code> and <code class="command">ifdown</code> (see also the corresponding man pages). Typically, the head-node has at least two network interfaces, one for the external LAN and one for the internal cluster network. For all cluster-internal networks (boot/NFS, optional Infiniband and/or IPMI)<a id="idm140006539829296" class="indexterm"></a>, unofficial (not routed) IP addresses are used, usually in the range <em class="parameter"><code>192.168.x.0/255.255.255.0</code></em>, while for larger clusters the range <em class="parameter"><code>172.16.y.0/255.255.0.0</code></em> is often used. In the latter case, <em class="parameter"><code>y</code></em> might indicate the rack number.
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				The cluster-internal network ranges can be conveniently chosen during installation.
			</div>

		</div></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-dns">
      â </a>2.1.2.Â DNS</h3></div></div></div>
		
		 <a id="idm140006539825152" class="indexterm"></a>
		 <div class="para">
			This section describes how to configure the Domain Name System (DNS).
		</div>
		 <div class="para">
			The file <code class="filename">/etc/resolv.conf</code> contains the addresses of the DNS servers to use and a list of domain names to search when looking up a short (not fully qualified) hostname. Example:
		</div>
		 
<pre class="screen">search your.domain
nameserver 10.0.0.252
nameserver 10.0.0.253
</pre>
		 <div class="para">
			where 10.0.0.252 and 10.0.0.253 are the addresses of the DNS servers and your.domain is the DNS domain name.
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				Recent versions of Qlustar allow the DNS configuration to be configured directly in <code class="filename">/etc/network/interfaces</code>. To make use of this, <em class="parameter"><code>dns-</code></em> option lines need to be added to the relevant iface stanza. The following option names are supported: <em class="parameter"><code>dns-nameservers</code></em>, <em class="parameter"><code>dns-search</code></em>, and <em class="parameter"><code>dns-sortlist</code></em>. Get more details on these options by executing <code class="command">man resolvconf</code>.The <em class="parameter"><code>dns-nameservers</code></em> entry is added and configured automatically during installation.
			</div>

		</div></div>
		 <div class="para">
			If the compute-nodes also need DNS access, they should use the head-node(s) as a forward-only DNS server. In that case the package <span class="package">bind9</span> should be installed and the addresses of the DNS servers be entered in the file <code class="filename">/etc/bind/named.conf.options</code>. Example:
		</div>
		 
<pre class="screen">
options {
  ...
  forwarders {
    10.0.0.252;
    10.0.0.253;
  };
  ...
</pre>
		 <div class="para">
			The compute-nodes must then use the cluster-internal address of the head-node (192.168.52.254 in the example below) as their DNS server. This and the DNS domain name have to be entered in the DHCP template file of QluMan as follows, so that the compute-nodes may receive the required information via DHCP:
		</div>
		 
<pre class="screen">...
option domain-name "your.domain";
option domain-name-servers 192.168.52.254;
...
</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-dhcp">
      â </a>2.1.3.Â DHCP</h3></div></div></div>
		
		 <a id="idm140006539814208" class="indexterm"></a>
		 <div class="para">
			The IP configuration of the compute-nodes is done using the Dynamic Host Control Protocol (DHCP). The DHCP server is not only responsible to automatically supply the IP address and netmask to these nodes, but also additional configuration options like the gateway and DNS server addresses, DNS domain name and many other parameters. The DHCP server configuration file is <code class="filename">/etc/dhcp/dhcpd.conf</code> (type <code class="command">man dhcpd.conf</code>) and is auto-generated by QluMan. There is a general or global section and a per node section in this file. The global section determines values that are to be applied to all the nodes registered within the <code class="filename">dhcpd.conf</code> file. On the contrary, the contents of a node section applies only to that specific machine and is created from the corresponding host info of the QluMan database. The following listing shows the default global section that is automatically generated by QluMan during the installation of a Qlustar cluster:
		</div>
		 
<pre class="screen">option qlustar-cfgmnt code 132 = text;
option qlustar-cfgmnt "-t nfs -o rw,hard,intr 192.168.52.1:/srv/ql-common";
next-server 192.168.52.1;
filename "pxelinux.0";
option nis-domain "beo-cluster";
option nis-servers 192.168.52.1;
option routers 192.168.52.1;
option subnet-mask 255.255.255.0;
option ntp-servers 192.168.52.1;
option lpr-servers 192.168.52.1;
subnet 192.168.52.0 netmask 255.255.255.0 {
}
</pre>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				If a cluster has additional internal networks (e.g. Infiniband), the IP address of a node in that network is derived from its basic DHCP address and set automatically during boot. The addresses of additional networks can be specified during installation and in QluMan. Check the <a href="../QluMan_Guide/index.html#sec-DHCP-Config___blank___">QluMan Guide</a> for more details.
			</div>

		</div></div>
		 <div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="admin-man-sect-spcl-dhcp-opt">
      â </a>2.1.3.1.Â Special DHCP options</h4></div></div></div>
			
			 <a id="idm140006539807056" class="indexterm"></a>
			 <div class="para">
				You can set the name of the auto-mount master map with the DHCP option 128 (the default name is auto.master). To modify it, include a line in the <code class="filename">dhcpd.conf</code> header as follows:
			</div>
			 
<pre class="screen">
option option-128 code 128 = text;
option option-128 "auto.master";
</pre>

		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-ip-msq">
      â </a>2.1.4.Â IP Masquerading (NAT)</h3></div></div></div>
		
		 <a id="idm140006539803104" class="indexterm"></a>
		 <div class="para">
			IP masquerading (NAT) is configured by default on the head-node(s) during installation, to allow the compute-nodes direct TCP/IP connections to machines outside of the internal cluster network. This could be necessary e.g., when applications running on the compute-nodes need to contact a license server in the public LAN. All IP packets with unofficial sender IP addresses and a destination in the public LAN are then translated by the head-node to packets with its own official IP address. When a reply packet arrives, it is translated back to the unofficial IP address of the originating node inside of the cluster. The head-node works as a router in this case. The following section in <code class="filename">/etc/network/interfaces</code> shows, how masquerading is activated on boot and disabled on shutdown:
		</div>
		 
<pre class="screen">
iface eth1 inet static
  address 4.4.4.123
  netmask 255.255.0.0
  broadcast 4.4.255.255
  gateway 4.4.255.254
  up iptables -t nat -A POSTROUTING -s 192.168.52.0/24 \
    -o eth1 -j MASQUERADE
  down iptables -t nat -D POSTROUTING -s 192.168.52.0/24 \
    -o eth1 -j MASQUERADE
</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-time-server">
      â </a>2.1.5.Â Time Server</h3></div></div></div>
		
		 <a id="idm140006539798784" class="indexterm"></a>
		 <div class="para">
			Synchronized system time throughout the cluster is very crucial for its flawless operation. It is achieved using the Network Time Protocol (NTP) daemon. If the head-node has direct Internet access, publicly available time-servers on the Internet can be contacted and used as an accurate time reference. In order to set a list of time-servers, edit the file <code class="filename">/etc/ntp.conf</code> and add a line for every ntp-server to be contacted:
		</div>
		 
<pre class="screen">server ntpserver
</pre>

	</div>
</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-base-conf-serv">
      â </a>2.2.Â Basic Services</h2></div></div></div>
	
	 <a id="idm140006539794576" class="indexterm"></a>
	 <div class="para">
		This section describes the basic services running on a typical Qlustar cluster.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-disk-part-file-sys">
      â </a>2.2.1.Â Disk Partitions and File-systems</h3></div></div></div>
		
		 <a id="idm140006539792368" class="indexterm"></a>
		 <div class="para">
			Typically, a head-node has two mirrored boot disks. Sometimes it also holds additional data disks, that are setup either as a mirror, a RAID 5/6 or are part of an external storage system. The boot disk (or the RAID device in case of a RAID boot setup) is used as a physical volume for the basic system LVM volume group (default name vgroot). See <a class="xref" href="#admin-man-sect-vol-man">SectionÂ 6.2.2, âLogical Volume Managementâ</a> for more details on LVM. The system volume group is the container of the following logical volumes: root, var, tmp, swap, apps and data (the latter can also be chosen to be located on a separate volume group made from additionally available disks during installation). Each of these logical volumes is used as the underlying block device for the correspondingly named file-system. Hence, the whole head-node setup, including the / (root) file-system is typically under control of LVM, adding large flexibility for storage management.
		</div>
		 <div class="para">
			All additional disks or RAID sets are partitioned with a single partition of type LVM, and used as LVM physical devices. Static mount configuration for file-systems is entered in <code class="filename">/etc/fstab</code>. All file-systems are of type ext4 unless requested otherwise.
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				Newer Qlustar installations also have the option to use ZFS pools (see <a class="xref" href="#admin-man-sect-zpools-admin">SectionÂ 6.2.3.1, âZpool Administrationâ</a>) to setup additional data disks.
			</div>

		</div></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-nis">
      â </a>2.2.2.Â NIS</h3></div></div></div>
		
		 <a id="idm140006539785968" class="indexterm"></a>
		 <div class="para">
			NIS (Network Information System) is used as the cluster wide name service database. User account (passwd and shadow map) and group information (group map), hostname resolution (hosts map), auto-mounter (auto.master, auto.apps, auto.data), netgroup and services are the most important maps. The head-node is configured as a NIS master server when running <code class="command">qlustar-initial-config</code> during installation. In case of a HA head-node setup, the second head-node becomes a NIS slave server.
		</div>
		 <div class="para">
			The generated NIS databases are located on the NIS master server under <code class="filename">/var/yp</code> and the corresponding source files in the directory <code class="filename">/etc/qlustar/yp</code>. The passwd and shadow tables are updated automatically by the script <code class="filename">adduser.sh</code> when users are added (see <a class="xref" href="#admin-man-sect-add-user">SectionÂ 6.1.1, âAdding User Accountsâ</a>) and the host map is automatically generated from QluMan. Apart from that, usually nothing needs to be changed in the provided NIS configuration. For security reasons, the file <code class="filename">/etc/qlustar/yp/shadow</code> should be readable and writable only by root. In case NIS source files have been changed manually, the command <code class="command">make -C /var/yp</code> must be executed to regenerate the maps and activate the changes. For more detailed information about NIS, you may also consult the NIS package HowTo at <code class="filename">/usr/share/doc/nis/nis.debian.howto.gz</code>.
		</div>
		 <div class="para">
			Another important security aspect is the access restriction to the NIS server. Only the compute-nodes should be allowed to contact the NIS server. In case the head-node is also used as a work-group NIS server, additional access can be allowed for the corresponding subnet to which the work-group workstations are connected. The access settings are configured in <code class="filename">/etc/ypserv.securenets</code> (see <code class="command">man ypserv</code>).
		</div>
		 <div class="para">
			The master NIS server is also its own client. The corresponding configuration for the NIS client (ypbind) process is set in <code class="filename">/etc/yp.conf</code>. The NIS domain name is set in <code class="filename">/etc/defaultdomain</code> and usually defined as <em class="parameter"><code>qlustar</code></em>. On cluster nodes booting over the network, these settings are all configured automatically by DHCP (see also <a class="xref" href="#admin-man-sect-dhcp">SectionÂ 2.1.3, âDHCPâ</a>).
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				Like with any standard NIS setup, if a user wants to change his/her login password, the command <code class="command">yppasswd</code> should be used.
			</div>

		</div></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-nfs-sect">
      â </a>2.2.3.Â NFS</h3></div></div></div>
		
		 <a id="idm140006539773472" class="indexterm"></a>
		 <div class="para">
			To ensure a cluster wide homogeneous directory structure, the head-node provides NFS (Network File System) services to the compute-nodes. The kernel NFS server with protocol version 3 is used for accomplishing this goal. The typical Qlustar directory structure consists of three file-systems that are exported by the head-node via NFS to all other nodes: <code class="filename">/srv/apps</code>, <code class="filename">/srv/data</code> and <code class="filename">/srv/ql-common</code>. Note that in NFS version 4, one directory serves as the root path for all exported file-systems and all exported directories must be a sub-directory of this path. To achieve compatibility with NFS 4 in Qlustar, the directory <code class="filename">/srv</code> is used for this purpose. While <code class="filename">/srv/apps</code> and <code class="filename">/srv/data</code> are typically separate file-systems on the head-node, the entry <code class="filename">/srv/ql-common</code> is a bind mount of the global Qlustar configuration directory <code class="filename">/etc/qlustar/common</code>. This mount is generated from the following entry in <code class="filename">/etc/fstab</code>:
		</div>
		 
<pre class="screen">/etc/qlustar/common    /srv/ql-common    none    bind    0    0
</pre>
		 <div class="para">
			File-systems to be shared via NFS need an entry in the file <code class="filename">/etc/exports</code>. Execute <code class="command">man exports</code> for a detailed explanation of the corresponding syntax. For security reasons, access to shared file-systems should be limited to trusted networks. The directory <code class="filename">/srv</code> is exported with a special parameter <em class="parameter"><code>fsid</code></em>. An export entry with the parameter <em class="parameter"><code>no_root_squash</code></em> for a host will enable full write access for the root user on that host (without that parameter, root is mapped to the user nobody on NFS mounts). In the following example, root on the host login-c (default name of the FrontEnd node) will have full write access to all exported file-systems:
		</div>
		 
<pre class="screen">/srv login-c(async,rw,no_subtree_check,fsid=0,insecure,no_root_squash)\
  192.168.52.0/24(async,rw,no_subtree_check,fsid=0,insecure)
/srv/data login-c(async,rw,no_subtree_check,insecure,nohide,no_root_squash)\
  192.168.52.0/24(async,rw,no_subtree_check,insecure,nohide)
/srv/apps login-c(async,rw,no-subtree_check,insecure,nohide,no_root_squash)\
  192.168.52.0/24(async,rw,no_subtree_check,insecure,nohide)
/srv/ql-common login-c(async,rw,subtree_check,insecure,nohide,no_root_squash)\
  192.168.52.0/24(async,ro,subtree_check,insecure,nohide)
</pre>
		 <div class="para">
			After changing the exports information, the NFS server needs to reload its configuration to activate it. This is achieved by executing the command
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
/etc/init.d/nfs-kernel-server reload</code>
</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-auto-mounter">
      â </a>2.2.4.Â Automounter</h3></div></div></div>
		
		 <a id="idm140006539760608" class="indexterm"></a>
		 <div class="para">
			NFS mounts on Qlustar nodes booting via the network are mostly managed by the kernel automounter. The information needed to configure these mounts comes from the NIS automounter maps <em class="parameter"><code>auto.apps</code></em> and <em class="parameter"><code>auto.data</code></em>. You can view the contents of these maps using the commands <code class="command">ypcat -k auto.apps</code> and <code class="command">ypcat -k auto.data</code>. The automounter software is able to determine which mounts are being consulted at a given time using so-called master maps. In Qlustar, this is the NIS map <em class="parameter"><code>auto.master</code></em>. Its content is defined in the source file <code class="filename"> /etc/qlustar/yp/auto.master</code> with the following default settings:
		</div>
		 
<pre class="screen">/apps auto.apps
/data auto.data
</pre>
		 <div class="para">
			This means that <code class="filename">/apps</code> is the base mount path for all entries in auto.apps and <code class="filename">/data</code> for all entries in <code class="filename">auto.data</code>. The referenced maps themselves have entries of the form (example for <code class="filename">auto.data</code>):
		</div>
		 
<pre class="screen">home -fstype=nfs,rw,hard,intr $NFSSRV:/srv/data/&amp;
</pre>
		 <div class="para">
			The remote directory <code class="filename">/srv/data</code> should thus be mounted by the automounter at the path <code class="filename">/data/home</code> on the NFS client. The variable <code class="envar">$NFSSRV</code> contains the hostname of the NFS server. Its value defaults to beosrv-c and could be modified by setting the DHCP option <em class="parameter"><code>option-130</code></em> with the following lines in the QluMan DHCP template:
		</div>
		 
<pre class="screen">option option-130 code 130 = text;
option option-130 "nfsserver";
</pre>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="warning"><div class="admonition_header"><p><strong>Warning</strong></p></div><div class="admonition">
			<div class="para">
				In this example, the variable would be changed to <em class="parameter"><code>nfsserver</code></em>. Changing the default is only recommended for very special cases though.
			</div>

		</div></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-ssh">
      â </a>2.2.5.Â SSH - Secure Shell</h3></div></div></div>
		
		 <a id="idm140006539748496" class="indexterm"></a>
		 <div class="para">
			Remote shell access from the LAN to the head-node and from the head-node to the compute-nodes is only allowed using the OpenSSH secure shell (ssh). A correct configuration of the ssh daemon is of crucial importance for the security of the whole cluster. Most important is to allow only ssh protocol version 2 connections. The default configuration in the cluster allows for <em class="parameter"><code>AgentForwarding</code></em> and <em class="parameter"><code>X11Forwarding</code></em>. This way, X11 programs can be executed without any further hassle from any compute-node with the X display appearing on a users workstation in the LAN. The relevant ssh configuration files are <code class="filename">/etc/ssh/sshd_config</code> and <code class="filename">/etc/ssh/ssh_config</code>.
		</div>
		 <div class="para">
			To allow password-less root access from the head to the other cluster nodes, the root ssh public key that is generated on the head-node is automatically put into the file <code class="filename">/etc/qlustar/common/image-files/ssh/authorized_keys</code> during installation. This file is then copied into the directory <code class="filename">/root/.ssh</code> on any netboot node during its boot process.
		</div>
		 <div class="para">
			One last step is required in order to prevent interactive questions when using ssh logins between nodes: A file named <code class="filename">ssh_known_hosts</code> containing all hosts keys in the cluster must exist. It is automatically generated by QluMan, placed into the directory <code class="filename">/etc/qlustar/common/image-files/ssh</code> and linked to <code class="filename">/etc/ssh/ssh_known_hosts</code> on netboot nodes.
		</div>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term"> Host-based authentication </span></dt><dd>
					<div class="para">
						To enable host-based authentication, the parameter <em class="parameter"><code>HostbasedAuthentication</code></em> must be set to <em class="parameter"><code>yes</code></em> in <code class="filename">/etc/ssh/sshd_config</code> on the clients. This is the default in Qlustar. Furthermore, the file <code class="filename">/etc/ssh/shosts.equiv</code> must contain the hostnames of all hosts from where login should be allowed. This file is also automatically generated by QluMan. Note that this mechanism works for ordinary users but not for the root user.
					</div>

				</dd></dl></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-mail-server-postfix">
      â </a>2.2.6.Â Mail server - Postfix</h3></div></div></div>
		
		 <a id="idm140006539736560" class="indexterm"></a>
		 <div class="para">
			Mostly for the purpose of sending alert and other informational messages, the mail server <em class="parameter"><code>postfix</code></em> is setup on the head-node. Typically it is configured to simply transfer all mail to a central mail relay, whose name can be entered during installation. The main postfix configuration file is <code class="filename">/etc/postfix/main.cf</code>. Mail aliases can be added in <code class="filename">/etc/aliases</code> (initial aliases were configured during installation). A change in this file requires execution of the command <code class="command">postalias /etc/aliases</code> to activate the changes. Have a look at <a class="xref" href="#admin-man-sect-mail-trans-agent">Mail Transport Agent</a> to find out, how to configure mail on the compute-nodes.
		</div>

	</div>
</div>
</div>
	 <div xml:lang="en-US" class="chapter" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="admin-man-chap-hw-conf">
      â </a>ChapterÂ 3.Â Qlustar Hardware Configuration</h1></div></div></div><div class="toc"><dl class="toc"><dt><span class="section"><a href="#admin-man-sect-hw-IB">3.1. Infiniband Networks</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-hw-IB-diag">3.1.1. IB Fabric Verification/Diagnosis</a></span></dt><dt><span class="section"><a href="#admin-man-sect-hw-IB-opensm">3.1.2. OpenSM Configuration</a></span></dt></dl></dd></dl></div>
	
	 <a id="idm140006539730816" class="indexterm"></a>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-hw-IB">
      â </a>3.1.Â Infiniband Networks</h2></div></div></div>
	
	 <a id="idm140006539728752" class="indexterm"></a>
	 <div class="para">
		Many clusters with the need for high-throughput and/or low-latency communication between nodes use Infiniband (IB) network hardware. Qlustar fully supports Infiniband via the <a href="https://www.openfabrics.org/___blank___">OFED software stack</a>. The basic configuration for Infiniband networks is explained later in <a class="xref" href="#admin-man-sect-infiniband">SectionÂ 4.2.8, âInfinibandâ</a>.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-hw-IB-diag">
      â </a>3.1.1.Â IB Fabric Verification/Diagnosis</h3></div></div></div>
		
		 <a id="idm140006539725072" class="indexterm"></a>
		 <div class="para">
			Especially for large clusters, an IB network is a complex fabric. The desired performance can only be achieved, if all components work flawlessly. Hence, it is important to have tools that can verify the validity of the hardware setup. In this section, we describe a number of checks that can help to setup a flawless IB fabric.
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ibstat
</code>
CA 'mlx4_0'
CA type: MT26428
Number of ports: 1
Firmware version: 2.7.0
Hardware version: b0
Node GUID: 0x003048fffff4cb8c
System image GUID: 0x003048fffff4cb8f
Port 1:
State: Active
Physical state: LinkUp
Rate: 40
Base lid: 310
LMC: 0
SM lid: 1
Capability mask: 0x02510868
Port GUID: 0x003048fffff4cb8d
Link layer: InfiniBand
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
cat /sys/class/infiniband/mlx4_0/ports/1/rate
</code>
40 Gb/sec (4X QDR)

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
cat /sys/class/infiniband/mlx4_0/ports/1/state
</code>
4: ACTIVE

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
cat /sys/class/infiniband/mlx4_0/ports/1/phys_state
</code>
5: LinkUp

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
cat /sys/class/infiniband/mlx4_0/board_id
</code>
SM_2122000001000

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
cat /sys/class/infiniband/mlx4_0/fw_ver
</code>
2.7.0
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ibv_devinfo
</code>
hca_id: mlx4_0
fw_ver: 2.7.000
node_guid: 0030:48ff:fff4:cb8c
sys_image_guid: 0030:48ff:fff4:cb8f
vendor_id: 0x02c9
vendor_part_id: 26428
hw_ver: 0xB0
board_id: SM_2122000001000
phys_port_cnt: 1
port: 1
state: PORT_ACTIVE (4)
max_mtu: 2048 (4)
active_mtu: 2048 (4)
sm_lid: 1
port_lid: 310
port_lmc: 0x00
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ibv_devices
</code>
device node GUID
------ ----------------
mlx4_0 003048fffff4cb8c
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ifconfig ib0
</code>
ib0 Link encap:UNSPEC HWaddr 80-00-00-48-FE-80-00-00-00-00-00-00-00-00-00-00 
inet addr:172.17.7.105 Bcast:172.17.127.255 Mask:255.255.128.0
UP BROADCAST RUNNING MULTICAST MTU:2044 Metric:1
RX packets:388445 errors:0 dropped:0 overruns:0 frame:0
TX packets:34 errors:0 dropped:0 overruns:0 carrier:0
collisions:0 txqueuelen:256 
RX bytes:39462502 (39.4 MB) TX bytes:2040 (2.0 KB)
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ibswitches
</code>
Switch : 0x0002c902004885b0 ports 36 "MF0;cluster-ibs:IS5300/L18/U1" base port 0 lid 17 lmc 0
Switch : 0x0002c90200488f60 ports 36 "MF0;cluster-ibs:IS5300/L17/U1" base port 0 lid 22 lmc 0
Switch : 0x0002c902004885a8 ports 36 "MF0;cluster-ibs:IS5300/L16/U1" base port 0 lid 16 lmc 0
Switch : 0x0002c90200488fc8 ports 36 "MF0;cluster-ibs:IS5300/L15/U1" base port 0 lid 26 lmc 0
Switch : 0x0002c902004885a0 ports 36 "MF0;cluster-ibs:IS5300/L14/U1" base port 0 lid 15 lmc 0
Switch : 0x0002c90200488fc0 ports 36 "MF0;cluster-ibs:IS5300/L13/U1" base port 0 lid 25 lmc 0
Switch : 0x0002c902004884e0 ports 36 "MF0;cluster-ibs:IS5300/L12/U1" base port 0 lid 10 lmc 0
Switch : 0x0002c90200488f68 ports 36 "MF0;cluster-ibs:IS5300/L11/U1" base port 0 lid 23 lmc 0
Switch : 0x0002c90200488510 ports 36 "MF0;cluster-ibs:IS5300/L10/U1" base port 0 lid 12 lmc 0
Switch : 0x0002c902004885e8 ports 36 "MF0;cluster-ibs:IS5300/L09/U1" base port 0 lid 19 lmc 0
Switch : 0x0002c90200488f78 ports 36 "MF0;cluster-ibs:IS5300/L08/U1" base port 0 lid 24 lmc 0
Switch : 0x0002c90200488598 ports 36 "MF0;cluster-ibs:IS5300/L07/U1" base port 0 lid 14 lmc 0
Switch : 0x0002c90200488fd8 ports 36 "MF0;cluster-ibs:IS5300/L06/U1" base port 0 lid 27 lmc 0
Switch : 0x0002c902004885f8 ports 36 "MF0;cluster-ibs:IS5300/L05/U1" base port 0 lid 21 lmc 0
Switch : 0x0002c902004885f0 ports 36 "MF0;cluster-ibs:IS5300/L03/U1" base port 0 lid 20 lmc 0
Switch : 0x0002c90200488528 ports 36 "MF0;cluster-ibs:IS5300/L02/U1" base port 0 lid 13 lmc 0
Switch : 0x0002c902004885e0 ports 36 "MF0;cluster-ibs:IS5300/L01/U1" base port 0 lid 18 lmc 0
Switch : 0x0002c90200472eb0 ports 36 "MF0;cluster-ibs:IS5300/S09/U1" base port 0 lid 4 lmc 0
Switch : 0x0002c90200472f08 ports 36 "MF0;cluster-ibs:IS5300/S08/U1" base port 0 lid 9 lmc 0
Switch : 0x0002c90200472ec8 ports 36 "MF0;cluster-ibs:IS5300/S07/U1" base port 0 lid 7 lmc 0
Switch : 0x0002c90200472ed0 ports 36 "MF0;cluster-ibs:IS5300/S06/U1" base port 0 lid 8 lmc 0
Switch : 0x0002c90200472ec0 ports 36 "MF0;cluster-ibs:IS5300/S05/U1" base port 0 lid 6 lmc 0
Switch : 0x0002c90200472eb8 ports 36 "MF0;cluster-ibs:IS5300/S04/U1" base port 0 lid 5 lmc 0
Switch : 0x0002c9020046cc60 ports 36 "MF0;cluster-ibs:IS5300/S03/U1" base port 0 lid 2 lmc 0
Switch : 0x0002c9020046cd58 ports 36 "MF0;cluster-ibs:IS5300/S02/U1" base port 0 lid 3 lmc 0
Switch : 0x0002c90200479668 ports 36 "MF0;cluster-ibs:IS5300/S01/U1" enhanced port 0 lid 1 lmc
Switch : 0x0002c90200488500 ports 36 "MF0;cluster-ibs:IS5300/L04/U1" base port 0 lid 11 lmc 0
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ibnodes | head -10
</code>
Ca : 0x0002c902002141f0 ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x0002c90200214150 ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x0002c9020021412c ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x0002c90200214164 ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x0002c902002141c8 ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x0002c9020020d82c ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x0002c9020020d6c0 ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x0002c90200216eb8 ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x0002c9020021413c ports 1 "MT25204 InfiniHostLx Mellanox Technologies"
Ca : 0x002590ffff2fc5f4 ports 1 "MT25408 ConnectX Mellanox Technologies"
</pre>
		 <div class="para">
			Because the output of the following command is extremely long, we only show the first 40 lines here:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ibnetdiscover | head -40</code>

# Topology file: generated on Thu Oct 10 14:06:20 2013
#
# Initiated from node 0002c903000c817c port 0002c903000c817d
vendid=0x2c9
devid=0xbd36
sysimgguid=0x2c90200479470
switchguid=0x2c902004885b0(2c902004885b0)
Switch 36 "S-0002c902004885b0" # "MF0;cluster-ibs:IS5300/L18/U1" base port 0 lid 17 lmc 0
[19] "S-0002c90200479668"[35] # "MF0;cluster-ibs:IS5300/S01/U1" lid 1 4xQDR
[20] "S-0002c90200479668"[36] # "MF0;cluster-ibs:IS5300/S01/U1" lid 1 4xQDR
[21] "S-0002c9020046cd58"[35] # "MF0;cluster-ibs:IS5300/S02/U1" lid 3 4xQDR
[22] "S-0002c9020046cd58"[36] # "MF0;cluster-ibs:IS5300/S02/U1" lid 3 4xQDR
[23] "S-0002c9020046cc60"[35] # "MF0;cluster-ibs:IS5300/S03/U1" lid 2 4xQDR
[24] "S-0002c9020046cc60"[36] # "MF0;cluster-ibs:IS5300/S03/U1" lid 2 4xQDR
[25] "S-0002c90200472eb8"[35] # "MF0;cluster-ibs:IS5300/S04/U1" lid 5 4xQDR
[26] "S-0002c90200472eb8"[36] # "MF0;cluster-ibs:IS5300/S04/U1" lid 5 4xQDR
[27] "S-0002c90200472ec0"[35] # "MF0;cluster-ibs:IS5300/S05/U1" lid 6 4xQDR
[28] "S-0002c90200472ec0"[36] # "MF0;cluster-ibs:IS5300/S05/U1" lid 6 4xQDR
[29] "S-0002c90200472ed0"[35] # "MF0;cluster-ibs:IS5300/S06/U1" lid 8 4xQDR
[30] "S-0002c90200472ed0"[36] # "MF0;cluster-ibs:IS5300/S06/U1" lid 8 4xQDR
[31] "S-0002c90200472ec8"[35] # "MF0;cluster-ibs:IS5300/S07/U1" lid 7 4xQDR
[32] "S-0002c90200472ec8"[36] # "MF0;cluster-ibs:IS5300/S07/U1" lid 7 4xQDR
[33] "S-0002c90200472f08"[35] # "MF0;cluster-ibs:IS5300/S08/U1" lid 9 4xQDR
[34] "S-0002c90200472f08"[36] # "MF0;cluster-ibs:IS5300/S08/U1" lid 9 4xQDR
[35] "S-0002c90200472eb0"[35] # "MF0;cluster-ibs:IS5300/S09/U1" lid 4 4xQDR
[36] "S-0002c90200472eb0"[36] # "MF0;cluster-ibs:IS5300/S09/U1" lid 4 4xQDR
vendid=0x2c9
devid=0xbd36
sysimgguid=0x2c90200479470
switchguid=0x2c90200488f60(2c90200488f60)
Switch 36 "S-0002c90200488f60" # "MF0;cluster-ibs:IS5300/L17/U1" base port 0 lid 22 lmc 0
[1] "H-0002c9020021413c"[1](2c9020021413d) # "MT25204 InfiniHostLx Mellanox Technologies" lid 284 4xDDR
[2] "H-0002c90200216eb8"[1](2c90200216eb9) # "MT25204 InfiniHostLx Mellanox Technologies" lid 241 4xDDR
[3] "H-0002c9020020d6c0"[1](2c9020020d6c1) # "MT25204 InfiniHostLx Mellanox Technologies" lid 244 4xDDR
[4] "H-0002c9020020d82c"[1](2c9020020d82d) # "MT25204 InfiniHostLx Mellanox Technologies" lid 242 4xDDR
[6] "H-0002c902002141c8"[1](2c902002141c9) # "MT25204 InfiniHostLx Mellanox Technologies" lid 243 4xDDR
[9] "H-0002c90200214164"[1](2c90200214165) # "MT25204 InfiniHostLx Mellanox Technologies" lid 293 4xDDR
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ibcheckstate 
</code>
## Summary: 215 nodes checked, 0 bad nodes found
## 1024 ports checked, 0 ports with bad state found
</pre>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ibcheckwidth
</code>
## Summary: 215 nodes checked, 0 bad nodes found
## 1024 ports checked, 0 ports with 1x width in error found
</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-hw-IB-opensm">
      â </a>3.1.2.Â OpenSM Configuration</h3></div></div></div>
		
		 <a id="idm140006539701936" class="indexterm"></a>
		 <div class="para">
			A functional IB fabric needs at least one node with a running subnet manager. This can be a switch or any other node connected to the fabric. In the latter case, OpenSM will be used. Please check the <a href="../QluMan_Guide/index.html#sec-Configuring-OpenSM___blank___">QluMan Guide</a> for details about how to setup OpenSM on a compute-node. If OpenSM should run on a head-node, you will have to install the package <span class="package">opensm</span> and configure it manually, if necessary (for simple networks, the default configuration will be sufficient).
		</div>

	</div>
	
</div>
</div>
	 <div xml:lang="en-US" class="chapter" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="admin-man-chap-node-manage">
      â </a>ChapterÂ 4.Â Cluster Node Management</h1></div></div></div><div class="toc"><dl class="toc"><dt><span class="section"><a href="#admin-man-sect-boot-process">4.1. Boot Process</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-comp-node-boot">4.1.1. Compute-node booting</a></span></dt><dt><span class="section"><a href="#admin-man-sect-tftp-boot-serv">4.1.2. TFTP Boot Server</a></span></dt><dt><span class="section"><a href="#admin-man-sect-RAM-disk-img">4.1.3. RAM-disk image</a></span></dt><dt><span class="section"><a href="#admin-man-sect-qluman-execd">4.1.4. QluMan Remote Execution Server</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-node-customize">4.2. Node Customization</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-node-options">4.2.1. Dynamic Configuration Settings</a></span></dt><dt><span class="section"><a href="#admin-man-sect-DHCP-client">4.2.2. DHCP-Client</a></span></dt><dt><span class="section"><a href="#admin-man-sect-config-dir">4.2.3. Cluster-wide Configuration Directory</a></span></dt><dt><span class="section"><a href="#admin-man-sect-NFS-boot-scripts">4.2.4. NFS boot scripts</a></span></dt><dt><span class="section"><a href="#admin-man-sect-add-dirs-files-links">4.2.5. Adding directories, files, links</a></span></dt><dt><span class="section"><a href="#admin-man-sect-hard-disc-init">4.2.6. Hard disc initialization</a></span></dt><dt><span class="section"><a href="#admin-man-sect-mail-trans-agent">4.2.7. Mail Transport Agent</a></span></dt><dt><span class="section"><a href="#admin-man-sect-infiniband">4.2.8. Infiniband</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-remote-control">4.3. Node Remote Control</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-ser-cons-para">4.3.1. Serial Console Parameter</a></span></dt><dt><span class="section"><a href="#admin-man-sect-ipmi-config">4.3.2. IPMI Configuration</a></span></dt></dl></dd></dl></div>
	
	 <a id="idm140006571077216" class="indexterm"></a>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-boot-process">
      â </a>4.1.Â Boot Process</h2></div></div></div>
	
	 <a id="idm140006539695664" class="indexterm"></a>
	 <div class="para">
		This section describes the boot process of Qlustar cluster-nodes.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-comp-node-boot">
      â </a>4.1.1.Â Compute-node booting</h3></div></div></div>
		
		 <a id="idm140006539693552" class="indexterm"></a>
		 <div class="para">
			The boot process of the compute-nodes follows precise rules. It takes place in four steps:
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="orderedlist"><ol><li class="listitem">
				<div class="para">
					The PXE boot ROM of the network card sends a DHCP request. If the node is already registered in QluMan, the request is answered by the DHCP server running on the head-node(s), allowing the adapter to configure its basic IP settings.
				</div>

			</li><li class="listitem">
				<div class="para">
					The boot ROM requests a PXE loader program from the TFTP server on the head-node (the TFTP server specified by DHCP could also be on another node, but this is not the default). The PXE loader is then sent to the compute-node via TFTP.
				</div>

			</li><li class="listitem">
				<div class="para">
					PXELinux downloads the Qlustar Linux kernel and the assigned RAM-disk (OS) image, boots the kernel and mounts the RAM-disk.
				</div>

			</li><li class="listitem">
				<div class="para">
					The usual Linux boot process proceeds.
				</div>

			</li></ol></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-tftp-boot-serv">
      â </a>4.1.2.Â TFTP Boot Server</h3></div></div></div>
		
		 <a id="idm140006539686608" class="indexterm"></a>
		 <div class="para">
			The Advanced TFTP server transfers the boot image to the compute-nodes. All files that should be served by tftp must reside in the directory <code class="filename">/var/lib/tftpboot</code>. On a Qlustar installation, it contains three symbolic links:
		</div>
		 
<pre class="screen">
pxelinux.0 -&gt; <code class="filename">/usr/lib/syslinux/pxelinux.0</code>
pxelinux.cfg -&gt; <code class="filename">/etc/qlustar/pxelinux.cfg</code>
qlustar -&gt; <code class="filename">/var/lib/qlustar</code></pre>
		 <div class="para">
			The directory <code class="filename">/etc/qlustar/pxelinux.cfg</code> contains the PXE boot configuration files for the compute-nodes. There is a default configuration that applies to any node without an assigned custom boot configuration in QluMan. For every host with a custom boot configuration, QluMan adds a symbolic link pointing to the actual configuration file. The links are named after the node's <em class="parameter"><code>Hostid</code></em>, which you can find out with the <code class="command">gethostip</code> command. For more details about how to define boot configurations see the corresponding section of the <a href="../QluMan_Guide/index.html#sec-Boot-Configs___blank___">QluMan Guide</a>.
		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-RAM-disk-img">
      â </a>4.1.3.Â RAM-disk image</h3></div></div></div>
		
		 <a id="idm140006539679344" class="indexterm"></a>
		 <div class="para">
			The RAM-disk image is the file-system holding the node OS that is mounted as the root filesystem of the compute-nodes. It is assembled on the head-node(s) from the image modules, you are able to select in QluMan. Every RAM-disk image contains at least the core module. See the corresponding section of the <a href="../QluMan_Guide/index.html#sec-Qlustar-OS-Images___blank___">QluMan Guide</a> for more details. All available image modules are displayed and selectable in QluMan and the configuration and assembly of images is done automatically from within QluMan.
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				By default, the root password of a Qlustar OS image and hence the node booting it, is taken from the head-node(s) <code class="filename">/etc/shadow</code> file and is therefore the same as on the head-node(s). If you want to change this, you can call <code class="command">qlustar-image-reconfigure &lt;image-name&gt;</code>. (Replacing &lt;image-name&gt; with the actual name of the image). You can then specify a different root password for the node OS image.
			</div>

		</div></div>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term"> Changelogs </span></dt><dd>
					<div class="para">
						Any Qlustar node OS image contains changelogs of the various image modules it is composed of. They are located in the directory <code class="filename">/usr/share/doc/qlustar-image</code>. The main changelog file is <code class="filename">core.changelog.gz</code>. The other files are automatically generated. The files <code class="filename">*.packages.version.gz</code> lists the packages each module is made of. The files <code class="filename">*.contents.changelog*.gz</code> lists the files that were changed between each version, and <code class="filename">*.packages.changelog*.gz</code> list differences in the package list and versions. Hence, you always have detailed information about what has been changed in new images as well as the package sources of their content.
					</div>

				</dd></dl></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-qluman-execd">
      â </a>4.1.4.Â QluMan Remote Execution Server</h3></div></div></div>
		
		 <a id="idm140006539669472" class="indexterm"></a>
		 <div class="para">
			The QluMan execution server (<em class="firstterm">qluman-execd</em>) runs on any head- and compute-node of a Qlustar cluster. It is one of Qlustar's main components, responsible for executing remote commands as well as writing configurations to disk.
		</div>
		 <div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="admin-man-sect-execd-dynamic-boot-scripts">
      â </a>4.1.4.1.Â Dynamic Boot Script Excecution</h4></div></div></div>
			
			 <a id="idm140006539666256" class="indexterm"></a>
			 <div class="para">
				When a compute-node boots and qluman-execd starts, it automatically performs some initialization/configuration tasks depending on the nodes QluMan configuration options. The following is a list of tasks managed by qluman-execd:
			</div>
			 <div class="variablelist"><dl class="variablelist"><dt><span class="term"> Infiniband IP configuration </span></dt><dd>
						<div class="para">
							Configuration of the Infiniband IPoIB address, if a node is configured to use Infiniband within QluMan.
						</div>

					</dd><dt><span class="term"> Infiniband OpenSM startup </span></dt><dd>
						<div class="para">
							Startup of OpenSM in case the node is configured to do so.
						</div>

					</dd><dt><span class="term"> IPMI IP configuration and channel selection </span></dt><dd>
						<div class="para">
							Reconfiguration of the node's IPMI address, if a node is configured correspondingly within QluMan.
						</div>

					</dd></dl></div>
			 <div class="para">
				For details about the configuration of the above components, see the corresponding section of the <a href="../QluMan_Guide/index.html#chap-RX-Engine___blank___">QluMan Guide</a>.
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
				<div class="para">
					If required for debugging, etc., the boot scripts managed by QluMan Execd can still be executed manually, like normal System V boot scripts.
				</div>

			</div></div>

		</div>

	</div>
</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-node-customize">
      â </a>4.2.Â Node Customization</h2></div></div></div>
	
	 <a id="idm140006539655488" class="indexterm"></a>
	 <div class="para">
		This section describes the customization options/tools for the configuration of Qlustar cluster-nodes.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-node-options">
      â </a>4.2.1.Â Dynamic Configuration Settings</h3></div></div></div>
		
		 <a id="idm140006539653328" class="indexterm"></a>
		 <div class="para">
			A number of configuration options are configured dynamically when a node boots. These settings will be stored either in the file <code class="filename">/etc/qlustar/options</code> or in a separate file in the directory <code class="filename">/etc/qlustar/options.d</code> of the nodes root filesystem. Usual BASH shell syntax is used for the options. An example for the latter are the configuration settings for a nodes Infiniband stack. They will be placed in the file <code class="filename">ib</code>, which is created on the fly by the QluMan execd process while booting (see also the <a class="xref" href="#admin-man-sect-qluman-execd">previous section</a> and <a class="xref" href="#admin-man-sect-infiniband">SectionÂ 4.2.8, âInfinibandâ</a>).
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				The settings in <code class="filename">/etc/qlustar/options</code> as well as the config files generated in <code class="filename">/etc/qlustar/options.d</code> should usually not be edited manually. However, when a node has trouble starting certain services or configuring some system components, it can make sense to inspect and possibly change the settings in these files to see whether that solves the problem. Please report such a situation as a bug, so that it can be fixed in a future release.
			</div>

		</div></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-DHCP-client">
      â </a>4.2.2.Â DHCP-Client</h3></div></div></div>
		
		 <a id="idm140006539645920" class="indexterm"></a>
		 <div class="para">
			The dhclient process started during the boot process of compute-nodes will configure the IP addresses and other parameters given. By default, only the network interface from which the node boots is managed by DHCP. In a QluMan boot config, you can set the kernel parameter <em class="parameter"><code>dhcp_ifaces</code></em> to a comma separated list of interface names to manage other interfaces as well. Example: <em class="parameter"><code>dhcp_ifaces=eth0,eth1</code></em>. The first interface in this list is the primary interface. Extended information like NIS-domain and NIS-servers is queried through this interface.
		</div>
		 <div class="para">
			Setting <em class="parameter"><code>dhcp_ifaces=bond0:eth0:eth1</code></em> you can easily define a bonding interface. This line would cause the interfaces <code class="interfacename">eth0</code> and <code class="interfacename">eth1</code> to act as slave interfaces associated to <code class="interfacename">bond0</code>. You can add additional options for the bonding interface separated with plus signs. These options include mode (defaults to 0) and miimon (defaults to 100). See the output of the command <code class="command">modinfo bonding</code> for a short explanation of these options.
		</div>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term">Examples</span></dt><dd>
					<div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
							<div class="para">
								To set the mode to active-backup 
<pre class="screen">
dhcp_ifaces=bond0:eth0:eth1+mode=1
</pre>

							</div>

						</li><li class="listitem">
							<div class="para">
								To increase the link check interval to 200ms: 
<pre class="screen">
dhcp_ifaces=bond0:eth0:eth1+miimon=200
</pre>

							</div>

						</li><li class="listitem">
							<div class="para">
								To add another interface <code class="interfacename">eth2</code> that dhcp-client should configure: 
<pre class="screen">
dhcp_ifaces=bond0:eth0:eth1,eth2
</pre>

							</div>

						</li></ul></div>

				</dd><dt><span class="term">Bridging</span></dt><dd>
					<div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
							<div class="para">
								Additionally you can define a bridge interface like this: 
<pre class="screen">
dhcp_ifaces=br0:eth0
</pre>
								 This makes the interface <code class="interfacename">eth0</code> be the the bridge port for <code class="interfacename">br0</code>.
							</div>

						</li></ul></div>

				</dd></dl></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-config-dir">
      â </a>4.2.3.Â Cluster-wide Configuration Directory</h3></div></div></div>
		
		 <a id="idm140006539629424" class="indexterm"></a>
		 <div class="para">
			The directory <code class="filename">/etc/qlustar/common</code> contains cluster-wide configuration files for the nodes. At an early stage of the boot processs this directory is mounted as an NFS directory from the head-node. By default the following arguments to mount are used:
		</div>
		 
<pre class="screen">
-t nfs -o rw,hard,intr 192.168.52.254:/srv/ql-common
</pre>
		 <div class="para">
			If you want to use different arguments, you can set the following DHCP parameter in the QluMan DHCP template (see also <a class="xref" href="#admin-man-sect-dhcp">SectionÂ 2.1.3, âDHCPâ</a>):
		</div>
		 
<pre class="screen">
option qlustar-cfgmnt code 132 = text;
option qlustar-cfgmnt "-t nfs -o rw,hard,intr 192.168.52.254:/srv/ql-common";
</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-NFS-boot-scripts">
      â </a>4.2.4.Â NFS boot scripts</h3></div></div></div>
		
		 <a id="idm140006539624464" class="indexterm"></a>
		 <div class="para">
			To allow for flexible configuration of compute-nodes, a specific NFS directory (<code class="filename">/etc/qlustar/common/rc.boot</code>) is searched for executable scripts in a late phase of the boot process. The scripts found are then executed one by one. You can use this mechanism to perform arbitrary modifications/customization of the compute node OS.
		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-add-dirs-files-links">
      â </a>4.2.5.Â Adding directories, files, links</h3></div></div></div>
		
		 <a id="idm140006539621120" class="indexterm"></a>
		 <div class="para">
			The script <code class="filename">/lib/qlustar/copy-files</code>, which is also executed at boot, consults a configuration file <code class="filename">/etc/qlustar/common/image-files/destinations</code>, where each line describes a directory to be created, a file to be copied from an NFS path to a local path, or a link that needs to be created in the RAM-disk. Example: 
<pre class="screen">
# remotefile is a path relative to /etc/qlustar/common/image-files
# and destdir is the absolute path of the directory where remotefile
# should be copied to. mode is used as input to chmod.
# Example:
# authorized_keys   /root/.ssh    root:root   600

# Directories
/etc/ldap

# remotefile            destdir         owner           permissions
ssh/authorized_keys     /root/.ssh      root:root       644
etc/nsswitch.conf       /etc            root:root       644
etc/ldap.conf           /etc            root:root       644
etc/timezone            /etc            root:root       644

# Symbolic links
# Source                target
/l/home                 /home
</pre>

		</div>
		 <div class="para">
			With this mechanism, it is also possible to specify additional files to process by adding an <em class="parameter"><code>#include</code></em> line like this: 
<pre class="screen">
#include ldapfiles
</pre>
			 In this example, the file <code class="filename">ldapfile</code> will be processed just like the <code class="filename">destinations</code> file.
		</div>
		 <div class="para">
			Furthermore, if the file <code class="filename">/etc/qlustar/common/softgroups</code> exists, it may specify a group name directly (without whitespace) followed by a colon followed by a hostlist. An example softgroups file may look like this: 
<pre class="screen">
version2: beo-[01-04]
version3: beo-[05-08]
</pre>
			 This will make hosts <code class="systemitem">beo-01</code> - <code class="systemitem">beo-04</code> additionally consult the file <code class="filename">/etc/qlustar/common/image-files/destinations.version2</code> and hosts <code class="systemitem">beo-05</code> - <code class="systemitem">beo-08</code> consult <code class="filename">/etc/qlustar/common/image-files/destinations.version3</code>. The group name defined in the softgroups is the extension to the destinations file. The files could look like this: 
<pre class="screen"># destinations.version2 - use version2 of the program:
       /apps/prod/bin/program.version2 /usr/bin/program # destinations.version3 - use version3
       of the program: /apps/prod/bin/program.version3 /usr/bin/program
</pre>
			 Hence, with this mechanism, you can have parts of your cluster use different versions of the same program.
		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-hard-disc-init">
      â </a>4.2.6.Â Hard disc initialization</h3></div></div></div>
		
		 <a id="idm140006539608704" class="indexterm"></a>
		 <div class="para">
			The boot script <code class="filename">/etc/init.d/disk-auto-setup</code> is responsible for checking whether the hard disc of the compute-node is already partitioned correctly and has the needed filesystems and swap setup. If this is not the case, it partitions the drive, generates filesystems and swap space, and finally mounts the filesystems and activates swap. You can force creating of the partitions and filesystems by adding the hostnames to the initialize group in <code class="filename">/etc/qlustar/common/disk-configs/raidgroups</code>:
		</div>
		 
<pre class="screen">
initialize: beo-01 beo-[12][0-9]
</pre>
		 <div class="para">
			Note that you can also use regular expressions to match hostnames, so that a line like the one above would affect node <code class="systemitem">beo-01</code> as well as nodes 10 to 29. The disc is typically setup with a <code class="filename">/var</code> (1GB) filesystem, 2GB of swap space and a /scratch filesystem occupying the remainder of the disc. The configuration file resides in <code class="filename">/etc/qlustar/common/disk-configs/initdisc.cf</code> and by default looks like this:
		</div>
		 
<pre class="screen">
disk_config sda
primary   /var          1024       ;
primary   swap          8192       ;
primary   /scratch      1000-      ;
</pre>
		 <div class="para">
			This instructs the script disk-auto-setup to create a <code class="filename">/var partition/filesystem</code> of size 1GB, a swap partition of size 8GB and to use the rest of the disk for the /scratch partition. Note that the partitions <code class="filename">/var</code> and <code class="filename">/scratch</code> are automatically mounted and swap automatically activated. Therefore you should define at least these three partitions. Special configurations with RAID 0 devices on multiple discs are also possible in cases where disc I/O performance is of central importance. Example:
		</div>
		 
<pre class="screen">
disk_config sda ; sdb sdc
primary   /var          2048       ; raid0 md0
primary   swap          4096       ; raid0 md1
primary /scratch        1000-      ; raid0 md2
</pre>
		 <div class="para">
			You can add a parameter <em class="parameter"><code>sched_config</code></em> and set it to the desired I/O Scheduler. The possible values are: noop, anticipatory, deadline and cfq. To set the scheduler to deadline add this line:
		</div>
		 
<pre class="screen">
sched_config deadline
</pre>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term"> Diskgroups </span></dt><dd>
					<div class="para">
						Similar to Softgroups you can divide your cluster into groups with different hard disk setups. The file <code class="filename">/etc/qlustar/common/disk-configs/diskgroups</code> defines the groups and should be created by the ql-cluster-portal. It may look like this: 
<pre class="screen">
moreswap: beo-01 beo-02 beo-03 beo-04
</pre>
						 Now the file <code class="filename">/etc/qlustar/common/disk-configs/initdisc.cf.moreswap</code> could have this content: 
<pre class="screen">
disk_config sda
primary   /var           1024       ;
primary   swap           16384      ;
primary   /scratch       1000-      ;
</pre>
						 and hosts <code class="systemitem">beo-01</code> - <code class="systemitem">beo-04</code> will have more swap space available.
					</div>

				</dd></dl></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-mail-trans-agent">
      â </a>4.2.7.Â Mail Transport Agent</h3></div></div></div>
		
		 <a id="idm140006539592832" class="indexterm"></a>
		 <div class="para">
			By default the compute-nodes do not send mail. You can however activate the simple MTA ssmtp. In the file <code class="filename">/etc/qlustar/common/beobox.conf</code> you can specify to use it:
		</div>
		 
<pre class="screen">
BEOBOX_MAILER=ssmtp
</pre>
		 <div class="para">
			The configuration can be found in the directory <code class="filename">/etc/qlustar/common/ssmtp/</code> and consists of two files, <code class="filename">ssmtp.conf</code> and <code class="filename">revaliases</code>. In <code class="filename">ssmtp.conf</code> you should set these parameters:
		</div>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term"> Root </span></dt><dd>
					<div class="para">
						The address to send mail to for users with id less than 1000.
					</div>

				</dd><dt><span class="term"> Mailhub </span></dt><dd>
					<div class="para">
						The host to send all mail to.
					</div>

				</dd><dt><span class="term"> RewriteDomain </span></dt><dd>
					<div class="para">
						Make all mail look like originating from this domain.
					</div>

				</dd><dt><span class="term"> FromLineOverride </span></dt><dd>
					<div class="para">
						Allow users to override the domain, must be âyesâ or ânoâ.
					</div>

				</dd><dt><span class="term"> Hostname </span></dt><dd>
					<div class="para">
						The fully qualified name of this host
					</div>

				</dd></dl></div>
		 <div class="para">
			An example configuration file would be:
		</div>
		 
<pre class="screen">
Root=user@domain.com
Mailhub=relayhost
RewriteDomain=domain.com
FromLineOverride=Yes
Hostname=thishost.domain.com
</pre>
		 <div class="para">
			In the file <code class="filename">revaliases</code> you can specify how mails to local accounts should be forwarded to outgoing addresses and which mail server to use. Example:
		</div>
		 
<pre class="screen">
user:user@mailprovider.com:mailserver.mailprovider.com
</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-infiniband">
      â </a>4.2.8.Â Infiniband</h3></div></div></div>
		
		 <a id="idm140006539577168" class="indexterm"></a>
		 <div class="para">
			If the dynamically created file <code class="filename">/etc/qlustar/options.d/ib</code> exists (see <a class="xref" href="#admin-man-sect-node-options">SectionÂ 4.2.1, âDynamic Configuration Settingsâ</a> for details on the mechanism), the Infiniband stack will be initialized according to the settings in there as follows: 
			<div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						The required kernel modules will be loaded
					</div>

				</li><li class="listitem">
					<div class="para">
						The IPoIB (IP over IB) adapter <code class="filename">ib0</code> will receive an IP address.
					</div>

				</li><li class="listitem">
					<div class="para">
						and the subnet manager OpenSM will be started, if so configured with QluMan.
					</div>

				</li></ul></div>
			 The configurable options in this file are:
		</div>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term"> IB_IP </span></dt><dd>
					<div class="para">
						the network part of the current address
					</div>

				</dd><dt><span class="term"> IB_MASK </span></dt><dd>
					<div class="para">
						The network mask of the IB network
					</div>

				</dd><dt><span class="term"> OPENSM_START </span></dt><dd>
					<div class="para">
						Should OpenSM be started (true/false)
					</div>

				</dd><dt><span class="term"> OPENSM_OPTS </span></dt><dd>
					<div class="para">
						Command line options for OpenSM
					</div>

				</dd></dl></div>
		 <div class="para">
			An example <code class="filename">/etc/qlustar/options.d/ib</code> options file:
		</div>
		
<pre class="screen">
IB_IP=172.16.0.200
IB_MASK=255.255.255.0
OPENSM_START=true
OPENSM_OPTS="-R updn"
</pre>

	</div>
	 
</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-remote-control">
      â </a>4.3.Â Node Remote Control</h2></div></div></div>
	
	 <a id="idm140006539562224" class="indexterm"></a>
	 <div class="para">
		This section describes the tools and configuration options for the remote control of Qlustar cluster nodes.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-ser-cons-para">
      â </a>4.3.1.Â Serial Console Parameter</h3></div></div></div>
		
		 <a id="idm140006539560064" class="indexterm"></a>
		 <div class="para">
			The default configuration <code class="filename">/etc/qlustar/pxelinux.cfg/default</code> contains the kernel commandline that is passed to the nodes. If you need to modify the console parameter you can change it there.
		</div>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term"> Access to the Serial Console </span></dt><dd>
					<div class="para">
						To access the serial console use the command <code class="command">console-login</code>. It allows to select the node whose console you wish to open. Depending on the type of console you need different keystrokes to exit. If you are using
					</div>

				</dd><dt><span class="term"> ipmiconsole </span></dt><dd>
					<div class="para">
						then you need to type <span class="keycap"><strong>&amp;</strong></span>+<span class="keycap"><strong>.</strong></span>
					</div>

				</dd><dt><span class="term"> ipmitool </span></dt><dd>
					<div class="para">
						then type <span class="keycap"><strong>~</strong></span>+<span class="keycap"><strong>.</strong></span>
					</div>

				</dd><dt><span class="term"> minicom </span></dt><dd>
					<div class="para">
						then use <span class="keycap"><strong>Ctrl-a</strong></span>+<span class="keycap"><strong>x</strong></span>
					</div>

				</dd></dl></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-ipmi-config">
      â </a>4.3.2.Â IPMI Configuration</h3></div></div></div>
		
		 <a id="idm140006539547072" class="indexterm"></a>
		 <div class="para">
			Many computers nowadays are equipped with an Intelligent Platform Management Interface (IPMI). Q-Leap Qlustar allows to automatically configure the ip address of these interfaces. The required configuration files are located in the directory <code class="filename">/etc/qlustar/common/ipmi</code>. The file <code class="filename">overwrite-settings</code> determines which hosts should configure the <code class="interfacename">ipmi</code> interface. It contains the name overwrite followed by a regular expression that should match the hostnames. To configure the IPMI interface of all hosts the file should look like this:
		</div>
		 
<pre class="screen">
overwrite: .*
</pre>
		 <div class="para">
			The configuration file <code class="filename">ipmi.conf</code> contains the default settings that can be overwritten by group specific files. The settings are:
		</div>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term"> ge_net </span></dt><dd>
					<div class="para">
						the network of an already configured network interface
					</div>

				</dd><dt><span class="term"> ipmi_net </span></dt><dd>
					<div class="para">
						the network used for the IPMI network, substitutes ge_net and the result will be the new address
					</div>

				</dd><dt><span class="term"> netmask </span></dt><dd>
					<div class="para">
						the netmask
					</div>

				</dd><dt><span class="term"> gateway </span></dt><dd>
					<div class="para">
						the default gateway to use
					</div>

				</dd><dt><span class="term"> lan_channel </span></dt><dd>
					<div class="para">
						the IPMI channel for the lan interface
					</div>

				</dd></dl></div>
		 <div class="para">
			Example:
		</div>
		 
<pre class="screen">
ge_net=192.168.52
ipmi_net=10.0.0
netmask=255.255.255.0
gateway=10.0.0.254
lan_channel=1
</pre>
		 <div class="para">
			In this example a host with address 192.168.52.1 will configure the IPMI interface with address 10.0.0.1. Groups are defined in the file <code class="filename">ipmigroups</code>. This file contains group names followed by regular expressions that should match the members of this group.
		</div>
		 
<pre class="screen">
grp1:grp2:beo-1[0-9]
beo-2[0-9]
</pre>
		 <div class="para">
			Members of the group grp1 will use an additional file <code class="filename">ipmi.conf.grp1</code> and members of the group grp2 the file <code class="filename">ipmi.conf.grp2</code>. In these files you can overwrite the above default values.
		</div>

	</div>
	
</div>
</div>
	 <div xml:lang="en-US" class="chapter" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="admin-man-chap-monitoring">
      â </a>ChapterÂ 5.Â Monitoring Infrastructure</h1></div></div></div><div class="toc"><dl class="toc"><dt><span class="section"><a href="#admin-man-sect-ganglia">5.1. Ganglia</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-monit-the-nodes">5.1.1. Monitoring the nodes</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-nagios">5.2. Nagios</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-nag-plug-ins">5.2.1. Nagios Plugins</a></span></dt><dt><span class="section"><a href="#admin-man-sect-head-node">5.2.2. Monitoring the head-node(s)</a></span></dt><dt><span class="section"><a href="#admin-man-sect-web-interface">5.2.3. Webinterface</a></span></dt><dt><span class="section"><a href="#admin-man-sect-restart">5.2.4. Restart</a></span></dt></dl></dd></dl></div>
	
	 <a id="idm140006539530080" class="indexterm"></a>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-ganglia">
      â </a>5.1.Â Ganglia</h2></div></div></div>
	
	 <a id="idm140006539528080" class="indexterm"></a>
	 <div class="para">
		This section describes the Qlustar Ganglia setup. Nagios in combination with Ganglia is used to monitor the hardware of the compute nodes as well as the head-node.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-monit-the-nodes">
      â </a>5.1.1.Â Monitoring the nodes</h3></div></div></div>
		
		 <a id="idm140006539525856" class="indexterm"></a>
		 <div class="para">
			<div class="mediaobject"><img src="./images/ganglia-overview.png" width="70%" alt="Cluster Monitoring Web Interface over Ganglia" /><div class="caption">
				<div class="para">
					Cluster Monitoring Web Interface over Ganglia
				</div>
				</div></div>
			 <div class="mediaobject"><img src="./images/ganglia-node.png" width="70%" alt="Node Monitoring with Ganglia" /><div class="caption">
				<div class="para">
					Node Monitoring with Ganglia
				</div>
				</div></div>
			 Each node sends sensor data and other information such as swap usage, fill level of file systems and S.M.A.R.T. data of the hard disks to a multicast address where the head-node can collect them. The way each node collects the sensor data depends on the hardware type. The Qlustar Cluster Suite detects the type that is suitable for a specific compute node. You can list the current metrics by running <code class="command">ganglia --help</code>. The package <span class="package">ganglia-webfrontend</span> allows to view the state of your cluster and each node from within a web-browser. It suffices to visit the Link <code class="literal">http://<em class="replaceable">&lt;head-node&gt;</em>/ganglia</code>.
		</div>

	</div>
</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-nagios">
      â </a>5.2.Â Nagios</h2></div></div></div>
	
	 <a id="idm140006539514000" class="indexterm"></a>
	 <div class="para">
		This section describes the Qlustar Nagios setup.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-nag-plug-ins">
      â </a>5.2.1.Â Nagios Plugins</h3></div></div></div>
		
		 <a id="idm140006539511632" class="indexterm"></a>
		 <div class="para">
			The package <span class="package">qlustar-nagios-plugins</span> contains the tools required to process the data received from Ganglia. The thresholds, services and nodes to monitor and group definitions are set in files located in the directory <code class="filename">/etc/nagios3/conf.d/qlustar</code>. The file <code class="filename">nodes.cfg</code> lists the nodes. This file is auto-generated from QluMan. A few lines are required for each node as shown in this example:
		</div>
		 
<pre class="screen">
define host {
  host_name      beo-01 
  use            generic-node
  register       1
}
define host {
  host_name      beo-10
  use            generic-node
  register       1
}
</pre>
		 <div class="para">
			The file <code class="filename">hostgroup-definitions.cfg</code> defines which nodes belong to which hostgroup:
		</div>
		 
<pre class="screen">
define hostgroup {
  hostgroup_name        ganglia-nodes
  members               beo-0.
  register              1
}
</pre>
		 <div class="para">
			The regular expression <code class="systemitem">beo-0.</code> specifies, that all nodes with a hostname matching the expression are member of this group. If you need to create additional groups because you have different types of nodes with a different set of available metrics, or with metrics that require different thresholds, then you can define them here. Example:
		</div>
		 
<pre class="screen">
define hostgroup {
  hostgroup_name          opterons
  members                 beo-1[3-6]
  register                1 
}
</pre>
		 <div class="para">
			The file <code class="filename">services.cfg</code> lists all metrics that should be monitored. It includes the thresholds, and for which groups each service is defined. For cluster nodes, the metric data is delivered via Ganglia. The following example defines the monitoring of the fan speed for the members of the group <code class="systemitem">opterons</code>:
		</div>
		 
<pre class="screen">
define service {
  use                      generic-service
  hostgroup_name           opterons
  service_description      Ganglia fan1
  check_command            check_ganglia_fan!3000!0!"fan1"!$HOSTNAME$
  register                 1
}
</pre>
		 <div class="para">
			With this definition, the service will enter the <code class="literal">warning state</code> once the fan-speed goes below 3000, and if it completely fails (speed 0), it will enter the <code class="literal">error state</code>.
		</div>
		 <div class="para">
			The following is an example for a service that is monitored for the members of two hostgroups:
		</div>
		 
<pre class="screen">
define service {
  use                     generic-service
  hostgroup_name          ganglia-nodes,opterons
  service_description     Ganglia temp1
  check_command           check_ganglia_temp!50!60!"temperature1"!$HOSTNAME$
  register                1
}
</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-head-node">
      â </a>5.2.2.Â Monitoring the head-node(s)</h3></div></div></div>
		
		 <a id="idm140006539499408" class="indexterm"></a>
		 <div class="para">
			The file <code class="filename">localhost.cfg</code> lists the services that should be monitored for the head-node(s). The definitions are different because the data is not collected through Ganglia.
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				The software RAID (md) devices are monitored by mdadm and mail is sent to root if a device fails. The RAID devices are not monitored with the nagios setup by default.
			</div>

		</div></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-web-interface">
      â </a>5.2.3.Â Webinterface</h3></div></div></div>
		
		 <a id="idm140006539495360" class="indexterm"></a>
		 <div class="para">
			You can open the Nagios web interface at the address <code class="literal">http://<em class="replaceable">&lt;head-node&gt;</em>/nagios3/</code>. Login as nagiosadmin. The password can be changed by executing the following command as root:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
htpasswd /etc/nagios3/htpasswd.users nagiosadmin</code>
</pre>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-restart">
      â </a>5.2.4.Â Restart</h3></div></div></div>
		
		 <a id="idm140006539490576" class="indexterm"></a>
		 <div class="para">
			Nagios uses the information collected by Ganglia. In case this information source is not available, nagios will send warning mails. To avoid being flooded by these mails when you need to restart Ganglia, you should first stop Nagios:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
/etc/init.d/nagios3 stop</code>
</pre>
		 <div class="para">
			Then you can restart Ganglia:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
/etc/init.d/ganglia-monitor restart</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
/etc/init.d/gmetad restart</code>
</pre>
		 <div class="para">
			After restarting Ganglia on the head-node you need to restart Ganglia on the compute nodes as well:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
dsh -a /etc/init.d/ganglia-monitor restart</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
dsh -a /etc/init.d/gmetric stop</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
dsh -a /etc/init.d/gmetric start</code>
</pre>
		 <div class="para">
			Finally you can start Nagios again
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
/etc/init.d/nagios3 start</code>
</pre>

	</div>
	
</div>
</div>
	 <div xml:lang="en-US" class="chapter" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="admin-man-chap-gen-adm">
      â </a>ChapterÂ 6.Â General Administration Tasks</h1></div></div></div><div class="toc"><dl class="toc"><dt><span class="section"><a href="#admin-man-sect-user-manage">6.1. User Management</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-add-user">6.1.1. Adding User Accounts</a></span></dt><dt><span class="section"><a href="#admin-man-sect-rem-user">6.1.2. Removing User Accounts</a></span></dt><dt><span class="section"><a href="#admin-man-sect-man-user-restr">6.1.3. Managing user restrictions</a></span></dt><dt><span class="section"><a href="#admin-man-sect-shell-stp">6.1.4. Shell Setup</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-stor-man">6.2. Storage Management</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-raid">6.2.1. Raid</a></span></dt><dt><span class="section"><a href="#admin-man-sect-vol-man">6.2.2. Logical Volume Management</a></span></dt><dt><span class="section"><a href="#admin-man-sect-ZFS">6.2.3. Zpools and ZFS</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-man-sect-os-pkg-man">6.3. OS Package Management</a></span></dt><dd><dl><dt><span class="section"><a href="#admin-man-sect-pkg-srcs">6.3.1. Package sources</a></span></dt><dt><span class="section"><a href="#admin-man-sect-dpkg">6.3.2. dpkg</a></span></dt><dt><span class="section"><a href="#admin-man-sect-apt">6.3.3. apt</a></span></dt><dt><span class="section"><a href="#admin-man-sect-debian-pkg-altern">6.3.4. Debian Package Alternatives</a></span></dt></dl></dd></dl></div>
	
	 <a id="idm140006573851248" class="indexterm"></a>
	 <div class="para">
		Qlustar supports the most common GPU hardware types for GPU computing. There are hardware dependent packages and general development tools.
	</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-user-manage">
      â </a>6.1.Â User Management</h2></div></div></div>
	
	 <a id="idm140006539477856" class="indexterm"></a>
	 <div class="para">
		Currently user management is done on the command line.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-add-user">
      â </a>6.1.1.Â Adding User Accounts</h3></div></div></div>
		
		 <a id="idm140006539475568" class="indexterm"></a>
		 <div class="para">
			Adding users is conveniently performed by invoking the script
		</div>
		 <div class="para">
			<code class="command">/usr/sbin/adduser.sh.</code> Example:
		</div>
		 <div class="para">
			
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
/usr/sbin/adduser.sh -u <em class="replaceable">username</em> -n <em class="replaceable">'real name'</em></code>
</pre>

		</div>
		 <div class="para">
			This script performs all the tasks necessary for creating a new user account. There are a number of options to this script that you can see when invoking it using the <code class="command">-h</code> flag. This script reads the configuration file <code class="filename">/etc/qlustar/common/adduser.cf</code> for default values. Please note that the user ids of new accounts should be greater than 1000 to avoid a conflict with existing system accounts.
		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-rem-user">
      â </a>6.1.2.Â Removing User Accounts</h3></div></div></div>
		
		 <a id="idm140006539468576" class="indexterm"></a>
		 <div class="para">
			Use the script <code class="filename">/usr/sbin/removeuser.sh</code> to remove a user account from the system. Example:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
removeuser.sh <em class="replaceable">username</em></code>
</pre>
		 <div class="para">
			To recursively remove the userâs home-directory as well add the <code class="command">-r</code> option:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
removeuser.sh -r <em class="replaceable">username</em></code>
</pre>
		 <div class="para">
			There are other options to this script that you can view when invoking it with the <code class="command">-h</code> flag. This script also uses the configuration file <code class="filename">/etc/qlustar/common/adduser.cf</code> for default values.
		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-man-user-restr">
      â </a>6.1.3.Â Managing user restrictions</h3></div></div></div>
		
		 <a id="idm140006539460880" class="indexterm"></a>
		 <div class="para">
			In the default configuration of a Qlustar cluster all NIS registered users are allowed to run commands on the cluster nodes provided such commands donât require root rights. However, it is possible for system administrators of a BEOBOX cluster to easily change this standard behaviour. First of all there is a parameter <em class="parameter"><code>NO_USER_LOGIN_ON_NODES</code></em> within the file <code class="filename">/etc/qlustar/common/beobox.conf</code> which value should be set to <em class="parameter"><code>true</code></em> as seen below
		</div>
		 
<pre class="screen">
# Disallow user logins to nodes not running a PBS job
NO_USER_LOGIN_ON_NODES=true
</pre>
		 <div class="para">
			Finally it is necessary to restart the boot script <code class="filename">/etc/init.d/torque-mom</code> on all the nodes:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
dsh <em class="replaceable">-a</em> /etc/init.d/torque-mom restart</code>
</pre>
		 <div class="para">
			Following this procedure the administrator can change the cluster behaviour so that users are constrained to only run PBS jobs trough the <code class="command">qsub</code> command on the nodes. Any other commands being issued by any user other than root will not be allowed to run.
		</div>
		 <div class="para">
			The first question arising here is what to do if the administrator would like the users, say <em class="replaceable">usr1</em> and <em class="replaceable">usr2</em> to be an exception to this restrictive rule. In that case it suffices to put the name of those privileged users in the file <code class="filename">/etc/qlustar/yp/group</code> as shown:
		</div>
		 
<pre class="screen">
users::100
admgroup::2001:usr1,usr2
</pre>
		 <div class="para">
			After the regeneration of the NIS database:
		</div>
		 
<pre class="screen">
# cd /var/yp
/var/yp:# make
</pre>
		 <div class="para">
			users <em class="replaceable">usr1</em> and <em class="replaceable">usr2</em> will be allowed to run commands according to their system rights.
		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-shell-stp">
      â </a>6.1.4.Â Shell Setup</h3></div></div></div>
		
		 <a id="idm140006539449216" class="indexterm"></a>
		 <div class="para">
			The Qlustar shell setup supports <em class="replaceable">tcsh</em> and <em class="replaceable">bash</em>. There are global initialization files that are used in both shells so you only have to modify one file for environment variables, aliases and path variables. The global files are:
		</div>
		 <div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/env</code></span></dt><dd>
					<div class="para">
						Use this file to add or modify environment variables that are not path variables. The syntax of this file is as follows: lines beginning with a hash sign (#) and empty lines are ignored. Every other line consists of a variable name and the value for this variable separated with a space. Example: the following line sets the variable <code class="envar">VISUAL</code> to <code class="command">vi</code>: 
<pre class="screen">
VISUAL vi
</pre>
						 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
							<div class="para">
								A file <code class="filename">~/.ql-env</code> in a userâs home directory can define personal environment variables in the same manner.
							</div>

						</div></div>

					</div>

				</dd><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/alias</code></span></dt><dd>
					<div class="para">
						Use this file to define shell aliases. It has the same syntax as the file <code class="filename">env</code> described above. Again, a personal <code class="filename">~/.ql-alias</code> file can define personal aliases.
					</div>

				</dd><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/paths</code></span></dt><dd>
					<div class="para">
						This directory contains files with a name of the form <em class="replaceable">varname</em><code class="filename">.Linux</code>. The <em class="replaceable">varname</em> is converted to upper case and specifies a âPATH likeâ environment variable (e.g. <code class="envar">PATH</code>, <code class="envar">MANPATH</code>, <code class="envar">LD_LIBRARY_PATH</code>, <code class="envar">CLASSPATH</code> , â¦ ). Each line in this file is a directory to add to this environment variable. If the line begins with a âpâ followed by a space followed by a directory, then this directory is prepended to the path variable otherwise it is appended. A user can create his/her own <code class="filename">~/.paths</code> directory and so can use the same setup.
					</div>

				</dd></dl></div>
		 <div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="admin-man-sect-bash-setup">
      â </a>6.1.4.1.Â Bash Setup</h4></div></div></div>
			
			 <a id="idm140006539433472" class="indexterm"></a>
			 <div class="para">
				We provide an extensible <span class="application"><strong>bash</strong></span> setup and bash is the recommended login shell. The bash startup consists of global settings and user settings. Global settings are stored in files under <code class="filename">/etc/qlustar/common/skel</code>. User settings are stored in files in the corresponding home directory.
			</div>
			 <div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/bash/bashrc</code></span></dt><dd>
						<div class="para">
							This file sources the other bash files. Do not modify.
						</div>

					</dd><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/bash/bash-vars</code></span></dt><dd>
						<div class="para">
							This file is used for setting bash variables.
						</div>

					</dd><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/bash/alias</code></span></dt><dd>
						<div class="para">
							This file defines bash aliases.
						</div>

					</dd><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/bash/functions</code></span></dt><dd>
						<div class="para">
							You can use this file if you plan making bash functions available to users.
						</div>

					</dd></dl></div>
			 <div class="para">
				The file <code class="filename">~/.bashrc</code> also sources the following user specific files which have the same meaning as the global bash files. 
				<div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
						<div class="para">
							<code class="filename">~/.bash/env</code>
						</div>

					</li><li class="listitem">
						<div class="para">
							<code class="filename">~/.bash/bash-vars</code>
						</div>

					</li><li class="listitem">
						<div class="para">
							<code class="filename">~/.bash/alias</code>
						</div>

					</li><li class="listitem">
						<div class="para">
							<code class="filename">~/.bash/functions</code>
						</div>

					</li></ul></div>

			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="admin-man-sect-tcsh-setup">
      â </a>6.1.4.2.Â Tcsh Setup</h4></div></div></div>
			
			 <a id="idm140006539416816" class="indexterm"></a>
			 <div class="para">
				We provide a similar setup for the <em class="replaceable">tcsh</em>. The following files are used:
			</div>
			 <div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/tcsh/tcshrc</code></span></dt><dd>
						<div class="para">
							This global tcshrc is sourced first and sources other startup files.
						</div>

					</dd><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/tcsh/tcsh-vars</code></span></dt><dd>
						<div class="para">
							Use this file to set tcsh variables.
						</div>

					</dd><dt><span class="term"><code class="filename">/etc/qlustar/common/skel/tcsh/alias</code></span></dt><dd>
						<div class="para">
							You can use this file to define tcsh aliases.
						</div>

					</dd></dl></div>
			 <div class="para">
				The file <code class="filename">~/.tcshrc</code> also sources the following use specific files which have the same meaning as the global tcsh files. 
				<div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
						<div class="para">
							<code class="filename">~/.tcsh/alias</code>
						</div>

					</li><li class="listitem">
						<div class="para">
							<code class="filename">~/.tcsh/env</code>
						</div>

					</li><li class="listitem">
						<div class="para">
							<code class="filename">~/.tcsh/tcsh-vars</code>
						</div>

					</li></ul></div>

			</div>

		</div>

	</div>
	
</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-stor-man">
      â </a>6.2.Â Storage Management</h2></div></div></div>
	
	 <a id="idm140006539403232" class="indexterm"></a>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-raid">
      â </a>6.2.1.Â Raid</h3></div></div></div>
		
		 <a id="idm140006539401632" class="indexterm"></a>
		 <div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="admin-man-sect-kernel-sftw-raid">
      â </a>6.2.1.1.Â Kernel Software RAID</h4></div></div></div>
			
			 <a id="idm140006539399264" class="indexterm"></a>
			 <div class="para">
				Software RAID is part of the Linux kernel. RAID configuration is done with the <code class="command">mdadm</code> command. It is used to manage the RAID devices (see man page). Status information is obtained form <code class="filename">/proc/mdstat.</code>
			</div>
			 <div class="variablelist"><dl class="variablelist"><dt><span class="term">How to replace a failed Disk in a Software RAID Setup</span></dt><dd>
						<div class="para">
							In case of a disk failure use <code class="command">mdadm</code> to remove the failed disk from the raid-array, and after replacing the disk first partition it as the old one and again use mdadm to include the new disk into the raid-array. A failed disk is marked with a (F) in <code class="filename">/proc/mdstat.</code>
						</div>

					</dd></dl></div>
			 <div class="para">
				Example:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
cat /proc/mdstat</code>

Personalities : [raid0] [raid1] [raid5] [multipath]
read_ahead 1024 sectors
md0 : active raid1 sdb1[1] sda1[0]
104320 blocks [2/2] [UU]
md1 : active raid1 sdb2[0](F) sda2[1]
17414336 blocks [2/1] [_U]
md2 : active raid1 sdb3[1] sda3[0]
18322048 blocks [2/2] [UU]
unused devices: &lt;none&gt;
</pre>
			 <div class="para">
				So disk <code class="filename">/dev/sdb</code> has failed. In the example, the disk error affected only <code class="filename">/dev/md1</code>, but partitions of the faulty disk are also part of <code class="filename">/dev/md0</code> and <code class="filename">/dev/md2</code>. So they need to be removed as well before the disk can be replaced. Hence, the following commands need to be executed:
			</div>
			 <div class="para">
				To remove the faulty partition:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm --manage /dev/md1 -r /dev/sdb2</code>
</pre>
			 <div class="para">
				To mark the other affected partitions on the disk as faulty and remove them:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm --manage /dev/md0 -f /dev/sdb1</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm --manage /dev/md0 -r /dev/sdb1</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm --manage /dev/md2 -f /dev/sdb3</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm --manage /dev/md2 -r /dev/sdb3</code>
</pre>
			 <div class="para">
				Now the disk is not accessed any more and can be removed. After the new disk has been inserted, repartition it:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
sfdisk -d /dev/sda | sfdisk /dev/sdb</code>
</pre>
			 <div class="para">
				Now start the resync
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm --manage /dev/md0 -a /dev/sdb1</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm --manage /dev/md1 -a /dev/sdb2</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm --manage /dev/md2 -a /dev/sdb3</code>
</pre>
			 <div class="para">
				To watch the resync process, you can enter:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
watch --differences=cumulative cat /proc/mdstat</code> 
</pre>
			 <div class="para">
				Press <span class="keycap"><strong>Ctrl</strong></span>+<span class="keycap"><strong>C</strong></span> to exit the display
			</div>

		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-vol-man">
      â </a>6.2.2.Â Logical Volume Management</h3></div></div></div>
		
		 <a id="idm140006539376784" class="indexterm"></a>
		 <div class="para">
			The Linux Logical Volume Manager (LVM) provides a convenient and flexible way of managing storage. Storage devices like hard discs or RAID sets are registered as <code class="literal">physical volumes</code>, and are then assigned to volume groups. Volume groups contain one or more <code class="literal">logical volumes</code>, which can be resized according to the storage space available in the volume group. New physical volumes can be added to or removed from a volume group at any time, thereby transparently enlarging or reducing the storage space available in a volume group. Filesystems are created on top of logical volumes.
		</div>
		 <div class="para">
			Examples:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
pvcreate /dev/sdb1</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
vgcreate vg0 /dev/sdb1</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
lvcreate -n scratch -L 1GB vg0</code>
</pre>
		 <div class="para">
			These commands declare <code class="filename">/dev/sdb1</code> as a physical volume, create the volume group vg0 with the physical volume <code class="filename">/dev/sdb1</code>, and create a logical volume <code class="filename">/dev/vg0/scratch</code> of size 1GB. You can now create a filesystem on this logical volume and mount it:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mkfs.ext4 /dev/vg0/scratch</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mount /dev/vg0/scratch /scratch</code>
</pre>
		 <div class="para">
			To increase the size of the filesystem you do not have to unmount it but you have to increase the logical volume before resizing the filesystem:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
lvextend -L +1G /dev/vg0/scratch</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
resize2fs /dev/vg0/scratch</code>
</pre>
		 <div class="para">
			This increased the filesystem by 1 Gb. If you want to decrease the size of the filesystem you first need to unmount it. After that decrease the filesystem and finally reduce the logical volume:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
unmount /scratch</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
e2fsck -f /dev/vg0/scratch</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
resize2fs /dev/vg0/scratch 500M</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
lvreduce -L 500M /dev/vg0/scratch</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mount /dev/vg0/scratch /scratch</code>
</pre>
		 <div class="para">
			This decreased the filesystem to 500Mb. To check how much space is left in a volume group use the command <code class="command">vgdisplay</code>. Look for a line showing Free Size.
		</div>
		 <div class="para">
			Frequent commands: 
			<div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						Physical volumes: <code class="command">pvcreate</code>
					</div>

				</li><li class="listitem">
					<div class="para">
						Volume groups: <code class="command">vgscan</code>, <code class="command">vgchange</code>, <code class="command">vgdisplay</code>, <code class="command">vgcreate</code>, <code class="command">vgremove</code>
					</div>

				</li><li class="listitem">
					<div class="para">
						Logical volumes: <code class="command">lvdisplay</code>,<code class="command">lvcreate</code>, <code class="command">lvextend</code>, <code class="command">lvreduce</code>, <code class="command">lvremove</code>
					</div>

				</li></ul></div>

		</div>

	</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-ZFS">
      â </a>6.2.3.Â Zpools and ZFS</h3></div></div></div>
	
	 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
		<div class="para">
			This section borrows heavily from the excellent <a href="https://pthree.org/2012/04/17/install-zfs-on-debian-gnulinux/___blank___">ZFS tutorial series</a> by Aaron Toponce.
		</div>

	</div></div>
	 <div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="admin-man-sect-zpools-admin">
      â </a>6.2.3.1.Â Zpool Administration</h4></div></div></div>
		
		 <a id="idm140006539349360" class="indexterm"></a>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-VDEVs">
      â </a>6.2.3.1.1.Â VDEVs</h5></div></div></div>
			
			 <a id="idm140006539346416" class="indexterm"></a>
			 <h6><a id="idm140006539345264">
      â </a> Virtual Device Introduction </h6> <div class="para">
				To start, we need to understand the concept of virtual devices, or VDEVs, as ZFS uses them internally extensively. If you are already familiar with RAID, then this concept is not new to you, although you may not have referred to it as âVDEVsâ. Basically, we have a meta-device that represents one or more physical devices. In Linux software RAID, you might have a <code class="filename">/dev/md0</code> device that represents a RAID-5 array of 4 disks. In this case, <code class="filename">/dev/md0</code> would be your âVDEVâ.
			</div>
			 <div class="para">
				There are seven types of VDEVs in ZFS:
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="orderedlist"><ol><li class="listitem">
					<div class="para">
						disk (default)- The physical hard drives in your system.
					</div>

				</li><li class="listitem">
					<div class="para">
						file- The absolute path of pre-allocated files/images.
					</div>

				</li><li class="listitem">
					<div class="para">
						mirror- Standard software RAID-1 mirror.
					</div>

				</li><li class="listitem">
					<div class="para">
						raidz1/2/3- Non-standard distributed parity-based software RAID levels.
					</div>

				</li><li class="listitem">
					<div class="para">
						spare- Hard drives marked as a âhot spareâ for ZFS software RAID.
					</div>

				</li><li class="listitem">
					<div class="para">
						cache- Device used for a level 2 adaptive read cache (L2ARC).
					</div>

				</li><li class="listitem">
					<div class="para">
						log- A separate log (SLOG) called the âZFS Intent Logâ or ZIL.
					</div>

				</li></ol></div>
			 <div class="para">
				Itâs important to note that VDEVs are always dynamically striped. This will make more sense as we cover the commands below. However, suppose there are 4 disks in a ZFS stripe. The stripe size is calculated by the number of disks and the size of the disks in the array. If more disks are added, the stripe size can be adjusted as needed for the additional disk. Thus, the dynamic nature of the stripe.
			</div>
			 <h6><a id="idm140006539336240">
      â </a> Some zpool caveats </h6> <div class="para">
				I would be amiss if I didnât meantion some of the caveats that come with ZFS:
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						Once a device is added to a VDEV, it cannot be removed.
					</div>

				</li><li class="listitem">
					<div class="para">
						You cannot shrink a zpool, only grow it.
					</div>

				</li><li class="listitem">
					<div class="para">
						RAID-0 is faster than RAID-1, which is faster than RAIDZ-1, which is faster than RAIDZ-2, which is faster than RAIDZ-3.
					</div>

				</li><li class="listitem">
					<div class="para">
						Hot spares are not dynamically added unless you enable the setting, which is off by default.
					</div>

				</li><li class="listitem">
					<div class="para">
						A zpool will not dynamically rezise when larger disks fill the pool unless you enable the setting <span class="emphasis"><em>before</em></span> your first disk replacement, which is off by default.
					</div>

				</li><li class="listitem">
					<div class="para">
						A zpool will know about âadvanced formatâ 4K sector drives <span class="emphasis"><em>if and only if</em></span> the drive reports such.
					</div>

				</li><li class="listitem">
					<div class="para">
						Deduplication is <span class="emphasis"><em>extremely expensive</em></span>, will cause performance degredation if not enough RAM is installed, and is pool-wide, not local to filesystems.
					</div>

				</li><li class="listitem">
					<div class="para">
						On the other hand, compression is <span class="emphasis"><em>extremely cheap</em></span> on the CPU, yet it is disabled by default.
					</div>

				</li><li class="listitem">
					<div class="para">
						ZFS suffers a great deal from fragmentation, and full zpools will âfeelâ the performance degredation.
					</div>

				</li><li class="listitem">
					<div class="para">
						ZFS suports encryption natively, but it is <span class="emphasis"><em>not</em></span> Free Software. It is proprietary copyrighted by Oracle.
					</div>

				</li></ul></div>
			 <div class="para">
				For the next examples, we will assume 4 drives: <code class="filename">/dev/sde</code>, <code class="filename">/dev/sdf</code>, <code class="filename">/dev/sdg</code> and <code class="filename">/dev/sdh</code>, all 8 GB USB thumb drives. Between each of the commands, if you are following along, then make sure you follow the cleanup step at the end of each section.
			</div>
			 <h6><a id="idm140006539322032">
      â </a> A simple pool </h6> <div class="para">
				Letâs start by creating a simple zpool wyth my 4 drives. I could create a zpool named âtankâ with the following command:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create tank sde sdf sdg sdh</code>
</pre>
			 <div class="para">
				In this case, Iâm using four disk VDEVs. Notice that Iâm not using full device paths, although I could. Because VDEVs are always dynamically striped, this is effectively a RAID-0 between four drives (no redundancy). We should also check the status of the zpool:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

pool: tank
state: ONLINE
scan: none requested
config:

       NAME        STATE     READ WRITE CKSUM
       tank        ONLINE       0     0     0
         sde       ONLINE       0     0     0
         sdf       ONLINE       0     0     0 
         sdg       ONLINE       0     0     0
         sdh       ONLINE       0     0     0

errors: No known data errors	 
</pre>
			 <div class="para">
				Letâs tear down the zpool, and create a new one. Run the following before continuing, if youâre following along in your own terminal:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>
</pre>
			 <h6><a id="idm140006539316448">
      â </a> A simple mirrored zpool </h6> <div class="para">
				In this next example, I wish to mirror all four drives (<code class="filename">/dev/sde</code>, <code class="filename">/dev/sdf</code>, <code class="filename">/dev/sdg</code> and <code class="filename">/dev/sdh</code>). So, rather than using the disk VDEV, Iâll be using âmirrorâ. The command is as follows:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create tank mirror sde sdf sdg sdh</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: tank
 state: ONLINE
  scan: none requested
config:

       NAME        STATE     READ WRITE CKSUM
       tank        ONLINE       0     0     0
         mirror-0  ONLINE       0     0     0
           sde     ONLINE       0     0     0
           sdf     ONLINE       0     0     0
           sdg     ONLINE       0     0     0
           sdh     ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				Notice that âmirror-0â³ is now the VDEV, with each physical device managed by it. As mentioned earlier, this would be analogous to a Linux software RAID â/dev/md0â³ device representing the four physical devices. Letâs now clean up our pool, and create another.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>
</pre>
			 <h6><a id="idm140006539310240">
      â </a> Nested VDEVs </h6> <div class="para">
				VDEVs can be nested. A perfect example is a standard RAID-1+0 (commonly referred to as âRAID-10â³). This is a stripe of mirrors. In order to specify the nested VDEVs, I just put them on the command line in order (emphasis mine):
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create tank mirror sde sdf mirror sdg sdh</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status</code>

  pool: tank
 state: ONLINE
  scan: none requested
config:

       NAME        STATE     READ WRITE CKSUM
       tank        ONLINE       0     0     0
         mirror-0  ONLINE       0     0     0
           sde     ONLINE       0     0     0
           sdf     ONLINE       0     0     0
         mirror-1  ONLINE       0     0     0
           sdg     ONLINE       0     0     0
           sdh     ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				The first VDEV is âmirror-0â³ which is managing <code class="filename">/dev/sde</code> and <code class="filename">/dev/sdf</code>. This was done by calling â<code class="command">mirror sde sdf</code>â. The second VDEV is âmirror-1â³ which is managing <code class="filename">/dev/sdg</code> and <code class="filename">/dev/sdh</code>. This was done by calling â<code class="command">mirror sdg sdh</code>â. Because VDEVs are always dynamically striped, âmirror-0â³ and âmirror-1â³ are striped, thus creating the RAID-1+0 setup. Donât forget to cleanup before continuing:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>
</pre>
			 <h6><a id="idm140006539302960">
      â </a> File VDEVs </h6> <div class="para">
				As mentioned, pre-allocated files can be used per setting up zpools on your existing ext4 filesystem (or whatever). It should be noted that this is meant entirely for testing purposes, and not for storing production data. Using files is a great way to have a sandbox, where you can test compression ratio, the size of the deduplication table, or other things without actually committing production data to it. When creating file VDEVs, you cannot use relative paths, but must use absolute paths. Further, the image files must be preallocated, and not sparse files or thin provisioned. Letâs see how this works:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
for i in {1..4}; do dd if=/dev/zero of=/tmp/file$i bs=1G count=4
&amp;&gt; /dev/null; done</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create tank /tmp/file1 /tmp/file2 /tmp/file3 /tmp/file4</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: tank
 state: ONLINE
  scan: none requested
config:

             NAME          STATE     READ WRITE CKSUM
             tank          ONLINE       0     0     0
               /tmp/file1  ONLINE       0     0     0
               /tmp/file2  ONLINE       0     0     0
               /tmp/file3  ONLINE       0     0     0
               /tmp/file4  ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				In this case, we created a RAID-0. We used preallocated files using <code class="filename">/dev/zero</code> that are each 4GB in size. Thus, the size of our zpool is 16 GB in usable space. Each file, as with our first example using disks, is a VDEV. Of course, you can treat the files as disks, and put them into a mirror configuration, RAID-1+0, RAIDZ-1 (coming in the next post), etc.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>
</pre>
			 <h6><a id="idm140006539296608">
      â </a> Hybrid pools </h6> <div class="para">
				This last example should show you the complex pools you can setup by using different VDEVs. Using our four file VDEVs from the previous example, and our four disk VDEVs <code class="filename">/dev/sde</code> through <code class="filename">/dev/sdh</code>, letâs create a hybrid pool with cache and log drives. Again, I emphasized the nested VDEVs for clarity:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create tank mirror /tmp/file1 /tmp/file2 mirror /tmp/file3
/tmp/file4 log mirror sde sdf cache sdg sdh</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: tank
 state: ONLINE
  scan: none requested
config:

               NAME            STATE     READ WRITE CKSUM
               tank            ONLINE       0     0     0
                 mirror-0      ONLINE       0     0     0
                   /tmp/file1  ONLINE       0     0     0
                   /tmp/file2  ONLINE       0     0     0
                 mirror-1      ONLINE       0     0     0
                   /tmp/file3  ONLINE       0     0     0
                   /tmp/file4  ONLINE       0     0     0
               logs
                 mirror-2      ONLINE       0     0     0
                   sde         ONLINE       0     0     0
                   sdf         ONLINE       0     0     0
               cache
                 sdg           ONLINE       0     0     0
                 sdh           ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				Thereâs a lot going on here, so letâs dissect it. First, we created a RAID-1+0 using our four preallocated image files. Notice the VDEVs âmirror-0â³ and âmirror-1â³, and what they are managing. Second, we created a third VDEV called âmirror-2â³ that actually is not used for storing data in the pool, but is used as a ZFS intent log, or ZIL. Weâll cover the ZIL in more detail in another post. Then we created two VDEVs for caching data called âsdgâ and âsdhâ. The are standard disk VDEVs that weâve already learned about. However, they are also managed by the âcacheâ VDEV. So, in this case, weâve used 6 of the 7 VDEVs listed above, the only one missing is âspareâ.
			</div>
			 <div class="para">
				Noticing the indentation will help you see what VDEV is managing what. The âtankâ pool is comprised of the âmirror-0â³ and âmirror-1â³ VDEVs for long-term persistent storage. The ZIL is magaged by âmirror-2â³, which is comprised of <code class="filename">/dev/sde</code> and <code class="filename"> /dev/sdf</code>. The read-only cache VDEV is managed by two disks, <code class="filename">/dev/sdg</code> and <code class="filename">/dev/sdh</code>. Neither the âlogsâ nor the âcacheâ are long-term storage for the pool, thus creating a âhybrid poolâ setup.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>
</pre>
			 <h6><a id="idm140006539287600">
      â </a> Real life example </h6> <div class="para">
				In production, the files would be physical disk, and the ZIL and cache would be fast SSDs. Here is my current zpool setup which is storing this blog, among other things:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status pool</code>

  pool: pool
 state: ONLINE
  scan: scrub repaired 0 in 2h23m with 0 errors on Sun Dec  2 02:23:44 2012
config:

                NAME                                              STATE     READ WRITE CKSUM
                pool                                              ONLINE       0     0     0
                  raidz1-0                                        ONLINE       0     0     0
                    sdd                                           ONLINE       0     0     0
                    sde                                           ONLINE       0     0     0
                    sdf                                           ONLINE       0     0     0
                    sdg                                           ONLINE       0     0     0
                logs
                  mirror-1                                        ONLINE       0     0     0
                    ata-OCZ-REVODRIVE_OCZ-33W9WE11E9X73Y41-part1  ONLINE       0     0     0
                    ata-OCZ-REVODRIVE_OCZ-X5RG0EIY7MN7676K-part1  ONLINE       0     0     0
                cache
                  ata-OCZ-REVODRIVE_OCZ-33W9WE11E9X73Y41-part2    ONLINE       0     0     0
                  ata-OCZ-REVODRIVE_OCZ-X5RG0EIY7MN7676K-part2    ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				Notice that my âlogsâ and âcacheâ VDEVs are OCZ Revodrive SSDs, while the four platter disks are in a RAIDZ-1 VDEV (RAIDZ will be discussed in the next post). However, notice that the name of the SSDs is âata-OCZ-REVODRIVE_OCZ-33W9WE11E9X73Y41-part1â³, etc. These are found in <code class="filename">/dev/disk/by-id/</code>. The reason I chose these instead of âsdbâ and âsdcâ is because the cache and log devices donât necessarily store the same ZFS metadata. Thus, when the pool is being created on boot, they may not come into the pool, and could be missing. Or, the motherboard may assign the drive letters in a different order. This isnât a problem with the main pool, but is a big problem on GNU/Linux with logs and cached devices. Using the device name under <code class="filename">/dev/disk/by-id/</code> ensures greater persistence and uniqueness.
			</div>
			 <div class="para">
				Also do notice the simplicity in the implementation. Consider doing something similar with LVM, RAID and ext4. You would need to do the following:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mdadm -C /dev/md0 -l 0 -n 4 /dev/sde /dev/sdf /dev/sdg /dev/sdh</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
pvcreate /dev/md0</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
vgcreate /dev/md0 tank</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
lvcreate -l 100%FREE -n videos tank</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mkfs.ext4 /dev/tank/videos</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mkdir -p /tank/videos</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mount -t ext4 /dev/tank/videos /tank/videos</code>
</pre>
			 <div class="para">
				The above was done in ZFS (minus creating the logical volume, which will get to later) with one command, rather than seven.
			</div>
			 <h6><a id="idm140006539276336">
      â </a> Conclusion </h6> <div class="para">
				This should act as a good starting point for getting the basic understanding of zpools and VDEVs. The rest of it is all downhill from here. Youâve made it over the âbig hurdleâ of understanding how ZFS handles pooled storage. We still need to cover RAIDZ levels, and we still need to go into more depth about log and cache devices, as well as pool settings, such as deduplication and compression, but all of these will be handled in separate posts. Then we can get into ZFS filesystem datasets, their settings, and advantages and disagvantages. But, you now have a head start on the core part of ZFS pools.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-raidz">
      â </a>6.2.3.1.2.Â RAIDZ</h5></div></div></div>
			
			 <a id="idm140006539274272" class="indexterm"></a>
			 <h6><a id="idm140006539347968">
      â </a> Self-healing RAID </h6> <div class="para">
				ZFS can detect silent errors, and fix them on the fly. Suppose for a moment that there is bad data on a disk in the array, for whatever reason. When the application requests the data, ZFS constructs the stripe as we just learned, and compares each block against a SHA-256 checksum in the metadata. If the read stripe does not match the checksum, ZFS finds the corrupted block, it then reads the parity, and fixes it through combinatorial reconstruction. It then returns good data to the application. This is all accomplished in ZFS itself, without the help of special hardware. Another aspect of the RAIDZ levels is the fact that if the stripe is longer than the disks in the array, if there is a disk failure, not enough data with the parity can reconstruct the data. Thus, ZFS will mirror some of the data in the stripe to prevent this from happening.
			</div>
			 <div class="para">
				Again, if your RAID and filesystem are separate products, they are not aware of each other, so detecting and fixing silent data errors is not possible. So, with that out of the way, letâs build some RAIDZ pools. As with my previous post, Iâll be using 5 USB thumb drives <code class="filename">/dev/sde</code>, <code class="filename">/dev/sdf</code>, <code class="filename">/dev/sdg</code>, <code class="filename">/dev/sdh</code> and <code class="filename">/dev/sdi</code> which are all 8 GB in size.
			</div>
			 <h6><a id="idm140006539268944">
      â </a> RAIDZ-1 </h6> <div class="para">
				RAIDZ-1 is similar to RAID-5 in that there is a single parity bit distributed across all the disks in the array. The stripe width is variable, and could cover the exact width of disks in the array, fewer disks, or more disks, as evident in the image above. This still allows for one disk failure to maintain data. Two disk failures would result in data loss. A minimum of 3 disks should be used in a RAIDZ-1. The capacity of your storage will be the number of disks in your array times the storage of the smallest disk, minus one disk for parity storage (there is a caveat to zpool storage sizes Iâll get to in another post). So in my example, I should have roughly 16 GB of usable disk.
			</div>
			 <div class="para">
				To setup a zpool with RAIDZ-1, we use the âraidz1â³ VDEV, in this case using only 3 USB drives:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create tank raidz1 sde sdf sdg</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: pool
 state: ONLINE
  scan: none requested
config:

             NAME          STATE     READ WRITE CKSUM
             pool          ONLINE       0     0     0
               raidz1-0    ONLINE       0     0     0
                 sde       ONLINE       0     0     0
                 sdf       ONLINE       0     0     0
                 sdg       ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				Cleanup before moving on, if following in your terminal:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>
</pre>
			 <h6><a id="idm140006539263344">
      â </a> RAIDZ-2 </h6> <div class="para">
				RAIDZ-2 is similar to RAID-6 in that there is a dual parity bit distributed across all the disks in the array. The stripe width is variable, and could cover the exact width of disks in the array, fewer disks, or more disks, as evident in the image above. This still allows for two disk failures to maintain data. Three disk failures would result in data loss. A minimum of 4 disks should be used in a RAIDZ-2. The capacity of your storage will be the number of disks in your array times the storage of the smallest disk, minus two disks for parity storage. So in my example, I should have roughly 16 GB of usable disk.
			</div>
			 <div class="para">
				To setup a zpool with RAIDZ-2, we use the âraidz2â³ VDEV:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create tank raidz2 sde sdf sdg sdh</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: pool
 state: ONLINE
  scan: none requested
config:

            NAME          STATE     READ WRITE CKSUM
            pool          ONLINE       0     0     0
              raidz2-0    ONLINE       0     0     0
                sde       ONLINE       0     0     0
                sdf       ONLINE       0     0     0
                sdg       ONLINE       0     0     0
                sdh       ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				Cleanup before moving on, if following in your terminal:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>
</pre>
			 <h6><a id="idm140006539257904">
      â </a> RAIDZ-3 </h6> <div class="para">
				RAIDZ-3 does not have a standardized RAID level to compare it to. However, it is the logical continuation of RAIDZ-1 and RAIDZ-2 in that there is a triple parity bit distributed across all the disks in the array. The stripe width is variable, and could cover the exact width of disks in the array, fewer disks, or more disks, as evident in the image above. This still allows for three disk failures to maintain data. Four disk failures would result in data loss. A minimum of 5 disks should be used in a RAIDZ-3. The capacity of your storage will be the number of disks in your array times the storage of the smallest disk, minus three disks for parity storage. So in out example, we should have roughly 16 GB of usable disk.
			</div>
			 <div class="para">
				To setup a zpool with RAIDZ-3, we use the âraidz3â³ VDEV:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create tank raidz3 sde sdf sdg sdh sdi</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: pool
 state: ONLINE
  scan: none requested
config:

                NAME          STATE     READ WRITE CKSUM
                pool          ONLINE       0     0     0
                  raidz3-0    ONLINE       0     0     0
                    sde       ONLINE       0     0     0
                    sdf       ONLINE       0     0     0
                    sdg       ONLINE       0     0     0
                    sdh       ONLINE       0     0     0
                    sdi       ONLINE       0     0     0

 errors: No known data errors
</pre>
			 <div class="para">
				Cleanup before moving on, if following in your terminal:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>
</pre>
			 <h6><a id="idm140006539252176">
      â </a> Performance Considerations </h6> <div class="para">
				Lastly, in terms of performance, mirrors will always outperform RAIDZ levels. On both reads and writes. Further, RAIDZ-1 will outperform RAIDZ-2, which it turn will outperform RAIDZ-3. The more parity bits you have to calculate, the longer itâs going to take to both read and write the data. Of course, you can always add striping to your VDEVs to maximize on some of this performance. Nested RAID levels, such as RAID-1+0 are considered âthe Cadillac of RAID levelsâ due to the flexibility in which you can lose disks without parity, and the throughput you get from the stripe. So, in a nutshell, from fastest to slowest, your non-nested RAID levels will perform as:
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						RAID-0 (fastest)
					</div>

				</li><li class="listitem">
					<div class="para">
						RAID-1
					</div>

				</li><li class="listitem">
					<div class="para">
						RAIDZ-1
					</div>

				</li><li class="listitem">
					<div class="para">
						RAIDZ-2
					</div>

				</li><li class="listitem">
					<div class="para">
						RAIDZ-3 (slowest)
					</div>

				</li></ul></div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-ex-import-strg-pools">
      â </a>6.2.3.1.3.Â Exporting and Importing Storage Pools</h5></div></div></div>
			
			 <a id="idm140006539245280" class="indexterm"></a>
			 <h6><a id="idm140006539244128">
      â </a> Motivation </h6> <div class="para">
				As a GNU/Linux storage administrator, you may come across the need to move your storage from one server to another. This could be accomplished by physically moving the disks from one storage box to another, or by copying the data from the old live running system to the new. we will cover both cases in this series. The latter deals with sending and receiving ZFS snapshots, a topic that will take us some time getting to. This post will deal with the former; that is, physically moving the drives.
			</div>
			 <div class="para">
				One slick feature of ZFS is the ability to export your storage pool, so you can disassemble the drives, unplug their cables, and move the drives to another system. Once on the new system, ZFS gives you the ability to import the storage pool, regardless of the order of the drives. A good demonstration of this is to grab some USB sticks, plug them in, and create a ZFS storage pool. Then export the pool, unplug the sticks, drop them into a hat, and mix them up. Then, plug them back in at any random order, and re-import the pool on a new box. In fact, ZFS is smart enough to detect endianness. In other words, you can export the storage pool from a big endian system, and import the pool on a little endian system, without hiccup.
			</div>
			 <h6><a id="idm140006539241696">
      â </a> Exporting Storage Pools </h6> <div class="para">
				When the migration is ready to take place, before unplugging the power, you need to export the storage pool. This will cause the kernel to flush all pending data to disk, writes data to the disk acknowledging that the export was done, and removes all knowledge that the storage pool existed in the system. At this point, itâs safe to shut down the computer, and remove the drives.
			</div>
			 <div class="para">
				If you do not export the storage pool before removing the drives, you will not be able to import the drives on the new system, and you might not have gotten all unwritten data flushed to disk. Even though the data will remain consistent due to the nature of the filesystem, when importing, it will appear to the old system as a faulted pool. Further, the destination system will refuse to import a pool that has not been explicitly exported. This is to prevent race conditions with network attached storage that may be already using the pool.
			</div>
			 <div class="para">
				To export a storage pool, use the following command:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool export tank</code>
</pre>
			 <div class="para">
				This command will attempt to unmount all ZFS datasets as well as the pool. By default, when creating ZFS storage pools and filesystems, they are automatically mounted to the system. There is no need to explicitly unmount the filesystems as you with with ext3 or ext4. The export will handle that. Further, some pools may refuse to be exported, for whatever reason. You can pass the <code class="command">-f</code> switch if needed to force the export.
			</div>
			 <h6><a id="idm140006539237008">
      â </a> Importing Storage Pools </h6> <div class="para">
				Once the drives have been physically installed into the new server, you can import the pool. Further, the new system may have multiple pools installed, to which you will want to determine which pool to import, or to import them all. If the storage pool âtankâ does not already exist on the new server, and this is the pool you wish to import, then you can run the following command:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool import tank</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

 state: ONLINE
  scan: none requested
config:

                 NAME        STATE     READ WRITE CKSUM
                 tank        ONLINE       0     0     0
                   mirror-0  ONLINE       0     0     0
                     sde     ONLINE       0     0     0
                     sdf     ONLINE       0     0     0
                   mirror-1  ONLINE       0     0     0
                     sdg     ONLINE       0     0     0
                     sdh     ONLINE       0     0     0
                   mirror-2  ONLINE       0     0     0
                     sdi     ONLINE       0     0     0
                     sdj     ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				Your storage pool state may not be <span class="property">ONLINE</span>, meaning that everything is healthy. If the system does not recognize a disk in your pool, you may get a <span class="property">DEGRADED</span> state. If one or more of the drives appear as faulty to the system, then you may get a <span class="property">FAULTED</span> state in your pool. You will need to troubleshoot what drives are causing the problem, and fix accordingly.
			</div>
			 <div class="para">
				You can import multiple pools simultaneously by either specifying each pool as an argument, or by passing the <code class="command">-a</code> switch for importing all discovered pools. For importing the two pools âtank1â³ and âtank2â³, type:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool import tank1 tank2</code>
</pre>
			 <div class="para">
				For importing all known pools, type:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool import -a</code>
</pre>
			 <h6><a id="idm140006539228096">
      â </a> Recovering A Destroyed Pool </h6> <div class="para">
				If a ZFS storage pool was previously destroyed, the pool can still be imported to the system. Destroying a pool doesnât wipe the data on the disks, so the metadata is still in tact, and the pool can still be discovered. Letâs take a clean pool called âtankâ, destroy it, move the disks to a new system, then try to import the pool. You will need to pass the <code class="command">-D</code> switch to tell ZFS to import a destroyed pool. Do not provide the pool name as an argument, as you would normally do:
			</div>
			 
<pre class="screen">
(server A)
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool destroy tank</code>

(server B)
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool import -D</code>

  pool: tank
    id: 17105118590326096187
 state: ONLINE (DESTROYED)
action: The pool can be imported using its name or numeric identifier.
config:

                 tank        ONLINE
                   mirror-0  ONLINE
                     sde     ONLINE
                     sdf     ONLINE
                   mirror-1  ONLINE
                     sdg     ONLINE
                     sdh     ONLINE
                   mirror-2  ONLINE
                     sdi     ONLINE
                     sdj     ONLINE
                
                
  pool: tank
    id: 2911384395464928396
 state: UNAVAIL (DESTROYED)
status: One or more devices are missing from the system.
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://zfsonlinux.org/msg/ZFS-8000-6X
config:

                 tank          UNAVAIL  missing device
                   sdk         ONLINE
                   sdr         ONLINE

 Additional devices are known to be part of this pool, though their
 exact configuration cannot be determined.
</pre>
			 <div class="para">
				Notice that the state of the pool is <span class="property">ONLINE (DESTROYED)</span>. Even though the pool is <span class="property">ONLINE</span>, it is only partially online. Basically, itâs only been discovered, but itâs not available for use. If you run the <code class="command">df</code> command, you will find that the storage pool is not mounted. This means the ZFS filesystem datasets are not available, and you currently cannot store data into the pool. However, ZFS has found the pool, and you can bring it fully ONLINE for standard usage by running the import command one more time, this time specifying the pool name as an argument to import:
			</div>
			 
<pre class="screen">
(server B)
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool import -D tank</code>

cannot import 'tank': more than one matching pool
import by numeric ID instead

(server B)
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool import -D 17105118590326096187</code>

(server B)
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: tank
 state: ONLINE
  scan: none requested
config:

                   NAME        STATE     READ WRITE CKSUM
                   tank        ONLINE       0     0     0
                     mirror-0  ONLINE       0     0     0
                       sde     ONLINE       0     0     0
                       sdf     ONLINE       0     0     0
                     mirror-1  ONLINE       0     0     0
                       sdg     ONLINE       0     0     0
                       sdh     ONLINE       0     0     0
                     mirror-2  ONLINE       0     0     0
                       sdi     ONLINE       0     0     0
                       sdj     ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				Notice that ZFS was warning me that it found more than on storage pool matching the name âtankâ, and to import the pool, I must use its unique identifier. So, I pass that as an argument from my previous import. This is because in my previous output, we can see there are two known pools with the pool name âtankâ. However, after specifying its ID, I was able to successfully bring the storage pool to full <span class="property">ONLINE</span> status. You can identify this by checking its status:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: tank
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
        pool will no longer be accessible on older software versions.
 scrub: none requested
config:

                 NAME        STATE     READ WRITE CKSUM
                 tank        ONLINE       0     0     0
                   mirror-0  ONLINE       0     0     0
                     sde     ONLINE       0     0     0
                     sdf     ONLINE       0     0     0
                   mirror-1  ONLINE       0     0     0
                     sdg     ONLINE       0     0     0
                     sdh     ONLINE       0     0     0
                   mirror-2  ONLINE       0     0     0
                     sdi     ONLINE       0     0     0
                     sdj     ONLINE       0     0     0
</pre>
			 <h6><a id="idm140006539215072">
      â </a> Upgrading Storage Pools </h6> <div class="para">
				One thing that may crop up when migrating disk, is that there may be different pool and filesystem versions of the software. For example, you may have exported the pool on a system running pool version 20, while importing into a system with pool version 28 support. As such, you can upgrade your pool version to use the latest software for that release. As is evident with the previous example, it seems that the new server has an update version of the software. We are going to upgrade.
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="warning"><div class="admonition_header"><p><strong>Warning</strong></p></div><div class="admonition">
				<div class="para">
					Once you upgrade your pool to a newer version of ZFS, older versions will not be able to use the storage pool. So, make sure that when you upgrade the pool, you know that there will be no need for going back to the old system. Further, there is no way to revert the upgrade and revert to the old version.
				</div>

			</div></div>
			 <div class="para">
				First, we can see a brief description of features that will be available to the pool:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool upgrade -v</code>

This system is currently running ZFS pool version 28.
The following versions are supported:
          
          VER   DESCRIPTION
          ---   ---------------------------------------------------
           1    Initial ZFS version
           2    Ditto blocks (replicated metadata)
           3    Hot spares and double parity RAID-Z
           4    zpool history
           5    Compression using the gzip algorithm
           6    bootfs pool property
           7    Separate intent log devices
           8    Delegated administration
           9    refquota and refreservation properties
           10   Cache devices
           11   Improved scrub performance
           12   Snapshot properties
           13   snapused property
           14   passthrough-x aclinherit
           15   user/group space accounting
           16   stmf property support
           17   Triple-parity RAID-Z
           18   Snapshot user holds
           19   Log device removal
           20   Compression using zle (zero-length encoding)
           21   Deduplication
           22   Received properties
           23   Slim ZIL
           24   System attributes
           25   Improved scrub stats
           26   Improved snapshot deletion performance
           27   Improved snapshot creation performance
           28   Multiple vdev replacements

For more information on a particular version, including supported releases,
see the ZFS Administration Guide.
</pre>
			 <div class="para">
				So, letâs perform the upgrade to get to version 28 of the pool:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool upgrade -a</code>
</pre>
			 <div class="para">
				As a sidenote, when using ZFS on Linux, the RPM and Debian packages will contain an <span class="application"><strong>/etc/init.d/zfs</strong></span> init script for setting up the pools and datasets on boot. This is done by importing them on boot. However, at shutdown, the init script does not export the pools. Rather, it just unmounts them. So, if you migrate the disk to another box after only shutting down, you will be not be able to import the storage pool on the new box.
			</div>
			 <h6><a id="idm140006539206960">
      â </a> Conclusion </h6> <div class="para">
				There are plenty of situations where you may need to move disk from one storage server to another. Thankfully, ZFS makes this easy with exporting and importing pools. Further, the <code class="command">zpool</code> command has enough subcommands and switches to handle the most common scenarios when a pool will not export or import. Towards the very end of the series, weâll discuss the <code class="command">zdb</code> command, and how it may be useful here. But at this point, steer clear of zdb, and just focus on keeping your pools in order, and properly exporting and importing them as needed.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-scrub-and-resilvr">
      â </a>6.2.3.1.4.Â Scrub and Resilver</h5></div></div></div>
			
			 <a id="idm140006539203472" class="indexterm"></a>
			 <h6><a id="idm140006539202320">
      â </a> Standard Validation </h6> <div class="para">
				In GNU/Linux, we have a number of filesystem checking utilities for verifying data integrity on the disk. This is done through the âfsckâ utility. However, it has a couple major drawbacks. First, you must fsck the disk offline if you are intending on fixing data errors. This means downtime. So, you must use the <code class="command">umount</code> command to unmount your disks, before the fsck. For root partitions, this further means booting from another medium, like a CDROM or USB stick. Depending on the size of the disks, this downtime could take hours. Second, the filesystem, such as ext3 or ext4, knows nothing of the underlying data structures, such as LVM or RAID. You may only have a bad block on one disk, but a good block on another disk. Unfortunately, Linux software RAID has no idea which is good or bad, and from the perspective of ext3 or ext4, it will get good data if read from the disk containing the good block, and corrupted data from the disk containing the bad block, without any control over which disk to pull the data from, and fixing the corruption. These errors are known as <span class="errorname">silent data errors</span>, and there is really nothing you can do about it with the standard GNU/Linux filesystem stack.
			</div>
			 <h6><a id="idm140006539199568">
      â </a> ZFS Scrubbing </h6> <div class="para">
				With ZFS on Linux, detecting and correcting silent data errors is done through scrubbing the disks. This is similar in technique to ECC RAM, where if an error resides in the ECC DIMM, you can find another register that contains the good data, and use it to fix the bad register. This is an old technique that has been around for a while, so itâs surprising that itâs not available in the standard suite of journaled filesystems. Further, just like you can scrub ECC RAM on a live running system, without downtime, you should be able to scrub your disks without downtime as well. With ZFS, you can.
			</div>
			 <div class="para">
				While ZFS is performing a scrub on your pool, it is checking every block in the storage pool against its known SHA-256 checksum. Every block from top-to-bottom is checksummed using SHA-256 by default. This can be changed to using the Fletcher algorithm, although itâs not recommended. Because of SHA-256, you have a 1 in 2^256 or 1 in 10^77 chance that a corrupted block hashes to the same SHA-256 checksum. This is a 0.00000000000000000000000000000000000000000000000000000000000000000000000000001% chance. For reference, uncorrected ECC memory errors will happen on about 50 orders of magnitude more frequently, with the most reliable hardware on the market. So, when scrubbing your data, the probability is that either the checksum will match, and you have a good data block, or it wonât match, and you have a corrupted data block.
			</div>
			 <div class="para">
				Scrubbing ZFS storage pools is not something that happens automatically. You need to do it manually, and itâs highly recommended that you do it on a regularly scheduled interval. The recommended frequency at which you should scrub the data depends on the quality of the underlying disks. If you have SAS or FC disks, then once per month should be sufficient. If you have consumer grade SATA or SCSI, you should do once per week. You can schedule a scrub easily with the following command:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool scrub tank</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: tank
 state: ONLINE
  scan: scrub in progress since Sat Dec  8 08:06:36 2012
        32.0M scanned out of 48.5M at 16.0M/s, 0h0m to go
        0 repaired, 65.99% done
config:

                  NAME        STATE     READ WRITE CKSUM
                  tank        ONLINE       0     0     0
                    mirror-0  ONLINE       0     0     0
                      sde     ONLINE       0     0     0
                      sdf     ONLINE       0     0     0
                    mirror-1  ONLINE       0     0     0
                      sdg     ONLINE       0     0     0
                      sdh     ONLINE       0     0     0
                    mirror-2  ONLINE       0     0     0
                      sdi     ONLINE       0     0     0
                      sdj     ONLINE       0     0     0

errors: No known data errors
</pre>
			 <div class="para">
				As you can see, you can get a status of the scrub while it is in progress. Doing a scrub can severely impact performance of the disks and the applications needing them. So, if for any reason you need to stop the scrub, you can pass the <code class="command">-s</code> switch to the scrub subcommand. However, you should let the scrub continue to completion.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool scrub -s tank</code>
</pre>
			 <div class="para">
				You should put something similar to the following in your rootâs crontab, which will execute a scrub every Sunday at 02:00 in the morning:
			</div>
			 
<pre class="screen">
<code class="command">0 2 * * 0 /sbin/zpool scrub tank</code>
</pre>
			 <h6><a id="idm140006539190032">
      â </a> Self Healing Data </h6> <div class="para">
				If your storage pool is using some sort of redundancy, then ZFS will not only detect the silent data errors on a scrub, but it will also correct them if good data exists on a different disk. This is known as âself healingâ, and can be demonstrated in the following image. In our RAIDZ post, we discussed how the data is self-healed with RAIDZ, using the parity and a reconstruction algorithm. We are going to simplify it a bit, and use just a two way mirror. Suppose that an application needs some data blocks, and in those blocks, on of them is corrupted. How does ZFS know the data is corrupted? By checking the SHA-256 checksum of the block, as already mentioned. If a checksum does not match on a block, it will look at our other disk in the mirror to see if a good block can be found. If so, the good block is passed to the application, then ZFS will fix the bad block in the mirror, so that it also passes the SHA-256 checksum. As a result, the application will always get good data, and your pool will always be in a good, clean, consistent state.
			</div>
			 <div class="screenshot"> <div class="mediaobject"><img src="images/small-zfs-self-healing.png" width="70%" /><div class="caption">Image courtesy of root.cz, showing how ZFS self heals data.</div></div>
			 </div> <h6><a id="idm140006539185392">
      â </a> Resilvering Data </h6> <div class="para">
				Resilvering data is the same concept as rebuilding or resyncing data onto the new disk into the array. However, with Linux software RAID, hardware RAID controllers, and other RAID implementations, there is no distinction between which blocks are actually live, and which arenât. So, the rebuild starts at the beginning of the disk, and does not stop until it reaches the end of the disk. Because ZFS knows about the the RAID structure and the filesystem metadata, we can be smart about rebuilding the data. Rather than wasting our time on free disk, where live blocks are not stored, we can concern ourselves with ONLY those live blocks. This can provide significant time savings, if your storage pool is only partially filled. If the pool is only 10% filled, then that means only working on 10% of the drives. Win. Thus, with ZFS we need a new term than ârebuildingâ, âresyncingâ or âreconstructingâ. In this case, we refer to the process of rebuilding data as âresilveringâ.
			</div>
			 <div class="para">
				Unfortunately, disks will die, and need to be replaced. Provided you have redundancy in your storage pool, and can afford some failures, you can still send data to and receive data from applications, even though the pool will be in âDEGRADEDâ mode. If you have the luxury of hot swapping disks while the system is live, you can replace the disk without downtime (lucky you). If not, you will still need to identify the dead disk, and replace it. This can be a chore if you have many disks in your pool, say 24. However, most GNU/Linux operating system vendors, such as Debian or Ubuntu, provide a utility called âhdparmâ that allows you to discover the serial number of all the disks in your pool. This is, of course, that the disk controllers are presenting that information to the Linux kernel, which they typically do. So, you could run something like:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
for i in a b c d e f g; do echo -n "/dev/sd$i: "; hdparm -I
/dev/sd$i | awk '/Serial Number/ {print $3}'; done</code>

/dev/sda: OCZ-9724MG8BII8G3255
/dev/sdb: OCZ-69ZO5475MT43KNTU
/dev/sdc: WD-WCAPD3307153
/dev/sdd: JP2940HD0K9RJC
/dev/sde: /dev/sde: No such file or directory
/dev/sdf: JP2940HD0SB8RC
/dev/sdg: S1D1C3WR
</pre>
			 <div class="para">
				It appears that <code class="filename">/dev/sde</code> is my dead disk. I have the serial numbers for all the other disks in the system, but not this one. So, by process of elimination, I can go to the storage array, and find which serial number was not printed. This is my dead disk. In this case, I find serial number âJP2940HD01VLMCâ. I pull the disk, replace it with a new one, and see if <code class="filename">/dev/sde</code> is repopulated, and the others are still online. If so, Iâve found my disk, and can add it to the pool. This has actually happened to me twice already, on both of my personal hypervisors. It was a snap to replace, and I was online in under 10 minutes.
			</div>
			 <div class="para">
				To replace an dead disk in the pool with a new one, you use the <code class="command">replace</code> subcommand. Suppose the new disk also identifed itself as <code class="filename">/dev/sde</code>, then I would issue the following command:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool replace tank sde sde</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status tank</code>

  pool: tank
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        continue to function, possibly in a degraded state.
action: Wait for the resilver to complete.
        scrub: resilver in progress for 0h2m, 16.43% done, 0h13m to go
config:

                  NAME          STATE       READ WRITE CKSUM
                  tank          DEGRADED       0     0     0
                    mirror-0    DEGRADED       0     0     0
                      replacing DEGRADED       0     0     0
                      sde       ONLINE         0     0     0
                      sdf       ONLINE         0     0     0
                    mirror-1    ONLINE         0     0     0
                      sdg       ONLINE         0     0     0
                      sdh       ONLINE         0     0     0
                    mirror-2    ONLINE         0     0     0
                      sdi       ONLINE         0     0     0
                      sdj       ONLINE         0     0     0
</pre>
			 <div class="para">
				The resilver is analagous to a rebuild with Linux software RAID. It is rebuilding the data blocks on the new disk until the mirror, in this case, is in a completely healthy state. Viewing the status of the resilver will help you get an idea of when it will complete.
			</div>
			 <h6><a id="idm140006539174768">
      â </a> Identifying Pool Problems </h6> <div class="para">
				Determining quickly if everything is functioning as it should be, without the full output of the <code class="command">zpool status</code> command can be done by passing the <code class="command">-x</code> switch. This is useful for scripts to parse without fancy logic, which could alert you in the event of a failure:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool status -x</code>

all pools are healthy
</pre>
			 <div class="para">
				The rows in the <code class="command">zpool status</code> command give you vital information about the pool, most of which are self-explanatory. They are defined as follows:
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> pool </span></dt><dd>
									<div class="para">
										The name of the pool
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> state </span></dt><dd>
									<div class="para">
										The current health of the pool. This information refers only to the ability of the pool to provide the necessary replication level.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> status </span></dt><dd>
									<div class="para">
										A description of what is wrong with the pool. This field is omitted if no problems are found.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> action </span></dt><dd>
									<div class="para">
										A recommended action for repairing the errors. This field is an abbreviated form directing the user to one of the following sections. This field is omitted if no problems are found.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> see </span></dt><dd>
									<div class="para">
										A reference to a knowledge article containing detailed repair information. Online articles are updated more often than this guide can be updated, and should always be referenced for the most up-to-date repair procedures. This field is omitted if no problems are found.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> scrub </span></dt><dd>
									<div class="para">
										Identifies the current status of a scrub operation, which might include the date and time that the last scrub was completed, a scrub in progress, or if no scrubbing was requested.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> errors </span></dt><dd>
									<div class="para">
										Identifies known data errors or the absence of known data errors.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> config </span></dt><dd>
									<div class="para">
										Describes the configuration layout of the devices comprising the pool, as well as their state and any errors generated from the devices. The state can be one of the following: ONLINE, FAULTED, DEGRADED, UNAVAILABLE, or OFFLINE. If the state is anything but ONLINE, the fault tolerance of the pool has been compromised.
									</div>

								</dd></dl></div>

					</div>

				</li></ul></div>
			 <div class="para">
				The columns in the status output, âREADâ, âWRITEâ and âCHKSUMâ are defined as follows:
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> NAME </span></dt><dd>
									<div class="para">
										The name of each VDEV in the pool, presented in a nested order.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> STATE </span></dt><dd>
									<div class="para">
										The state of each VDEV in the pool. The state can be any of the states found in âconfigâ above.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> READ </span></dt><dd>
									<div class="para">
										I/O errors occurred while issuing a read request.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> WRITE </span></dt><dd>
									<div class="para">
										I/O errors occurred while issuing a write request.
									</div>

								</dd></dl></div>

					</div>

				</li><li class="listitem">
					<div class="para">
						<div class="variablelist"><dl class="variablelist"><dt><span class="term"> CHKSUM </span></dt><dd>
									<div class="para">
										Checksum errors. The device returned corrupted data as the result of a read request.
									</div>

								</dd></dl></div>

					</div>

				</li></ul></div>
			 <h6><a id="idm140006539132416">
      â </a> Conclusion </h6> <div class="para">
				Scrubbing your data on regular intervals will ensure that the blocks in the storage pool remain consistent. Even though the scrub can put strain on applications wishing to read or write data, it can save hours of headache in the future. Further, because you could have a âdamaged deviceâ at any time (see <a href="http://docs.oracle.com/cd/E19082-01/817-2271/gbbvf/index.html">about damaged devices with ZFS</a>), properly knowing how to fix the device, and what to expect when replacing one, is critical to storage administration. Of course, there is plenty more I could discuss about this topic, but this should at least introduce you to the concepts of scrubbing and resilvering data.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect--setting-prop">
      â </a>6.2.3.1.5.Â Getting and Setting Properties</h5></div></div></div>
			
			 <a id="idm140006539129216" class="indexterm"></a>
			 <h6><a id="idm140006539128192">
      â </a> Motivation </h6> <div class="para">
				With ext4, and many filesystems in GNU/Linux, we have a way for tuning various flags in the filesystem. Things like setting labels, default mount options, and other tunables. With ZFS, itâs no different, and in fact, is far more verbose. These properties allow us to modify all sorts of variables, both for the pool, and for the datasets it contains. Thus, we can âtuneâ the filesystem to our liking or needs. However, not every property is tunable. Some are read-only. But, weâll define what each of the properties are and how they affect the pool. Note, we are only looking at zpool properties, and we will get to ZFS dataset properties when we reach the dataset subtopic.
			</div>
			 <h6><a id="idm140006539126720">
      â </a> Zpool Properties </h6> <div class="variablelist"><dl class="variablelist"><dt><span class="term"> allocated </span></dt><dd>
						<div class="para">
							The amount of data that has been committed into the pool by all of the ZFS datasets. This setting is read-only.
						</div>

					</dd><dt><span class="term"> altroot </span></dt><dd>
						<div class="para">
							Identifies an alternate root directory. If set, this directory is prepended to any mount points within the pool. This property can be used when examining an unknown pool, if the mount points cannot be trusted, or in an alternate boot environment, where the typical paths are not valid.Setting altroot defaults to using <span class="property">cachefile=none</span>, though this may be overridden using an explicit setting.
						</div>

					</dd><dt><span class="term"> ashift </span></dt><dd>
						<div class="para">
							Can only be set at pool creation time. Pool sector size exponent, to the power of 2. I/O operations will be aligned to the specified size boundaries. Default value is â9â³, as 2^9 = 512, the standard sector size operating system utilities use for reading and writing data. For advanced format drives with 4 KiB boundaries, the value should be set to <span class="property">ashift=12</span>, as 2^12 = 4096.
						</div>

					</dd><dt><span class="term"> autoexpand </span></dt><dd>
						<div class="para">
							Must be set before replacing the first drive in your pool. Controls automatic pool expansion when the underlying LUN is grown. Default is âoffâ. After all drives in the pool have been replaced with larger drives, the pool will automatically grow to the new size. This setting is a boolean, with values either âonâ or âoffâ.
						</div>

					</dd><dt><span class="term"> autoreplace </span></dt><dd>
						<div class="para">
							Controls automatic device replacement of a <span class="property">spare</span> VDEV in your pool. Default is set to âoffâ. As such, device replacement must be initiated manually by using the <code class="command">zpool replace</code> command. This setting is a boolean, with values either âonâ or âoffâ.
						</div>

					</dd><dt><span class="term"> bootfs </span></dt><dd>
						<div class="para">
							Read-only setting that defines the bootable ZFS dataset in the pool. This is typically set by an installation program.
						</div>

					</dd><dt><span class="term"> cachefile </span></dt><dd>
						<div class="para">
							Controls the location of where the pool configuration is cached. When importing a zpool on a system, ZFS can detect the drive geometry using the metadata on the disks. However, in some clustering environments, the cache file may need to be stored in a different location for pools that would not automatically be imported. Can be set to any string, but for most ZFS installations, the default location of <code class="filename">/etc/zfs/zpool.cache</code> should be sufficient.
						</div>

					</dd><dt><span class="term"> capacity </span></dt><dd>
						<div class="para">
							Read-only value that identifies the percentage of pool space used.
						</div>

					</dd><dt><span class="term"> comment </span></dt><dd>
						<div class="para">
							A text string consisting of no more than 32 printable ASCII characters that will be stored such that it is available even if the pool becomes faulted. An administrator can provide additional information about a pool using this setting.
						</div>

					</dd><dt><span class="term"> dedupditto </span></dt><dd>
						<div class="para">
							Sets a block deduplication threshold, and if the reference count for a deduplicated block goes above the threshold, a duplicate copy of the block is stored automatically. The default value is 0. Can be any positive number.
						</div>

					</dd><dt><span class="term"> dedupratio </span></dt><dd>
						<div class="para">
							Read-only deduplication ratio specified for a pool, expressed as a multiplier
						</div>

					</dd><dt><span class="term"> delegation </span></dt><dd>
						<div class="para">
							Controls whether a non-privileged user can be granted access permissions that are defined for the dataset. The setting is a boolean, defaults to âonâ and can be âonâ or âoffâ.
						</div>

					</dd><dt><span class="term"> expandsize </span></dt><dd>
						<div class="para">
							Amount of uninitialized space within the pool or device that can be used to increase the total capacity of the pool. Uninitialized space consists of any space on an EFI labeled vdev which has not been brought online (i.e. <code class="command">zpool online -e</code>). This space occurs when a LUN is dynamically expanded.
						</div>

					</dd><dt><span class="term"> failmode </span></dt><dd>
						<div class="para">
							Controls the system behavior in the event of catastrophic pool failure. This condition is typically a result of a loss of connectivity to the underlying storage device(s) or a failure of all devices within the pool. The behavior of such an event is determined as follows: 
							<div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
									<div class="para">
										<div class="variablelist"><dl class="variablelist"><dt><span class="term"> wait </span></dt><dd>
													<div class="para">
														Blocks all I/O access until the device connectivity is recovered and the errors are cleared. This is the default behavior.
													</div>

												</dd></dl></div>

									</div>

								</li><li class="listitem">
									<div class="para">
										<div class="variablelist"><dl class="variablelist"><dt><span class="term"> continue </span></dt><dd>
													<div class="para">
														Returns EIO to any new write I/O requests but allows reads to any of the remaining healthy devices. Any write requests that have yet to be committed to disk would be blocked.
													</div>

												</dd></dl></div>

									</div>

								</li><li class="listitem">
									<div class="para">
										<div class="variablelist"><dl class="variablelist"><dt><span class="term"> panic </span></dt><dd>
													<div class="para">
														Prints out a message to the console and generates a system crash dump.
													</div>

												</dd></dl></div>

									</div>

								</li></ul></div>

						</div>

					</dd><dt><span class="term"> free </span></dt><dd>
						<div class="para">
							Read-only value that identifies the number of blocks within the pool that are not allocated.
						</div>

					</dd><dt><span class="term"> guid </span></dt><dd>
						<div class="para">
							Read-only property that identifies the unique identifier for the pool. Similar to the UUID string for ext4 filesystems.
						</div>

					</dd><dt><span class="term"> health </span></dt><dd>
						<div class="para">
							Read-only property that identifies the current health of the pool, as either ONLINE, DEGRADED, FAULTED, OFFLINE, REMOVED, or UNAVAIL.
						</div>

					</dd><dt><span class="term"> listsnapshots </span></dt><dd>
						<div class="para">
							Controls whether snapshot information that is associated with this pool is displayed with the <code class="command">zfs list</code> command. If this property is disabled, snapshot information can be displayed with the <code class="command">zfs list -t snapshot</code> command. The default value is âoffâ. Boolean value that can be either âoffâ or âonâ.
						</div>

					</dd><dt><span class="term"> readonly </span></dt><dd>
						<div class="para">
							Boolean value that can be either âoffâ or âonâ. Default value is âoffâ. Controls setting the pool into read-only mode to prevent writes and/or data corruption.
						</div>

					</dd><dt><span class="term"> size </span></dt><dd>
						<div class="para">
							Read-only property that identifies the total size of the storage pool.
						</div>

					</dd><dt><span class="term"> version </span></dt><dd>
						<div class="para">
							Writable setting that identifies the current on-disk version of the pool. Can be any value from 1 to the output of the <code class="command">zpool upgrade -v</code> command. This property can be used when a specific version is needed for backwards compatibility.
						</div>

					</dd></dl></div>
			 <h6><a id="idm140006539126080">
      â </a> Getting and Setting Properties </h6> <div class="para">
				There are a few ways you can get to the properties of your pool- you can get all properties at once, only one property, or more than one, comma-separated. For example, suppose I wanted to get just the health of the pool. I could issue the following command:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool get health tank</code>

NAME  PROPERTY  VALUE   SOURCE
tank  health    ONLINE  -
</pre>
			 <div class="para">
				If I wanted to get multiple settings, say the health of the system, how much is free, and how much is allocated, I could issue this command instead:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool get health,free,allocated tank</code>

NAME  PROPERTY   VALUE   SOURCE
tank  health     ONLINE  -
tank  free       176G    -
tank  allocated  32.2G   -
</pre>
			 <div class="para">
				And of course, if I wanted to get all the settings available, I could run:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool get all tank</code>

NAME  PROPERTY       VALUE       SOURCE
tank  size           208G        -
tank  capacity       15%         -
tank  altroot        -           default
tank  health         ONLINE      -
tank  guid           1695112377970346970  default
tank  version        28          default
tank  bootfs         -           default
tank  delegation     on          default
tank  autoreplace    off         default
tank  cachefile      -           default
tank  failmode       wait        default
tank  listsnapshots  off         default
tank  autoexpand     off         default
tank  dedupditto     0           default
tank  dedupratio     1.00x       -
tank  free           176G        -
tank  allocated      32.2G       -
tank  readonly       off         -
tank  ashift         0           default
tank  comment        -           default
tank  expandsize     0           -
</pre>
			 <div class="para">
				Setting a property is just as easy. However, there is a catch. For properties that require a string argument, there is no way to get it back to default. At least not that I am aware of. With the rest of the properties, if you try to set a property to an invalid argument, an error will print to the screen letting you know what is available, but it will not notify you as to what is default. However, you can look at the âSOURCEâ column. If the value in that column is âdefaultâ, then itâs default. If itâs âlocalâ, then it was user-defined.
			</div>
			 <div class="para">
				Suppose we wanted to change the <span class="property">comment</span> property, this is how I would do it:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool set comment="Contact admins@example.com" tank</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool get comment tank</code>

NAME  PROPERTY  VALUE                       SOURCE
tank  comment   Contact admins@example.com  local
</pre>
			 <div class="para">
				As you can see, the SOURCE is âlocalâ for the <span class="property">comment</span> property. Thus, it was user-defined. As mentioned, I donât know of a way to get string properties back to default after being set. Further, any modifiable property can be set at pool creation time by using the <code class="command">-o</code> switch, as follows:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zpool create -o ashift=12 tank raid1 sda sdb</code>
</pre>
			 <h6><a id="idm140006539063728">
      â </a> Final Thoughts </h6> <div class="para">
				The zpool properties apply to the entire pool, which means ZFS datasets will inherit that property from the pool. Some properties that you set on your ZFS dataset, which will be discussed towards the end of this series, apply to the whole pool. For example, if you enable block deduplication for a ZFS dataset, it dedupes blocks found in the entire pool, not just in your dataset. However, only blocks in that dataset will be actively deduped, while other ZFS datasets may not. Also, setting a property is not retroactive. In the case of your <span class="property">autoexpand</span> zpool property to automatically expand the zpool size when all the drives have been replaced, if you replaced a drive before enabling the property, that drive will be considered a smaller drive, even if it physically isnât. Setting properties only applies to operations on the data moving forward, and never backward.
			</div>
			 <div class="para">
				Despite a few of these caveats, having the ability to change some parameters of your pool to fit your needs as a GNU/Linux storage administrator gives you great control that other filesystems donât. And, as weâve discovered thus far, everything can be handled with a single command <code class="command">zpool</code>, and easy-to-recall subcommands. Weâll have one more post discussing a thorough examination of caveats that you will want to consider before creating your pools, then we will leave the zpool category, and work our way towards ZFS datasets, the bread and butter of ZFS as a whole. If there is anything additional about zpools you would like me to post on, let me know now, and I can squeeze it in.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-best-practices">
      â </a>6.2.3.1.6.Â Best Practices and Caveats</h5></div></div></div>
			
			 <a id="idm140006539059312" class="indexterm"></a>
			 <h6><a id="idm140006539058160">
      â </a> Best Practices </h6> <div class="para">
				As with all recommendations, some of these guidelines carry a great amount of weight, while others might not. You may not even be able to follow them as rigidly as you would like. Regardless, you should be aware of them. Iâll try to provide a reason why for each. Theyâre listed in no specific order. The idea of âbest practicesâ is to optimize space efficiency, performance and ensure maximum data integrity.
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						Only run ZFS on 64-bit kernels. It has 64-bit specific code that 32-bit kernels cannot do anything with.
					</div>

				</li><li class="listitem">
					<div class="para">
						Install ZFS only on a system with lots of RAM. 1 GB is a bare minimum, 2 GB is better, 4 GB would be preferred to start. Remember, ZFS will use 7/8 of the available RAM for the ARC.
					</div>

				</li><li class="listitem">
					<div class="para">
						Use ECC RAM when possible for scrubbing data in registers and maintaining data consistency. The ARC is an actual read-only data cache of valuable data in RAM.
					</div>

				</li><li class="listitem">
					<div class="para">
						Use whole disks rather than partitions. ZFS can make better use of the on-disk cache as a result. If you must use partitions, backup the partition table, and take care when reinstalling data into the other partitions, so you donât corrupt the data in your pool.
					</div>

				</li><li class="listitem">
					<div class="para">
						Keep each VDEV in a storage pool the same size. If VDEVs vary in size, ZFS will favor the larger VDEV, which could lead to performance bottlenecks.
					</div>

				</li><li class="listitem">
					<div class="para">
						Use redundancy when possible, as ZFS can and will want to correct data errors that exist in the pool. You cannot fix these errors if you do not have a redundant good copy elsewhere in the pool. Mirrors and RAID-Z levels accomplish this.
					</div>

				</li><li class="listitem">
					<div class="para">
						For the number of disks in the storage pool, use the âpower of two plus parityâ recommendation. This is for storage space efficiency and hitting the âsweet spotâ in performance. So, for a RAIDZ-1 VDEV, use three (2+1), five (4+1), or nine (8+1) disks. For a RAIDZ-2 VDEV, use four (2+2), six (4+2), ten (8+2), or eighteen (16+2) disks. For a RAIDZ-3 VDEV, use five (2+3), seven (4+3), eleven (8+3), or nineteen (16+3) disks. For pools larger than this, consider striping across mirrored VDEVs.
					</div>

				</li><li class="listitem">
					<div class="para">
						Consider using RAIDZ-2 or RAIDZ-3 over RAIDZ-1. Youâve heard the phrase âwhen it rains, it poursâ. This is true for disk failures. If a disk fails in a RAIDZ-1, and the hot spare is getting resilvered, until the data is fully copied, you cannot afford another disk failure during the resilver, or you will suffer data loss. With RAIDZ-2, you can suffer two disk failures, instead of one, increasing the probability you have fully resilvered the necessary data before the second, and even third disk fails.
					</div>

				</li><li class="listitem">
					<div class="para">
						Perform regular (at least weekly) backups of the full storage pool. Itâs not a backup, unless you have multiple copies. Just because you have redundant disk, does not ensure live running data in the event of a power failure, hardware failure or disconnected cables.
					</div>

				</li><li class="listitem">
					<div class="para">
						Use hot spares to quickly recover from a damaged device. Set the âautoreplaceâ property to on for the pool.
					</div>

				</li><li class="listitem">
					<div class="para">
						Consider using a hybrid storage pool with fast SSDs or NVRAM drives. Using a fast SLOG and L2ARC can greatly improve performance.
					</div>

				</li><li class="listitem">
					<div class="para">
						If using a hybrid storage pool with multiple devices, mirror the SLOG and stripe the L2ARC.
					</div>

				</li><li class="listitem">
					<div class="para">
						If using a hybrid storage pool, and partitioning the fast SSD or NVRAM drive, unless you know you will need it, 1 GB is likely sufficient for your SLOG. Use the rest of the SSD or NVRAM drive for the L2ARC. The more storage for the L2ARC, the better.
					</div>

				</li><li class="listitem">
					<div class="para">
						Keep pool capacity under 80% for best performance. Due to the copy-on-write nature of ZFS, the filesystem gets heavily fragmented. Email reports of capacity at least monthly.
					</div>

				</li><li class="listitem">
					<div class="para">
						Scrub consumer-grade SATA and SCSI disks weekly and enterprise-grade SAS and FC disks monthly.
					</div>

				</li><li class="listitem">
					<div class="para">
						Email reports of the storage pool health weekly for redundant arrays, and bi-weekly for non-redundant arrays.
					</div>

				</li><li class="listitem">
					<div class="para">
						When using advanced format disks that read and write data in 4 KB sectors, set the âashiftâ value to 12 on pool creation for maximum performance. Default is 9 for 512-byte sectors.
					</div>

				</li><li class="listitem">
					<div class="para">
						Set âautoexpandâ to on, so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones. Default is off.
					</div>

				</li><li class="listitem">
					<div class="para">
						Always export your storage pool when moving the disks from one physical system to another.
					</div>

				</li><li class="listitem">
					<div class="para">
						When considering performance, know that for sequential writes, mirrors will always outperform RAID-Z levels. For sequential reads, RAID-Z levels will perform more slowly than mirrors on smaller data blocks and faster on larger data blocks. For random reads and writes, mirrors and RAID-Z seem to perform in similar manners. Striped mirrors will outperform mirrors and RAID-Z in both sequential, and random reads and writes.
					</div>

				</li><li class="listitem">
					<div class="para">
						Compression is disabled by default. This doesnât make much sense with todayâs hardware. ZFS compression is extremely cheap, extremely fast, and barely adds any latency to the reads and writes. In fact, in some scenarios, your disks will respond faster with compression enabled than disabled. A further benefit is the massive space benefits.
					</div>

				</li></ul></div>
			 <h6><a id="idm140006539056688">
      â </a> Caveats </h6> <div class="para">
				The point of the caveat list is by no means to discourage you from using ZFS. Instead, as a storage administrator planning out your ZFS storage server, these are things that you should be aware of, so as not to catch you with your pants down, and without your data. If you donât head these warnings, you could end up with corrupted data. The line may be blurred with the âbest practicesâ list above. Iâve tried making this list all about data corruption if not headed. Read and head the caveats, and you should be good.
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						Your VDEVs determine the IOPS of the storage, and the slowest disk in that VDEV will determine the IOPS for the entire VDEV.
					</div>

				</li><li class="listitem">
					<div class="para">
						ZFS uses 1/64 of the available raw storage for metadata. So, if you purchased a 1 TB drive, the actual raw size is 976 GiB. After ZFS uses it, you will have 961 GiB of available space. The âzfs listâ command will show an accurate representation of your available storage. Plan your storage keeping this in mind.
					</div>

				</li><li class="listitem">
					<div class="para">
						ZFS wants to control the whole block stack. It checksums, resilvers live data instead of full disks, self-heals corrupted blocks, and a number of other unique features. If using a RAID card, make sure to configure it as a true JBOD (or âpassthrough modeâ), so ZFS can control the disks. If you canât do this with your RAID card, donât use it. Best to use a real HBA.
					</div>

				</li><li class="listitem">
					<div class="para">
						Do not use other volume management software beneath ZFS. ZFS will perform better, and ensure greater data integrity, if it has control of the whole block device stack. As such, avoid using dm-crypt, mdadm or LVM beneath ZFS.
					</div>

				</li><li class="listitem">
					<div class="para">
						Do not share a SLOG or L2ARC DEVICE across pools. Each pool should have its own physical DEVICE, not logical drive, as is the case with some PCI-Express SSD cards. Use the full card for one pool, and a different physical card for another pool. If you share a physical device, you will create race conditions, and could end up with corrupted data.
					</div>

				</li><li class="listitem">
					<div class="para">
						Do not share a single storage pool across different servers. ZFS is not a clustered filesystem. Use GlusterFS, Ceph, Lustre or some other clustered filesystem on top of the pool if you wish to have a shared storage backend.
					</div>

				</li><li class="listitem">
					<div class="para">
						Other than a spare, SLOG and L2ARC in your hybrid pool, do not mix VDEVs in a single pool. If one VDEV is a mirror, all VDEVs should be mirrors. If one VDEV is a RAIDZ-1, all VDEVs should be RAIDZ-1. Unless of course, you know what you are doing, and are willing to accept the consequences. ZFS attempts to balance the data across VDEVs. Having a VDEV of a different redundancy can lead to performance issues and space efficiency concerns, and make it very difficult to recover in the event of a failure.
					</div>

				</li><li class="listitem">
					<div class="para">
						Do not mix disk sizes or speeds in a single VDEV. Do mix fabrication dates, however, to prevent mass drive failure.
					</div>

				</li><li class="listitem">
					<div class="para">
						In fact, do not mix disk sizes or speeds in your storage pool at all.
					</div>

				</li><li class="listitem">
					<div class="para">
						Do not mix disk counts across VDEVs. If one VDEV uses 4 drives, all VDEVs should use 4 drives.
					</div>

				</li><li class="listitem">
					<div class="para">
						Do not put all the drives from a single controller in one VDEV. Plan your storage, such that if a controller fails, it affects only the number of disks necessary to keep the data online.
					</div>

				</li><li class="listitem">
					<div class="para">
						When using advanced format disks, you must set the ashift value to 12 at pool creation. It cannot be changed after the fact. Use âzpool create -o ashift=12 tank mirror sda sdbâ as an example.
					</div>

				</li><li class="listitem">
					<div class="para">
						Hot spare disks will not be added to the VDEV to replace a failed drive by default. You MUST enable this feature. Set the autoreplace feature to on. Use âzpool set autoreplace=on tankâ as an example.
					</div>

				</li><li class="listitem">
					<div class="para">
						The storage pool will not auto resize itself when all smaller drives in the pool have been replaced by larger ones. You MUST enable this feature, and you MUST enable it before replacing the first disk. Use âzpool set autoexpand=on tankâ as an example.
					</div>

				</li><li class="listitem">
					<div class="para">
						ZFS does not restripe data in a VDEV nor across multiple VDEVs. Typically, when adding a new device to a RAID array, the RAID controller will rebuild the data, by creating a new stripe width. This will free up some space on the drives in the pool, as it copies data to the new disk. ZFS has no such mechanism. Eventually, over time, the disks will balance out due to the writes, but even a scrub will not rebuild the stripe width.
					</div>

				</li><li class="listitem">
					<div class="para">
						You cannot shrink a zpool, only grow it. This means you cannot remove VDEVs from a storage pool.
					</div>

				</li><li class="listitem">
					<div class="para">
						You can only remove drives from mirrored VDEV using the âzpool detachâ command. You can replace drives with another drive in RAIDZ and mirror VDEVs however.
					</div>

				</li><li class="listitem">
					<div class="para">
						Do not create a storage pool of files or ZVOLs from an existing zpool. Race conditions will be present, and you will end up with corrupted data. Always keep multiple pools separate.
					</div>

				</li><li class="listitem">
					<div class="para">
						The Linux kernel may not assign a drive the same drive letter at every boot. Thus, you should use the /dev/disk/by-id/ convention for your SLOG and L2ARC. If you donât, your zpool devices could end up as a SLOG device, which would in turn clobber your ZFS data.
					</div>

				</li><li class="listitem">
					<div class="para">
						Donât create massive storage pools âjust because you canâ. Even though ZFS can create 78-bit storage pool sizes, that doesnât mean you need to create one.
					</div>

				</li><li class="listitem">
					<div class="para">
						Donât put production directly into the zpool. Use ZFS datasets instead.
					</div>

				</li><li class="listitem">
					<div class="para">
						Donât commit production data to file VDEVs. Only use file VDEVs for testing scripts or learning the ins and outs of ZFS.
					</div>

				</li></ul></div>

		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="admin-man-sect-zfs-admin">
      â </a>6.2.3.2.Â ZFS Filesystem Administration</h4></div></div></div>
		
		 <a id="idm140006539010416" class="indexterm"></a>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-crt-flsys">
      â </a>6.2.3.2.1.Â Creating Filesystems</h5></div></div></div>
			
			 <a id="idm140006539008112" class="indexterm"></a>
			 <h6><a id="idm140006539006960">
      â </a> Background </h6> <div class="para">
				First, we need to understand how traditional filesystems and volume management work in GNU/Linux before we can get a thorough understanding of ZFS datasets. To treat this fairly, we need to assemble Linux software RAID, LVM, and ext4 or another Linux kernel supported filesystem together.
			</div>
			 <div class="para">
				This is done by creating a redundant array of disks, and exporting a block device to represent that array. Then, we format that exported block device using LVM. If we have multiple RAID arrays, we format each of those as well. We then add all these exported block devices to a âvolume groupâ which represents my pooled storage. If I had five exported RAID arrays, of 1 TB each, then I would have 5 TB of pooled storage in this volume group. Now, I need to decide how to divide up the volume, to create logical volumes of a specific size. If this was for an Ubuntu or Debian installation, maybe I would give 100 GB to one logical volume for the root filesystem. That 100 GB is now marked as occupied by the volume group. I then give 500 GB to my home directory, and so forth. Each operation exports a block device, representing my logical volume. Itâs these block devices that I format with ex4 or a filesystem of my choosing.
			</div>
			 <div class="screenshot"> <div class="mediaobject"><img src="images/lvm.png" width="60%" /><div class="caption"> Linux RAID, LVM, and filesystem stack. Each filesystem is limited in size. </div></div>
			 </div> <div class="para">
				In this scenario, each logical volume is a fixed size in the volume group. It cannot address the full pool. So, when formatting the logical volume block device, the filesystem is a fixed size. When that device fills, you must resize the logical volume and the filesystem together. This typically requires a myriad of commands, and itâs tricky to get just right without losing data.
			</div>
			 <div class="para">
				ZFS handles filesystems a bit differently. First, there is no need to create this stacked approach to storage. Weâve already covered how to pool the storage, now we well cover how to use it. This is done by creating a dataset in the filesystem. By default, this dataset will have full access to the entire storage pool. If our storage pool is 5 TB in size, as previously mentioned, then our first dataset will have access to all 5 TB in the pool. If I create a second dataset, it too will have full access to all 5 TB in the pool. And so on and so forth.
			</div>
			 <div class="screenshot"> <div class="mediaobject"><img src="images/zfs.png" width="80%" /><div class="caption"> Each ZFS dataset can use the full underlying storage. </div></div>
			 </div> <div class="para">
				Now, as files are placed in the dataset, the pool marks that storage as unavailable to all datasets. This means that each dataset is aware of what is available in the pool and what is not by all other datasets in the pool. There is no need to create logical volumes of limited size. Each dataset will continue to place files in the pool, until the pool is filled. As the cards fall, they fall. You can, of course, put quotas on datasets, limiting their size, or export ZVOLs, topics weâll cover later.
			</div>
			 <div class="para">
				So, letâs create some datasets.
			</div>
			 <h6><a id="idm140006538996448">
      â </a> Basic Creation </h6> <div class="para">
				In these examples, we will assume our ZFS shared storage is named âtankâ. Further, we will assume that the pool is created with 4 preallocated files of 1 GB in size each, in a RAIDZ-1 array. Letâs create some datasets.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create tank/test</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list</code>

          NAME         USED  AVAIL  REFER  MOUNTPOINT
          tank         175K  2.92G  43.4K  /tank
          tank/test   41.9K  2.92G  41.9K  /tank/test
</pre>
			 <div class="para">
				Notice that the dataset âtank/testâ is mounted to â/tank/testâ by default, and that it has full access to the entire pool. Also notice that it is occupying only 41.9 KB of the pool. Letâs create 4 more datasets, then look at the output:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create tank/test2</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create tank/test3</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create tank/test4</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create tank/test5</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list</code>

          NAME         USED  AVAIL  REFER  MOUNTPOINT
          tank         392K  2.92G  47.9K  /tank
          tank/test   41.9K  2.92G  41.9K  /tank/test
          tank/test2  41.9K  2.92G  41.9K  /tank/test2
          tank/test3  41.9K  2.92G  41.9K  /tank/test3
          tank/test4  41.9K  2.92G  41.9K  /tank/test4
          tank/test5  41.9K  2.92G  41.9K  /tank/test5
</pre>
			 <div class="para">
				Each dataset is automatically mounted to its respective mount point, and each dataset has full unfettered access to the storage pool. Letâs fill up some data in one of the datasets, and see how that affects the underlying storage:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
cd /tank/test3</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
for i in {1..10}; do dd if=/dev/urandom of=file$i.img bs=1024 count=$RANDOM
&amp;&gt; /dev/null; done</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list</code>

          NAME         USED  AVAIL  REFER  MOUNTPOINT
          tank         159M  2.77G  49.4K  /tank
          tank/test   41.9K  2.77G  41.9K  /tank/test
          tank/test2  41.9K  2.77G  41.9K  /tank/test2
          tank/test3   158M  2.77G   158M  /tank/test3
          tank/test4  41.9K  2.77G  41.9K  /tank/test4
          tank/test5  41.9K  2.77G  41.9K  /tank/test5
</pre>
			 <div class="para">
				Notice that in my case, âtank/test3â³ is occupying 158 MB of disk, so according to the rest of the datasets, there is only 2.77 GB available in the pool, where previously there was 2.92 GB. So as you can see, the big advantage here is that I do not need to worry about preallocated block devices, as I would with LVM. Instead, ZFS manages the entire stack, so it understands how much data has been occupied, and how much is available.
			</div>
			 <h6><a id="idm140006538984592">
      â </a> Mounting Datasets </h6> <div class="para">
				Itâs important to understand that when creating datasets, you arenât creating exportable block devices by default. This means you donât have something directly to mount. In conclusion, there is nothing to add to your /etc/fstab file for persistence across reboots.
			</div>
			 <div class="para">
				So, if there is nothing to add do the /etc/fstab file, how do the filesystems get mounted? This is done by importing the pool, if necessary, then running the âzfs mountâ command. Similarly, we have a âzfs unmountâ command to unmount datasets, or we can use the standard âumountâ utility:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
umount /tank/test5</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mount | grep tank</code>

tank/test on /tank/test type zfs (rw,relatime,xattr)
tank/test2 on /tank/test2 type zfs (rw,relatime,xattr)
tank/test3 on /tank/test3 type zfs (rw,relatime,xattr)
tank/test4 on /tank/test4 type zfs (rw,relatime,xattr)
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs mount tank/test5</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mount | grep tank</code>

tank/test on /tank/test type zfs (rw,relatime,xattr)
tank/test2 on /tank/test2 type zfs (rw,relatime,xattr)
tank/test3 on /tank/test3 type zfs (rw,relatime,xattr)
tank/test4 on /tank/test4 type zfs (rw,relatime,xattr)
tank/test5 on /tank/test5 type zfs (rw,relatime,xattr)
</pre>
			 <div class="para">
				By default, the mount point for the dataset is â/&lt;pool-name&gt;/&lt;dataset-name&gt;â. This can be changed, by changing the dataset property. Just as storage pools have properties that can be tuned, so do datasets. Weâll dedicate a full post to dataset properties later. We only need to change the âmountpointâ property, as follows:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs set mountpoint=/mnt/test tank/test</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mount | grep tank</code>

tank on /tank type zfs (rw,relatime,xattr)
tank/test2 on /tank/test2 type zfs (rw,relatime,xattr)
tank/test3 on /tank/test3 type zfs (rw,relatime,xattr)
tank/test4 on /tank/test4 type zfs (rw,relatime,xattr)
tank/test5 on /tank/test5 type zfs (rw,relatime,xattr)
tank/test on /mnt/test type zfs (rw,relatime,xattr)
</pre>
			 <h6><a id="idm140006538976576">
      â </a> Nested Datasets </h6> <div class="para">
				Datasets donât need to be isolated. You can create nested datasets within each other. This allows you to create namespaces, while tuning a nested directory structure, without affecting the other. For example, maybe you want compression on /var/log, but not on the parent /var. there are other benefits as well, with some caveats that we will look at later.
			</div>
			 <div class="para">
				To create a nested dataset, create it like you would any other, by providing the parent storage pool and dataset. In this case we will create a nested log dataset in the test dataset:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create tank/test/log</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list</code>

          NAME            USED  AVAIL  REFER  MOUNTPOINT
          tank            159M  2.77G  47.9K  /tank
          tank/test      85.3K  2.77G  43.4K  /mnt/test
          tank/test/log  41.9K  2.77G  41.9K  /mnt/test/log
          tank/test2     41.9K  2.77G  41.9K  /tank/test2
          tank/test3      158M  2.77G   158M  /tank/test3
          tank/test4     41.9K  2.77G  41.9K  /tank/test4
          tank/test5     41.9K  2.77G  41.9K  /tank/test5
</pre>
			 <h6><a id="idm140006538972688">
      â </a> Additional Dataset Administration </h6> <div class="para">
				Along with creating datasets, when you no longer need them, you can destroy them. This frees up the blocks for use by other datasets, and cannot be reverted without a previous snapshot, which weâll cover later. To destroy a dataset:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs destroy tank/test5</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list</code>

          NAME            USED  AVAIL  REFER  MOUNTPOINT
          tank            159M  2.77G  49.4K  /tank
          tank/test      41.9K  2.77G  41.9K  /mnt/test
          tank/test/log  41.9K  2.77G  41.9K  /mnt/test/log
          tank/test2     41.9K  2.77G  41.9K  /tank/test2
          tank/test3      158M  2.77G   158M  /tank/test3
          tank/test4     41.9K  2.77G  41.9K  /tank/test4
</pre>
			 <div class="para">
				We can also rename a dataset if needed. This is handy when the purpose of the dataset changes, and you want the name to reflect that purpose. The arguments take a dataset source as the first argument and the new name as the last argument. To rename the tank/test3 dataset to music:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs rename tank/test3 tank/music</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list</code>

          NAME            USED  AVAIL  REFER  MOUNTPOINT
          tank            159M  2.77G  49.4K  /tank
          tank/music      158M  2.77G   158M  /tank/music
          tank/test      41.9K  2.77G  41.9K  /mnt/test
          tank/test/log  41.9K  2.77G  41.9K  /mnt/test/log
          tank/test2     41.9K  2.77G  41.9K  /tank/test2
          tank/test4     41.9K  2.77G  41.9K  /tank/test4
</pre>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-comp-and-dudup">
      â </a>6.2.3.2.2.Â Subsection Compression</h5></div></div></div>
			
			 <a id="idm140006538965984" class="indexterm"></a>
			 <div class="para">
				Compression is transparent with ZFS if you enable it. This means that every file you store in your pool can be compressed. From your point of view as an application, the file does not appear to be compressed, but appears to be stored uncompressed. In other words, if you run the âfileâ command on your plain text configuration file, it will report it as such. Instead, underneath the file layer, ZFS is compressing and decompressing the data on disk on the fly. And because compression is so cheap on the CPU, and exceptionally fast with some algorithms, it should not be noticeable.
			</div>
			 <div class="para">
				Compression is enabled and disabled per dataset. Further, the supported compression algorithms are LZJB, ZLE, and Gzip. With Gzip, the standards levels of 1 through 9 are supported, where 1 is as fast as possible, with the least compression, and 9 is as compressed as possible, taking as much time as necessary. The default is 6, as is standard in GNU/Linux and other Unix operating systems. LZJB, on the other hand, was invented by Jeff Bonwick, who is also the author of ZFS. LZJB was designed to be fast with tight compression ratios, which is standard with most Lempel-Ziv algorithms. LZJB is the default. ZLE is a speed demon, with very light compression ratios. LZJB seems to provide the best all around results it terms of performance and compression.
			</div>
			 <div class="para">
				Obviously, compression can vary on the disk space saved. If the dataset is storing mostly uncompressed data, such as plain text log files, or configuration files, the compression ratios can be massive. If the dataset is storing mostly compressed images and video, then you wonât see much if anything in the way of disk savings. With that said, compression is disabled by default, and enabling LZJB doesnât seem to yield any performance impact. So even if youâre storing largely compressed data, for the data files that are not compressed, you can get those compression savings, without impacting the performance of the storage server. So, IMO, I would recommend enabling compression for all of your datasets.
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="warning"><div class="admonition_header"><p><strong>Warning</strong></p></div><div class="admonition">
				<div class="para">
					Enabling compression on a dataset is not retroactive! It will only apply to newly committed or modified data. Any previous data in the dataset will remain uncompressed. So, if you want to use compression, you should enable it before you begin committing data.
				</div>

			</div></div>
			 <div class="para">
				To enable compression on a dataset, we just need to modify the âcompressionâ property. The valid values for that property are: âonâ, âoffâ, âlzjbâ, âgzipâ, âgzip[1-9]â, and âzleâ.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create tank/log</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs set compression=lzjb tank/log</code>
</pre>
			 <div class="para">
				Now that weâve enabled compression on this dataset, letâs copy over some uncompressed data, and see what sort of savings we would see. A great source of uncompressed data would be the /etc/ and /var/log/ directories. Letâs create a tarball of these directories, see itâs raw size and see what sort of space savings we achieved:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
tar -cf /tank/test/text.tar /var/log/ /etc/</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -lh /tank/test/text.tar</code>

-rw-rw-r-- 1 root root 24M Dec 17 21:24 /tank/test/text.tar

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list tank/test</code>

          NAME        USED  AVAIL  REFER  MOUNTPOINT
          tank/test  11.1M  2.91G  11.1M  /tank/test

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs get compressratio tank/test</code>

          NAME       PROPERTY       VALUE  SOURCE
          tank/test  compressratio  2.14x  -
</pre>
			 <div class="para">
				So, in my case, I created a 24 MB uncompressed tarball. After copying it to the dataset that had compression enabled, it only occupied 11.1 MB. This is less than half the size (text compresses very well)! We can read the âcompressratioâ property on the dataset to see what sort of space savings we are achieving. In my case, the output is telling me that the compressed data would occupy 2.14 times the amount of disk space, if uncompressed. Very nice.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-snap-clones">
      â </a>6.2.3.2.3.Â Snapshots and Clones</h5></div></div></div>
			
			 <a id="idm140006538951312" class="indexterm"></a>
			 <div class="para">
				Snapshots with ZFS are similar to snapshots with Linux LVM. A snapshot is a first class read-only filesystem. It is a mirrored copy of the state of the filesystem at the time you took the snapshot. Think of it like a digital photograph of the outside world. Even though the world is changing, you have an image of what the world was like at the exact moment you took that photograph. Snapshots behave in a similar manner, except when data changes that was part of the dataset, you keep the original copy in the snapshot itself. This way, you can maintain persistence of that filesystem.
			</div>
			 <div class="para">
				You can keep up to 2^64 snapshots in your pool, ZFS snapshots are persistent across reboots, and they donât require any additional backing store; they use the same storage pool as the rest of your data. If you remember our post about the nature of copy-on-write filesystems, you will remember our discussion about Merkle trees. A ZFS snapshot is a copy of the Merkle tree in that state, except we make sure that the snapshot of that Merkle tree is never modified.
			</div>
			 <div class="para">
				Creating snapshots is near instantaneous, and they are cheap. However, once the data begins to change, the snapshot will begin storing data. If you have multiple snapshots, then multiple deltas will be tracked across all the snapshots. However, depending on your needs, snapshots can still be exceptionally cheap.
			</div>
			 <h6><a id="idm140006538947568">
      â </a> Creating Snapshots </h6> <div class="para">
				You can create two types of snapshots: pool snapshots and dataset snapshots. Which type of snapshot you want to take is up to you. You must give the snapshot a name, however. The syntax for the snapshot name is:
			</div>
			 
<pre class="screen">
- pool/dataset@snapshot-name
- pool@snapshot-name
</pre>
			 <div class="para">
				To create a snapshot, we use the âzfs snapshotâ command. For example, to take a snapshot of the âtank/testâ dataset, we would issue:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs snapshot tank/test@tuesday</code>
</pre>
			 <div class="para">
				Even though a snapshot is a first class filesystem, it does not contain modifiable properties like standard ZFS datasets or pools. In fact, everything about a snapshot is read-only. For example, if you wished to enable compression on a snapshot, here is what would happen:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs set compression=lzjb tank/test@friday</code>

cannot set property for 'tank/test@friday': this property can not be modified for
snapshots
</pre>
			 <h6><a id="idm140006538942784">
      â </a> Listing Snapshots </h6> <div class="para">
				Snapshots can be displayed two ways: by accessing a hidden â.zfsâ directory in the root of the dataset, or by using the âzfs listâ command. First, letâs discuss the hidden directory. Check out this madness:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -a /tank/test</code>

./  ../  boot.tar  text.tar  text.tar.2

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
cd /tank/test/.zfs/</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -a</code>

./  ../  shares/  snapshot/
</pre>
			 <div class="para">
				Even though the â.zfsâ directory was not visible, even with âls -aâ, we could still change directory to it. If you wish to have the â.zfsâ directory visible, you can change the âsnapdirâ property on the dataset. The valid values are âhiddenâ and âvisibleâ. By default, itâs hidden. Letâs change it:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs set snapdir=visible tank/test</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -a /tank/test</code>

./  ../  boot.tar  text.tar  text.tar.2  .zfs/
</pre>
			 <div class="para">
				The other way to display snapshots is by using the âzfs listâ command, and passing the â-t snapshotâ argument, as follows:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list -t snapshot</code>

          NAME                              USED  AVAIL  REFER  MOUNTPOINT
          pool/cache@2012:12:18:51:2:19:00     0      -   525M  -
          pool/cache@2012:12:18:51:2:19:15     0      -   525M  -
          pool/home@2012:12:18:51:2:19:00  18.8M      -  28.6G  -
          pool/home@2012:12:18:51:2:19:15  18.3M      -  28.6G  -
          pool/log@2012:12:18:51:2:19:00    184K      -  10.4M  -
          pool/log@2012:12:18:51:2:19:15    184K      -  10.4M  -
          pool/swap@2012:12:18:51:2:19:00      0      -    76K  -
          pool/swap@2012:12:18:51:2:19:15      0      -    76K  -
          pool/vmsa@2012:12:18:51:2:19:00      0      -  1.12M  -
          pool/vmsa@2012:12:18:51:2:19:15      0      -  1.12M  -
          pool/vmsb@2012:12:18:51:2:19:00      0      -  1.31M  -
          pool/vmsb@2012:12:18:51:2:19:15      0      -  1.31M  -
          tank@2012:12:18:51:2:19:00           0      -  43.4K  -
          tank@2012:12:18:51:2:19:15           0      -  43.4K  -
          tank/test@2012:12:18:51:2:19:00      0      -  37.1M  -
          tank/test@2012:12:18:51:2:19:15      0      -  37.1M  -
</pre>
			 <div class="para">
				Notice that by default, it will show all snapshots for all pools.
			</div>
			 <div class="para">
				If you want to be more specific with the output, you can see all snapshots of a given parent, whether it be a dataset, or a storage pool. You only need to pass the â-râ switch for recursion, then provide the parent. In this case, Iâll see only the snapshots of the storage pool âtankâ, and ignore those in âpoolâ:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list -r -t snapshot tank</code>

          NAME                              USED  AVAIL  REFER  MOUNTPOINT
          tank@2012:12:18:51:2:19:00           0      -  43.4K  -
          tank@2012:12:18:51:2:19:15           0      -  43.4K  -
          tank/test@2012:12:18:51:2:19:00      0      -  37.1M  -
          tank/test@2012:12:18:51:2:19:15      0      -  37.1M  -
</pre>
			 <h6><a id="idm140006538931856">
      â </a> Destroying Snapshots </h6> <div class="para">
				Just as you would destroy a storage pool, or a ZFS dataset, you use a similar method for destroying snapshots. To destroy a snapshot, use the âzfs destroyâ command, and supply the snapshot as an argument that you want to destroy:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs destroy tank/test@2012:12:18:51:2:19:15</code>
</pre>
			 <div class="para">
				An important thing to know, is if a snapshot exists, itâs considered a child filesystem to the dataset. As such, you cannot remove a dataset until all snapshots, and nested datasets have been destroyed.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs destroy tank/test</code>

cannot destroy 'tank/test': filesystem has children
use '-r' to destroy the following datasets:
tank/test@2012:12:18:51:2:19:15
tank/test@2012:12:18:51:2:19:00
</pre>
			 <div class="para">
				Destroying snapshots can free up additional space that other snapshots may be holding onto, because they are unique to those snapshots.
			</div>
			 <h6><a id="idm140006538927456">
      â </a> Renaming Snapshots </h6> <div class="para">
				You can rename snapshots, however, they must be renamed in the storage pool and ZFS dataset from which they were created. Other than that, renaming snapshots is pretty straight forward:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs rename tank/test@2012:12:18:51:2:19:15 tank/test@tuesday-19:15</code>
</pre>
			 <h6><a id="idm140006538925456">
      â </a> Rolling Back to a Snapshot </h6> <div class="para">
				A discussion about snapshots would not be complete without a discussion about rolling back your filesystem to a previous snapshot.
			</div>
			 <div class="para">
				Rolling back to a previous snapshot will discard any data changes between that snapshot and the current time. Further, by default, you can only rollback to the most recent snapshot. In order to rollback to an earlier snapshot, you must destroy all snapshots between the current time and that snapshot you wish to rollback to. If thatâs not enough, the filesystem must be unmounted before the rollback can begin. This means downtime.
			</div>
			 <div class="para">
				To rollback the âtank/testâ dataset to the âtuesdayâ snapshot, we would issue:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs rollback tank/test@tuesday</code>

cannot rollback to 'tank/test@tuesday': more recent snapshots exist
use '-r' to force deletion of the following snapshots:
tank/test@wednesday
tank/test@thursday
</pre>
			 <div class="para">
				As expected, we must remove the â@wednesdayâ and â@thursdayâ snapshots before we can rollback to the â@tuesdayâ snapshot.
			</div>
			 <h6><a id="idm140006538921440">
      â </a> ZFS Clones </h6> <div class="para">
				A ZFS clone is a writeable filesystem that was âupgradedâ from a snapshot. Clones can only be created from snapshots, and a dependency on the snapshot will remain as long as the clone exists. This means that you cannot destroy a snapshot, if you cloned it. The clone relies on the data that the snapshot gives it, to exist. You must destroy the clone before you can destroy the snapshot.
			</div>
			 <div class="para">
				Creating clones is nearly instantaneous, just like snapshots, and initially does not take up any additional space. Instead, it occupies all the initial space of the snapshot. As data is modified in the clone, it begins to take up space separate from the snapshot.
			</div>
			 <h6><a id="idm140006538919584">
      â </a> Creating ZFS Clones </h6> <div class="para">
				Creating a clone is done with the âzfs cloneâ command, the snapshot to clone, and the name of the new filesystem. The clone does not need to reside in the same dataset as the clone, but it does need to reside in the same storage pool. For example, if I wanted to clone the âtank/test@tuesdayâ snapshot, and give it the name of âtank/tuesdayâ, I would run the following command:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs clone tank/test@tuesday tank/tuesday</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
dd if=/dev/zero of=/tank/tuesday/random.img bs=1M count=100</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs list -r tank</code>

          NAME           USED  AVAIL  REFER  MOUNTPOINT
          tank           161M  2.78G  44.9K  /tank
          tank/test     37.1M  2.78G  37.1M  /tank/test
          tank/tuesday   124M  2.78G   161M  /tank/tuesday
</pre>
			 <h6><a id="idm140006538915856">
      â </a> Destroying Clones </h6> <div class="para">
				As with destroying datasets or snapshots, we use the âzfs destroyâ command. Again, you cannot destroy a snapshot until you destroy the clones. So, if we wanted to destroy the âtank/tuesdayâ clone:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs destroy tank/tuesday</code>
</pre>
			 <div class="para">
				Just like you would with any other ZFS dataset.
			</div>
			 <h6><a id="idm140006538913456">
      â </a> Some Final Thoughts </h6> <div class="para">
				Because keeping snapshots is very cheap, itâs recommended to snapshot your datasets frequently. Sun Microsystems provided a Time Slider that was part of the GNOME Nautilus file manager. Time Slider keeps snapshots in the following manner:
			</div>
			 <div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem">
					<div class="para">
						frequent- snapshots every 15 mins, keeping 4 snapshots
					</div>

				</li><li class="listitem">
					<div class="para">
						hourly- snapshots every hour, keeping 24 snapshots
					</div>

				</li><li class="listitem">
					<div class="para">
						daily- snapshots every day, keeping 31 snapshots
					</div>

				</li><li class="listitem">
					<div class="para">
						weekly- snapshots every week, keeping 7 snapshots
					</div>

				</li><li class="listitem">
					<div class="para">
						monthly- snapshots every month, keeping 12 snapshots
					</div>

				</li></ul></div>
			 <div class="para">
				Unfortunately, Time Slider is not part of the standard GNOME desktop, so itâs not available for GNU/Linux. However, the ZFS on Linux developers have created a âzfs-auto-snapshotâ package that you can install from the <a href="https://launchpad.net/~zfs-native/+archive/stable">projectâs PPA</a> if running Ubuntu. If running another GNU/Linux operating system, you could easily write a Bash or Python script that mimics that functionality, and place it on your rootâs crontab.
			</div>
			 <div class="para">
				Because both snapshots and clones are cheap, itâs recommended that you take advantage of them. Clones can be useful to test deploying virtual machines, or development environments that are cloned from production environments. When finished, they can easily be destroyed, without affecting the parent dataset from which the snapshot was created.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-send-and-recv-fsys">
      â </a>6.2.3.2.4.Â Sending and Receiving Filesystems</h5></div></div></div>
			
			 <a id="idm140006538904720" class="indexterm"></a>
			 <h6><a id="idm140006538903568">
      â </a> ZFS Send </h6> <div class="para">
				Sending a ZFS filesystem means taking a snapshot of a dataset, and sending the snapshot. This ensures that while sending the data, it will always remain consistent, which is crux for all things ZFS. By default, we send the data to a file. We then can move that single file to an offsite backup, another storage server, or whatever. The advantage a ZFS send has over âddâ, is the fact that you do not need to take the filesystem offilne to get at the data. This is a Big Win IMO.
			</div>
			 <div class="para">
				To send a filesystem to a file, you first must make a snapshot of the dataset. After the snapshot has been made, you send the snapshot. This produces an output stream, that must be redirected. As such, you would issue something like the following:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs snapshot tank/test@tuesday</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs send tank/test@tuesday &gt; /backup/test-tuesday.img</code>
</pre>
			 <div class="para">
				Now, your brain should be thinking. You have at your disposal a whole suite of Unix utilities to manipulate data. So, rather than storing the raw data, how about we compress it with the âxzâ utility?
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs send tank/test@tuesday | xz &gt; /backup/test-tuesday.img.xz</code>
</pre>
			 <div class="para">
				Want to encrypt the backup? You could use OpenSSL or GnuPG:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs send tank/test@tuesday | xz | openssl enc -aes-256-cbc -a
-salt &gt; /backup/test-tuesday.img.xz.asc</code>
</pre>
			 <h6><a id="idm140006538896816">
      â </a> ZFS Receive </h6> <div class="para">
				Receiving ZFS filesystems is the other side of the coin. Where you have a data stream, you can import that data into a full writable filesystem. It wouldnât make much sense to send the filesystem to an image file, if you canât really do anything with the data in the file.
			</div>
			 <div class="para">
				Just as âzfs sendâ operates on streams, âzfs receiveâ does the same. So, suppose we want to receive the â/backup/test-tuesday.imgâ filesystem. We can receive it into any storage pool, and it will create the necessary dataset.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs receive tank/test2 &lt; /backup/test-tuesday.img</code>
</pre>
			 <div class="para">
				Of course, in our sending example, I compressed and encrypted a sent filesystem. So, to reverse that process, I do the commands in the reverse order:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
openssl enc -d -aes-256-cbc -a -in /storage/temp/testzone.gz.ssl
| unxz | zfs receive tank/test2</code>
</pre>
			 <div class="para">
				The âzfs recvâ command can be used as a shortcut.
			</div>
			 <h6><a id="idm140006538892000">
      â </a> Combining Send and Receive </h6> <div class="para">
				Both âzfs sendâ and âzfs receiveâ operate on streams of input and output. So, it would make sense that we can send a filesystem into another. Of course we can do this locally:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs send tank/test@tuesday | zfs receive pool/test</code>
</pre>
			 <div class="para">
				This is perfectly acceptable, but it doesnât make a lot of sense to keep multiple copies of the filesystem on the same storage server. Instead, it would make better sense to send the filesystem to a remote box. You can do this trivially with OpenSSH:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs send tank/test@tuesday | ssh user@server.example.com "zfs receive pool/test"</code>
</pre>
			 <div class="para">
				Check out the simplicity of that command. Youâre taking live, running and consistent data from a snapshot, and sending that data to another box. This is epic for offsite storage backups. On your ZFS storage servers, you would run frequent snapshots of the datasets. Then, as a nightly cron job, you would âzfs sendâ the latest snapshot to an offsite storage server using âzfs receiveâ. And because you are running a secure, tight ship, you encrypt the data with OpenSSL and XZ. Win.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="admin-man-sect-zvols">
      â </a>6.2.3.2.5.Â ZVOLS</h5></div></div></div>
			
			 <a id="idm140006538886192" class="indexterm"></a>
			 <h6><a id="idm140006538885040">
      â </a> What is a ZVOL? </h6> <div class="para">
				A ZVOL is a âZFS volumeâ that has been exported to the system as a block device. So far, when dealing with the ZFS filesystem, other than creating our pool, we havenât dealt with block devices at all, even when mounting the datasets. Itâs almost like ZFS is behaving like a userspace application more than a filesystem. I mean, on GNU/Linux, when working with filesystems, youâre constantly working with block devices, whether they be full disks, partitions, RAID arrays or logical volumes. Yet somehow, weâve managed to escape all that with ZFS. Well, not any longer. Now we get our hands dirty with ZVOLs.
			</div>
			 <div class="para">
				A ZVOL is a ZFS block device that resides in your storage pool. This means that the single block device gets to take advantage of your underlying RAID array, such as mirrors or RAID-Z. It gets to take advantage of the copy-on-write benefits, such as snapshots. It gets to take advantage of online scrubbing, compression and data deduplication. It gets to take advantage of the ZIL and ARC. Because itâs a legitimate block device, you can do some very interesting things with your ZVOL. Weâll look at three of them here- swap, ext4, and VM storage. First, we need to learn how to create a ZVOL.
			</div>
			 <h6><a id="idm140006538882624">
      â </a> Creating a ZVOL </h6> <div class="para">
				To create a ZVOL, we use the â-Vâ switch with our âzfs createâ command, and give it a size. For example, if we wanted to create a 1 GB ZVOL, we could issue the following command. Notice further that there are a couple new symlinks that exist in /dev/zvol/tank/ and /dev/tank/ which points to a new block device in /dev/:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create -V 1G tank/disk1</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -l /dev/zvol/tank/disk1</code>

lrwxrwxrwx 1 root root 11 Dec 20 22:10 /dev/zvol/tank/disk1 -&gt; ../../zd144

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -l /dev/tank/disk1</code>

lrwxrwxrwx 1 root root 8 Dec 20 22:10 /dev/tank/disk1 -&gt; ../zd144
</pre>
			 <div class="para">
				Because this is a full fledged, 100% bona fide block device that is 1 GB in size, we can do anything with it that we would do with any other block device, and we get all the benefits of ZFS underneath. Plus, creating a ZVOL is near instantaneous, regardless of size. Now, I could create a block device with GNU/Linux from a file on the filesystem. For example, if running ext4, I can create a 1 GB file, then make a block device out of it as follows:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
fallocate -l 1G /tmp/file.img</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
losetup /dev/loop0 /tmp/file.img</code>
</pre>
			 <div class="para">
				I now have the block device /dev/loop0 that represents my 1 GB file. Just as with any other block device, I can format it, add it to swap, etc. But itâs not as elegant, and it has severe limitations. First off, by default you only have 8 loopback devices for your exported block devices. You can change this number, however. With ZFS, you can create 2^64 ZVOLs by default. Also, it requires a preallocated image, on top of your filesystem. So, you are managing three layers of data: the block device, the file, and the blocks on the filesystem. With ZVOLs, the block device is exported right off the storage pool, just like any other dataset.
			</div>
			 <div class="para">
				Letâs look at some things we can do with this ZVOL.
			</div>
			 <h6><a id="idm140006538875136">
      â </a> Swap on a ZVOL </h6> <div class="para">
				It can act as part of a healthy system, keeping RAM dedicated to what the kernel actively needs. But, when active RAM starts spilling over to swap, then you have âthe swap of deathâ, as your disks thrash, trying to keep up with the demands of the kernel. So, depending on your system and needs, you may or may not need swap.
			</div>
			 <div class="para">
				First, letâs create 1 GB block device for our swap. Weâll call the dataset âtank/swapâ to make it easy to identify its intention. Before we begin, letâs check out how much swap we currently have on our system with the âfreeâ command:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
free</code>
	
	                  total       used       free     shared    buffers     cached
             Mem:      12327288    8637124    3690164          0     175264    1276812
             -/+ buffers/cache:    7185048    5142240
             Swap:            0          0          0
</pre>
			 <div class="para">
				In this case, we do not have any swap enabled. So, letâs create 1 GB of swap on a ZVOL, and add it to the kernel:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create -V 1G tank/swap</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
mkswap /dev/zvol/tank/swap</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
swapon /dev/zvol/tank/swap</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
free</code>
	
                 total       used       free     shared    buffers     cached
    Mem:      12327288    8667492    3659796          0     175268    1276804
    -/+ buffers/cache:    7215420    5111868
    Swap:      1048572          0    1048572
</pre>
			 <div class="para">
				It worked! We have a legitimate Linux kernel swap device on top of ZFS. Sweet. As is typical with swap devices, they donât have a mountpoint. They are either enabled, or disabled, and this swap device is no different.
			</div>
			 <h6><a id="idm140006538867632">
      â </a> Ext4 on a ZVOL </h6> <div class="para">
				This may sound wacky, but you could put another filesystem, and mount it, on top of a ZVOL. In other words, you could have an ext4 formatted ZVOL and mounted to /mnt. You could even partition your ZVOL, and put multiple filesystems on it. Letâs do that!
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs create -V 100G tank/ext4</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
fdisk /dev/tank/ext4</code>

( follow the prompts to create 2 partitions- the first 1 GB in size, the second to
fill the rest )

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
fdisk -l /dev/tank/ext4</code>
	
    Disk /dev/tank/ext4: 107.4 GB, 107374182400 bytes
    16 heads, 63 sectors/track, 208050 cylinders, total 209715200 sectors
    Units = sectors of 1 * 512 = 512 bytes
    Sector size (logical/physical): 512 bytes / 8192 bytes
    I/O size (minimum/optimal): 8192 bytes / 8192 bytes
    Disk identifier: 0x000a0d54

	
                         Device Boot       Start         End      Blocks   Id  System
    /dev/tank/ext4p1            2048     2099199     1048576   83  Linux
    /dev/tank/ext4p2         2099200   209715199   103808000   83  Linux
</pre>
			 <div class="para">
				Letâs create some filesystems, and mount them:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs set compression=lzjb pool/ext4</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
tar -cf /mnt/zd0p1/files.tar /etc/</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
tar -cf /mnt/zd0p2/files.tar /etc /var/log/</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
zfs snapshot tank/ext4@001</code>
</pre>
			 <div class="para">
				You probably didnât notice, but you just enabled transparent compression and took a snapshot of your ext4 filesystem. These are two things you canât do with ext4 natively. You also have all the benefits of ZFS that ext4 normally couldnât give you. So, now you regularly snapshot your data, you perform online scrubs, and send it offsite for backup. Most importantly, your data is consistent.
			</div>
			 <h6><a id="idm140006538859408">
      â </a> ZVOL storage for VMs </h6> <div class="para">
				Lastly, you can use these block devices as the backend storage for VMs. Itâs not uncommon to create logical volume block devices as the backend for VM storage. After having the block device available for Qemu, you attach the block device to the virtual machine, and from its perspective, you have a â/dev/vdaâ or â/dev/sdaâ depending on the setup.
			</div>
			 <div class="para">
				If using libvirt, you would have a /etc/libvirt/qemu/vm.xml file. In that file, you could have the following, where â/dev/zd0â³ is the ZVOL block device:
			</div>
			 
<pre class="screen">
&lt;disk type='block' device='disk'&gt;
&lt;driver name='qemu' type='raw' cache='none'/'&gt;
&lt;source dev='/dev/zd0'/'&gt;
&lt;target dev='vda' bus='virtio'/'&gt;
&lt;alias name='virtio-disk0'/'&gt;
&lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/'&gt;
&lt;/disk'&gt;
</pre>
			 <div class="para">
				At this point, your VM gets all the ZFS benefits underneath, such as snapshots, compression, deduplication, data integrity, drive redundancy, etc.
			</div>
			 <h6><a id="idm140006538856480">
      â </a> Conclusion </h6> <div class="para">
				ZVOLs are a great way to get to block devices quickly while taking advantage of all of the underlying ZFS features. Using the ZVOLs as the VM backing storage is especially attractive. However, I should note that when using ZVOLs, you cannot replicate them across a cluster. ZFS is not a clustered filesystem. If you want data replication across a cluster, then you should not use ZVOLs, and use file images for your VM backing storage instead. Other than that, you get all of the amazing benefits of ZFS that we have been blogging about up to this point, and beyond, for whatever data resides on your ZVOL.
			</div>

		</div>

	</div>
	 
</div>
	
</div>
	 <div xml:lang="en-US" class="section" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="admin-man-sect-os-pkg-man">
      â </a>6.3.Â OS Package Management</h2></div></div></div>
	
	 <a id="idm140006538853712" class="indexterm"></a>
	 <div class="para">
		Ubuntu/Debian package management is based on the concept of package sources or repositories. A configuration file <code class="filename">/etc/apt/sources.list</code> (or separate files in the directory <code class="filename">/etc/apt/sources.list.d</code>) specifies the location of package sources.
	</div>
	 <div class="para">
		When a package is to be installed, all package source locations are checked whether they contain the desired package. If the desired package is found in only one package repository that one is taken, if it is found in more than one, then the package with the newest version is installed.
	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-pkg-srcs">
      â </a>6.3.1.Â Package sources</h3></div></div></div>
		
		 <a id="idm140006538849760" class="indexterm"></a>
		 <div class="para">
			Package sources are usually specified in <code class="filename">/etc/apt/sources.list</code> and can be of many different types, like http, ftp, file, cdrom, â¦ (see <code class="command">man sources.list</code>). In a default Qlustar installation this file is empty, since all the Qlustar package sources are defined in the file <code class="filename">/etc/apt/sources.list.d/qlustar.list</code>. If your system has access to the Internet either directly or through a http proxy the file will look like this:
		</div>
		 
<pre class="screen">
deb http://repo.qlustar.com/repo/ubuntu 9.1-trusty main universe non-free
deb http://repo.qlustar.com/repo/ubuntu 9.1-trusty-proposed-updates main universe non-free
</pre>
		 <div class="para">
			This enables access to the Qlustar 8.1 software repository.
		</div>
		 <div xmlns:d="http://docbook.org/ns/docbook" class="note"><div class="admonition_header"><p><strong>Note</strong></p></div><div class="admonition">
			<div class="para">
				The file <code class="filename">/etc/apt/sources.list.d/qlustar.list</code> is managed by Qlustar and should usually not be edited manually. If you prefer not to receive the proposed updates, you can comment out the second line in the file. Be aware, that this will prevent you from receiving timely security updates as well.
			</div>

		</div></div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-dpkg">
      â </a>6.3.2.Â dpkg</h3></div></div></div>
		
		 <a id="idm140006538843376" class="indexterm"></a>
		 <div class="para">
			<code class="command">dpkg</code> (see <code class="command">man dpkg</code>) is the basic package management tool for Ubuntu/Debian, comparable to rpm (<span class="application"><strong>Red Hat Package Manager</strong></span>). It is not capable of automatically resolving package dependencies.
		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-apt">
      â </a>6.3.3.Â apt</h3></div></div></div>
		
		 <a id="idm140006538839328" class="indexterm"></a>
		 <div class="para">
			<code class="command">apt</code> is the high-level package management tool for Ubuntu/Debian. <code class="command">apt-get</code> (<code class="command">man apt-get</code>) with its sub-commands provides all the functionality needed to maintain an Ubuntu/Debian system. A seem-less and fast upgrade of an Ubuntu/Debian system is typically performed running the two commands
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
apt-get update</code>

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
apt-get dist-upgrade.</code>
</pre>
		 <div class="para">
			Detailed upgrade instructions for a Qlustar system can be found in the <a class="xref" href="#sec-Qlustar">Qlustar Update Section</a>
		</div>
		 <div class="para">
			New packages can be installed by running <code class="command">apt-get install <em class="replaceable">&lt;package name&gt;</em></code>. If <em class="replaceable">package name</em> depends on, or conflicts with other packages those will be automatically installed or removed upon confirmation.
		</div>

	</div>
	 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-man-sect-debian-pkg-altern">
      â </a>6.3.4.Â Debian Package Alternatives</h3></div></div></div>
		
		 <a id="idm140006538830304" class="indexterm"></a>
		 <div class="para">
			The possibility to concurrently run different versions of the same application on a single cluster is often crucial. In principle, this is achievable in a couple of ways, each one requiring more or less handwork depending on the type of application in question. Fortunately, Ubuntu/Debian provides the built-in "alternatives mechanism" to manage software versions in automated form. It has the additional advantage that it works appropriately for any kind of application.
		</div>
		 <div class="para">
			Let us consider the case of the GNU C compiler <code class="command">gcc</code> as an example of the situation described above. Simply installing the <span class="package">gcc</span> package via <code class="command">apt-get </code> is all you need to do in this case. The alternatives are automatically configured for you.
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
gcc --version</code>

gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</pre>
		 <div class="para">
			This tells us that currently we are running the version 4.6.3 of the GNU C compiler. Let us inspect the <span class="application"><strong>gcc</strong></span> binary.
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
which gcc</code>

/usr/bin/gcc

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -l /usr/bin/gcc</code>

lrwxrwxrwx ... /usr/bin/gcc -&gt; /etc/alternatives/gcc
</pre>
		 <div class="para">
			As we tried to locate the <code class="command">gcc</code> command, we found that it was being executed from the <code class="filename">/usr/bin</code> path. However we also discovered, that itâs a symbolic link pointing to <code class="filename">/etc/alternatives/gcc</code>. The directory <code class="filename">/etc/alternatives</code> is the place, where all the software alternatives are configured in Ubuntu/Debian. Let us inspect a little further.
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -l /etc/alternatives/gcc</code>

lrwxrwxrwx 1 root root 16 Mai 13 19:23 /etc/alternatives/gcc -&gt; /usr/bin/gcc-4.6

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -l /usr/bin/gcc-4.6</code>

-rwxr-xr-x 1 root root 353216 Apr 16  2012 /usr/bin/gcc-4.6
</pre>
		 <div class="para">
			We have another symbolic link, this time referring to <code class="filename">/usr/bin/gcc-4.6</code> and a little digging afterward reveals that this is the real <code class="command">gcc</code> executable. If alternative versions for a program are available, the alternatives system will create a link with the name of the program in the default path pointing to the appropriate file in <code class="filename">/etc/alternatives</code>. This will finally link to the executable, we actually want to use. Instead of manually manipulating these links, choosing a different default version for a program should be done using the command <code class="command">update-alternatives</code>.
		</div>
		 <div class="para">
			Using <code class="command">update-alternatives</code> we can quickly figure out which alternatives are currently configured for a certain executable. Let us look at the current setup of <span class="application"><strong>gcc</strong></span>.
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
update-alternatives --display gcc</code>

gcc - auto mode
link currently points to /usr/bin/gcc-4.6
/usr/bin/gcc-4.8 - priority 20
/usr/bin/gcc-4.6 - priority 50
Current âbestâ version is /usr/bin/gcc-4.6
</pre>
		 <div class="para">
			As you can see the current link points to <code class="filename">/usr/bin/gcc-4.6</code> as we already discovered. There is another alternative, namely <code class="filename">/usr/bin/gcc-4.8</code> with a priority of 20. The priority of <code class="filename">/usr/bin/gcc-4.6</code> is 50. The line <code class="code">'gcc - auto modeâ</code> means that the alternatives system will look for the package with the highest priority in order to use that executable.
		</div>
		 <div class="para">
			The last line in the above output says that the current best priority link is <code class="filename">/usr/bin/gcc-4.6</code>. Now if we want to use version 4.8 instead, we can tell <code class="command">update-alternatives</code> to use exactly this version:
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
update-alternatives --config gcc</code>

There are 2 alternatives which provide âgccâ.
Selection Alternative
-----------------------------------------------
+ 1 /usr/bin/gcc-4.8
2 /usr/bin/gcc-4.6
*Press enter to keep the default[*], or type selection number:
</pre>
		 <div class="para">
			We already knew by looking at the output that there where only two options. Now we can select which option we want. Letâs look at the output of the command again.
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
update-alternatives --display gcc</code>

gcc - status is manual.
link currently points to /usr/bin/4.8
/usr/bin/gcc-4.6 - priority 50
/usr/bin/gcc-4.8 - priority 20
Current âbestâ version is /usr/bin/gcc-4.6.
</pre>
		 <div class="para">
			We got a link currently pointing to <code class="filename">/usr/bin/gcc-4.8</code> so we quickly do the same checking as we did before.
		</div>
		 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
which gcc</code>

/usr/bin/gcc

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -l /usr/bin/gcc</code>

lrwxrwxrwx 1 root root 21 Sep  2 18:32 /usr/bin/gcc -&gt; /etc/alternatives/gcc

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
ls -l /etc/alternatives/gcc</code>

lrwxrwxrwx 1 root root 16 Sep  2 18:32 /etc/alternatives/gcc -&gt; /usr/bin/gcc-4.8

<code class="prompt">0 root@cl-head ~ #</code><code class="command">
gcc --version</code>

gcc (Ubuntu 4.8.1-2ubuntu1~10.04.1) 4.8.1
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</pre>
		 <div class="para">
			Only the link within <code class="filename">/etc/alternatives</code> has changed. When looking at the gcc version we see that itâs using the gcc-4.8 version.
		</div>
		 <div class="para">
			A further useful feature of <code class="command">update-alternatives</code> is the possibility of creating groups of files having a relation with each other. These so called slaves will not only allow you to update the link to the desired executable but also any other information related to it like man pages, documentation, etc. Please consult <code class="command">man update-alternatives</code> for more information on the capabilities of this powerful software versioning method.
		</div>

	</div>

</div>

</div>
	 <div xml:lang="en-US" class="chapter" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="chap-Updating-Qlustar">
      â </a>ChapterÂ 7.Â Updating Qlustar</h1></div></div></div><div class="toc"><dl class="toc"><dt><span class="section"><a href="#sec-Qlustar">7.1. Qlustar updates</a></span></dt><dd><dl><dt><span class="section"><a href="#updating-headnodes">7.1.1. Updating the head-node(s)</a></span></dt><dt><span class="section"><a href="#updating-chroots">7.1.2. Updating the chroot(s)</a></span></dt><dt><span class="section"><a href="#updating-nodes">7.1.3. Updating the nodes</a></span></dt></dl></dd></dl></div>
	
	 <div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="sec-Qlustar">
      â </a>7.1.Â Qlustar updates</h2></div></div></div>
		
		 <div class="para">
			Updating a <span class="productname">Qlustar</span> cluster is a multi-step process. The detailed steps depend on whether the update involves a kernel update or not (the release notes will mention it, if a new kernel is part of the update) and what module versions you have selected for your images. We'll explain the differences below. Follow the steps in the following order.
		</div>
		 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="updating-headnodes">
      â </a>7.1.1.Â Updating the head-node(s)</h3></div></div></div>
			
			 <div class="para">
				Apply the standard <span class="application"><strong>Debian</strong></span> procedure to update the head-node(s) installation. Execute as root:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
apt-get update</code>
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
apt-get dist-upgrade</code>
</pre>
			 <div class="para">
				This will update all packages. In most cases, if new versions of the <span class="productname">Qlustar</span> image module packages are available, this will also automatically rebuild the images defined in <span class="application"><strong>QluMan</strong></span>. We elaborate on this <a class="xref" href="#image-update"> below</a>.
			</div>
			 <div class="para">
				If the kernel was updated by this process reboot the head-node(s) after executing the above commands, otherwise you're done with this step.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="updating-chroots">
      â </a>7.1.2.Â Updating the chroot(s)</h3></div></div></div>
			
			 <div class="para">
				This is also a standard Debian upgrade as explained in the <a href="../First_Steps_Guide/index.html#sec-Adding-Software___blank___">First Step Guide</a>. If you have setup multiple chroots, you'll have to update all of them.
			</div>

		</div>
		 <div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="updating-nodes">
      â </a>7.1.3.Â Updating the nodes</h3></div></div></div>
			
			 <div class="para">
				If the update contains a new kernel, reboot all nodes. In case you have some storage nodes that export a global file-system (e.g. <em class="firstterm">NFS</em> or <em class="firstterm">Lustre</em>), reboot them first, then all the other nodes.
			</div>
			 <div class="para">
				If the update doesn't contain a new kernel, you have the choice of either rebooting the nodes like above or using the <span class="application"><strong>Qlustar online update mechanism</strong></span> (currently the online update has to be executed manually, future versions of <span class="application"><strong>QluMan</strong></span> will allow to do this from the GUI) as follows. Execute the following command on the head-node for each node (of course, this can be done in a shell <code class="command">for loop</code>):
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
qlustar-image-update -u &lt;image name&gt; &lt;hostname&gt;</code>
</pre>
			 <div class="para">
				where &lt;image name&gt; has to be the name of the image that is currently booted on the corresponding node. You also have the option to first check what exactly the update will change. To find out, execute the following for one sample node per image type:
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
qlustar-image-update -c &lt;image name&gt; &lt;hostname&gt;</code>
</pre>
			 <div class="para">
				This will show the files that will be changed by this update.
			</div>
			 
<pre class="screen">
<code class="prompt">0 root@cl-head ~ #</code><code class="command">
qlustar-image-update -s &lt;image name&gt; &lt;hostname&gt;</code>
</pre>
			 <div class="para">
				This will show all services (daemons) that need to be restarted as a result of this update.
			</div>
			 <div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="image-update">
      â </a>7.1.3.1.Â When will images be updated/rebuilt?</h4></div></div></div>
				
				 <div class="para">
					There are a couple of cases to distinguish, depending on what version is selected for an image in QluMan (see also the corresponding section in the <a href="../QluMan_Guide/index.html#sec-Qlustar-OS-Images___blank___">QluMan Guide</a>
				</div>
				 <div xmlns:d="http://docbook.org/ns/docbook" class="orderedlist"><ol><li class="listitem">
						<div class="para">
							Selected image version is of <span class="type">type x.y.z</span>, e.g. 8.1.1. In this case, the image will be rebuilt if and only if there is an update for the selected version (8.1.1) of the modules. Such updates will always be bug-/security-fixes.
						</div>

					</li><li class="listitem">
						<div class="para">
							Selected image version is of <span class="type">type x.y</span>, e.g. 8.1 with 8.1.1 being the most recent 8.1.z version before applying the update. In this case the image is rebuilt, if the update entails a new maintenance release (e.g. 8.1.2 which will then become the new real image version) or an update for the currently installed (8.1.1) modules.
						</div>

					</li><li class="listitem">
						<div class="para">
							Selected image version is of <span class="type">type x</span>, e.g. 8 with 8.1 being the most recent 8.y and 8.1.1 being the most recent 8.1.z version before applying the update. In this case the image is rebuilt, if either the update entails a new feature release (e.g. 8.2, the latest 8.2.z will then become the new real image version) or a new maintenance release (e.g. 8.1.2 which will then become the new real image version) or an update for the currently installed (8.1.1) modules. With this option, manual intervention to obtain a new image version is only necessary when upgrading to a new major release (e.g. 9.y.z).
						</div>

					</li></ol></div>

			</div>

		</div>

	</div>
</div>
	 <div xml:lang="en-US" class="appendix" lang="en-US"><div class="titlepage"><div><div><h1 class="title"><a id="idm140006538772208">
      â </a>AppendixÂ A.Â Revision History</h1></div></div></div>
	
	 <div xmlns:d="http://docbook.org/ns/docbook" class="para"><p></p>
		<div class="revhistory"><table summary="Revision History"><tr><th align="left" valign="top" colspan="3"><strong>Revision History</strong></th></tr>
			<tr><td align="left">Revision 9.1-1</td><td align="left">Fri Jul 3 2015</td><td align="left"><span class="author"><span class="firstname">Qlustar</span> <span class="surname">Documentation Team</span></span></td></tr><tr><td align="left" colspan="3">
					<table border="0" summary="Simple list" class="simplelist"><tr><td>Updates for 9.1 release</td></tr></table>

				</td></tr>
			 <tr><td align="left">Revision 9.0-1</td><td align="left">Thu Jan 29 2015</td><td align="left"><span class="author"><span class="firstname">Qlustar</span> <span class="surname">Documentation Team</span></span></td></tr><tr><td align="left" colspan="3">
					<table border="0" summary="Simple list" class="simplelist"><tr><td>Updates for 9.0 release</td></tr></table>

				</td></tr>
			 <tr><td align="left">Revision 8.1-1</td><td align="left">Wed Jan 15 2014</td><td align="left"><span class="author"><span class="firstname">Qlustar</span> <span class="surname">Documentation Team</span></span></td></tr><tr><td align="left" colspan="3">
					<table border="0" summary="Simple list" class="simplelist"><tr><td>Updates for 8.1 release</td></tr></table>

				</td></tr>
			 <tr><td align="left">Revision 8.0-1</td><td align="left">Fri Mar 1 2013</td><td align="left"><span class="author"><span class="firstname">Qlustar</span> <span class="surname">Documentation Team</span></span></td></tr><tr><td align="left" colspan="3">
					<table border="0" summary="Simple list" class="simplelist"><tr><td>Initial 8.0 version</td></tr></table>

				</td></tr>

		</table></div>

	</div>
</div>
	 <div class="index"><div class="titlepage"><div><div><h1 class="title"><a id="idm140006538754976">
      â </a>Index</h1></div></div></div><div class="index"><div class="indexdiv"><h3>B</h3><dl><dt>Base Configuration, <a class="indexterm" href="#admin-man-chap-base-conf">Qlustar Base Configuration</a></dt><dt>Boot Process, <a class="indexterm" href="#admin-man-sect-boot-process">Boot Process</a></dt><dd><dl><dt>Compute-node booting, <a class="indexterm" href="#admin-man-sect-comp-node-boot">Compute-node booting</a></dt><dt>Dynamic Execution, <a class="indexterm" href="#admin-man-sect-execd-dynamic-boot-scripts">Dynamic Boot Script Excecution</a></dt><dt>RAM-disk image, <a class="indexterm" href="#admin-man-sect-RAM-disk-img">RAM-disk image</a></dt><dt>TFTP Boot Server, <a class="indexterm" href="#admin-man-sect-tftp-boot-serv">TFTP Boot Server</a></dt></dl></dd></dl></div><div class="indexdiv"><h3>F</h3><dl><dt>feedback</dt><dd><dl><dt>contact information for Qlustar, <a class="indexterm" href="#idm140006539861472">Feedback requested</a></dt></dl></dd><dt>Front-End nodes, <a class="indexterm" href="#admin-man-chap-introduction">Introduction</a></dt></dl></div><div class="indexdiv"><h3>G</h3><dl><dt>Ganglia, <a class="indexterm" href="#admin-man-sect-ganglia">Ganglia</a></dt><dd><dl><dt>Monitoring the nodes, <a class="indexterm" href="#admin-man-sect-monit-the-nodes">Monitoring the nodes</a></dt></dl></dd><dt>General Administration Tasks, <a class="indexterm" href="#admin-man-chap-gen-adm">General Administration Tasks</a></dt></dl></div><div class="indexdiv"><h3>H</h3><dl><dt>Hardware Configuration, <a class="indexterm" href="#admin-man-chap-hw-conf">Qlustar Hardware Configuration</a></dt><dt>head-node, <a class="indexterm" href="#admin-man-chap-introduction">Introduction</a></dt></dl></div><div class="indexdiv"><h3>I</h3><dl><dt>Infiniband, <a class="indexterm" href="#admin-man-sect-hw-IB">Infiniband Networks</a></dt><dd><dl><dt>Network diagnosis, <a class="indexterm" href="#admin-man-sect-hw-IB-diag">IB Fabric Verification/Diagnosis</a></dt><dt>OpenSM, <a class="indexterm" href="#admin-man-sect-hw-IB-opensm">OpenSM Configuration</a></dt></dl></dd></dl></div><div class="indexdiv"><h3>M</h3><dl><dt>Monitoring, <a class="indexterm" href="#admin-man-chap-monitoring">Monitoring Infrastructure</a></dt></dl></div><div class="indexdiv"><h3>N</h3><dl><dt>Nagios, <a class="indexterm" href="#admin-man-sect-nagios">Nagios</a></dt><dd><dl><dt>head-node, <a class="indexterm" href="#admin-man-sect-head-node">Monitoring the head-node(s)</a></dt><dt>Nagios Plugins, <a class="indexterm" href="#admin-man-sect-nag-plug-ins">Nagios Plugins</a></dt><dt>Restart, <a class="indexterm" href="#admin-man-sect-restart">Restart</a></dt><dt>Webinterface, <a class="indexterm" href="#admin-man-sect-web-interface">Webinterface</a></dt></dl></dd><dt>Network Configuration, <a class="indexterm" href="#admin-man-sect-network">Network Configuration and Services</a></dt><dd><dl><dt>Basic Network Configuration, <a class="indexterm" href="#admin-man-sect-ntwrk-config">Basic Network Configuration</a></dt><dt>cluster-internal networks, <a class="indexterm" href="#admin-man-sect-ntwrk-config">Basic Network Configuration</a></dt><dt>DHCP, <a class="indexterm" href="#admin-man-sect-dhcp">DHCP</a></dt><dd><dl><dt>Special DHCP options, <a class="indexterm" href="#admin-man-sect-spcl-dhcp-opt">Special DHCP options</a></dt></dl></dd><dt>DNS, <a class="indexterm" href="#admin-man-sect-dns">DNS</a></dt><dt>IP Masquerading, <a class="indexterm" href="#admin-man-sect-ip-msq">IP Masquerading (NAT)</a></dt><dt>Time Server, <a class="indexterm" href="#admin-man-sect-time-server">Time Server</a></dt></dl></dd><dt>Network Services, <a class="indexterm" href="#admin-man-sect-network">Network Configuration and Services</a></dt><dt>Node Customization, <a class="indexterm" href="#admin-man-sect-node-customize">Node Customization</a></dt><dd><dl><dt>Adding directories, files, links, <a class="indexterm" href="#admin-man-sect-add-dirs-files-links">Adding directories, files, links</a></dt><dt>Cluster-wide Configuration Directory, <a class="indexterm" href="#admin-man-sect-config-dir">Cluster-wide Configuration Directory</a></dt><dt>DHCP-Client, <a class="indexterm" href="#admin-man-sect-DHCP-client">DHCP-Client</a></dt><dt>Dynamic Configuration Settings, <a class="indexterm" href="#admin-man-sect-node-options">Dynamic Configuration Settings</a></dt><dt>Hard disc initialization, <a class="indexterm" href="#admin-man-sect-hard-disc-init">Hard disc initialization</a></dt><dt>Infiniband, <a class="indexterm" href="#admin-man-sect-infiniband">Infiniband</a></dt><dt>Mail Transport Agent, <a class="indexterm" href="#admin-man-sect-mail-trans-agent">Mail Transport Agent</a></dt><dt>NFS boot scripts, <a class="indexterm" href="#admin-man-sect-NFS-boot-scripts">NFS boot scripts</a></dt></dl></dd><dt>Node Management, <a class="indexterm" href="#admin-man-chap-node-manage">Cluster Node Management</a></dt></dl></div><div class="indexdiv"><h3>O</h3><dl><dt>Operating System</dt><dd><dl><dt>Cluster OS, <a class="indexterm" href="#admin-man-chap-introduction">Introduction</a></dt></dl></dd></dl></div><div class="indexdiv"><h3>P</h3><dl><dt>Package Management, <a class="indexterm" href="#admin-man-sect-os-pkg-man">OS Package Management</a></dt><dd><dl><dt>alternatives, <a class="indexterm" href="#admin-man-sect-debian-pkg-altern">Debian Package Alternatives</a></dt><dt>apt, <a class="indexterm" href="#admin-man-sect-apt">apt</a></dt><dt>dpkg, <a class="indexterm" href="#admin-man-sect-dpkg">dpkg</a></dt><dt>Package sources, <a class="indexterm" href="#admin-man-sect-pkg-srcs">Package sources</a></dt></dl></dd></dl></div><div class="indexdiv"><h3>Q</h3><dl><dt>QluMan</dt><dd><dl><dt>Remote Execution Server, <a class="indexterm" href="#admin-man-sect-qluman-execd">QluMan Remote Execution Server</a></dt></dl></dd></dl></div><div class="indexdiv"><h3>R</h3><dl><dt>Remote Control, <a class="indexterm" href="#admin-man-sect-remote-control">Node Remote Control</a></dt><dd><dl><dt>Serial Console Parameter, <a class="indexterm" href="#admin-man-sect-ser-cons-para">Serial Console Parameter</a>, <a class="indexterm" href="#admin-man-sect-ipmi-config">IPMI Configuration</a></dt></dl></dd></dl></div><div class="indexdiv"><h3>S</h3><dl><dt>Services, <a class="indexterm" href="#admin-man-sect-base-conf-serv">Basic Services</a></dt><dd><dl><dt>Automounter, <a class="indexterm" href="#admin-man-sect-auto-mounter">Automounter</a></dt><dt>Disk Partitions and File-systems, <a class="indexterm" href="#admin-man-sect-disk-part-file-sys">Disk Partitions and File-systems</a></dt><dt>Mail server - Postfix, <a class="indexterm" href="#admin-man-sect-mail-server-postfix">Mail server - Postfix</a></dt><dt>NFS, <a class="indexterm" href="#admin-man-sect-nfs-sect">NFS</a></dt><dt>NIS, <a class="indexterm" href="#admin-man-sect-nis">NIS</a></dt><dt>SSH - Secure Shell, <a class="indexterm" href="#admin-man-sect-ssh">SSH - Secure Shell</a></dt></dl></dd><dt>Shell setup, <a class="indexterm" href="#admin-man-sect-shell-stp">Shell Setup</a></dt><dd><dl><dt>Bash Setup, <a class="indexterm" href="#admin-man-sect-bash-setup">Bash Setup</a></dt><dt>Tcsh Setup, <a class="indexterm" href="#admin-man-sect-tcsh-setup">Tcsh Setup</a></dt></dl></dd><dt>Storage Management, <a class="indexterm" href="#admin-man-sect-stor-man">Storage Management</a></dt><dd><dl><dt>Logical Volume Management, <a class="indexterm" href="#admin-man-sect-vol-man">Logical Volume Management</a></dt><dt>Raid, <a class="indexterm" href="#admin-man-sect-raid">Raid</a></dt><dd><dl><dt>Kernel Software Raid, <a class="indexterm" href="#admin-man-sect-kernel-sftw-raid">Kernel Software RAID</a></dt></dl></dd></dl></dd></dl></div><div class="indexdiv"><h3>U</h3><dl><dt>User Management, <a class="indexterm" href="#admin-man-sect-user-manage">User Management</a></dt><dd><dl><dt>Adding users, <a class="indexterm" href="#admin-man-sect-add-user">Adding User Accounts</a></dt><dt>Managing restrictions, <a class="indexterm" href="#admin-man-sect-man-user-restr">Managing user restrictions</a></dt><dt>Removing users, <a class="indexterm" href="#admin-man-sect-rem-user">Removing User Accounts</a></dt></dl></dd></dl></div><div class="indexdiv"><h3>Z</h3><dl><dt>ZFS File System, <a class="indexterm" href="#admin-man-sect-zfs-admin">ZFS Filesystem Administration</a></dt><dd><dl><dt>Creating Filesystems, <a class="indexterm" href="#admin-man-sect-crt-flsys">Creating Filesystems</a></dt><dt>Sending and Receiving Filesystems, <a class="indexterm" href="#admin-man-sect-send-and-recv-fsys">Sending and Receiving Filesystems</a></dt><dt>Snapshots and Clones, <a class="indexterm" href="#admin-man-sect-snap-clones">Snapshots and Clones</a></dt><dt>Subsection Compression, <a class="indexterm" href="#admin-man-sect-comp-and-dudup">Subsection Compression</a></dt><dt>ZVOLS, <a class="indexterm" href="#admin-man-sect-zvols">ZVOLS</a></dt></dl></dd><dt>Zpools, <a class="indexterm" href="#admin-man-sect-zpools-admin">Zpool Administration</a></dt><dd><dl><dt>Best Practices and Caveats, <a class="indexterm" href="#admin-man-sect-best-practices">Best Practices and Caveats</a></dt><dt>Exporting and Importing Storage Pools, <a class="indexterm" href="#admin-man-sect-ex-import-strg-pools">Exporting and Importing Storage Pools</a></dt><dt>Getting and Setting Properties, <a class="indexterm" href="#admin-man-sect--setting-prop">Getting and Setting Properties</a></dt><dt>RAIDZ, <a class="indexterm" href="#admin-man-sect-raidz">RAIDZ</a></dt><dt>Scrub and Resilver, <a class="indexterm" href="#admin-man-sect-scrub-and-resilvr">Scrub and Resilver</a></dt><dt>VDEVs, <a class="indexterm" href="#admin-man-sect-VDEVs">VDEVs</a></dt></dl></dd></dl></div></div></div>
</div><div id="site_footer"></div><script type="text/javascript">
			$("#site_footer").load("../../../../../footer.html");
		</script></body></html>